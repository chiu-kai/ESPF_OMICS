{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n",
      "Exp tensor shape: torch.Size([476, 4692])\n",
      "Exp num_features 4692\n",
      "num_ccl,num_drug:  476 1440\n",
      "id_unrepeat_train (1152,)\n",
      "id_unrepeat_val (144,)\n",
      "id_unrepeat_test (144,)\n",
      "id_unrepeat_train_val (1296,)\n",
      "id_test.shape (68544,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-02-26 03:42:55 9678:9678 init.cpp:143] function cbapi->getCuptiStatus() failed with error CUPTI_ERROR_NOT_INITIALIZED (15)\n",
      "WARNING:2025-02-26 03:42:55 9678:9678 init.cpp:144] CUPTI initialization failed - CUDA profiler activities will be missing\n",
      "INFO:2025-02-26 03:42:55 9678:9678 init.cpp:146] If you see CUPTI_ERROR_INSUFFICIENT_PRIVILEGES, refer to https://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti\n",
      "STAGE:2025-02-26 03:42:55 9678:9678 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------------------------------------\n",
      "(1036,) (260,)\n",
      "id_train.shape (493136,)\n",
      "id_val.shape (123760,)\n",
      "State_dict for Sequential(\n",
      "  (0): Linear(in_features=4692, out_features=1000, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (3): ScaledSigmoid(scale=8)\n",
      "  (4): Linear(in_features=100, out_features=50, bias=True)\n",
      ") loaded successfully.\n",
      "\n",
      " Drug_SelfAttention is applied \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#main_kfold\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader, Subset\n",
    "import torch.nn.init as init\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import importlib.util\n",
    "from utils.ESPF_drug2emb import drug2emb_encoder\n",
    "from utils.Model import Omics_DrugESPF_Model\n",
    "from utils.split_data_id import split_id,repeat_func\n",
    "from utils.create_dataloader import OmicsDrugDataset\n",
    "from utils.train import train, evaluation\n",
    "from utils.correlation import correlation_func\n",
    "from utils.plot import loss_curve, correlation_density,Density_Plot_of_AUC_Values\n",
    "from utils.tools import get_data_value_range,set_seed,get_vram_usage\n",
    "from utils.Metrics import MetricsCalculator\n",
    "\n",
    "\n",
    "# pip install subword-nmt seaborn lifelines openpyxl matplotlib scikit-learn openTSNE\n",
    "# python3 ./main_kfold.py --config utils/config.py\n",
    "import torch.nn as nn\n",
    "from utils.Loss import Custom_LossFunction,Custom_Weighted_LossFunction\n",
    "from utils.Custom_Activation_Function import ScaledSigmoid\n",
    "\n",
    "test = False #False, True: batch_size = 3, num_epoch = 2, full dataset\n",
    "\n",
    "omics_files = {\n",
    "    'Mut': \"../data/CCLE/CCLE_match_TCGAgene_PRISMandEXPsample_binary_mutation_476_6009.txt\",\n",
    "    'Exp': \"../data/CCLE/CCLE_exp_476samples_4692genes.txt\",\n",
    "    # Add more omics types and paths as needed\n",
    "    }\n",
    "omics_dict = {'Mut':0,'Exp':1,'CN':2, 'Eff':3, 'Dep':4, 'Met':5}\n",
    "omics_data_dict = {}\n",
    "omics_data_tensor_dict = {}\n",
    "omics_numfeatures_dict = {}\n",
    "omics_encode_dim_dict ={'Mut':[1000,100,50],'Exp':[1000,100,50], # Dr.Chiu:exp[500,200,50]\n",
    "                        'CN':[100,50,30], 'Eff':[100,50,30], 'Dep':[100,50,30], 'Met':[100,50,30]}\n",
    "\n",
    "TCGA_pretrain_weight_path_dict = {'Mut': \"./results/Encoder_tcga_mut_1000_100_50_best_loss_0.0066.pt\",\n",
    "                                  'Exp': \"./results/Encoder_tcga_exp_1000_100_50_best_loss_0.7.pt\",\n",
    "                                  # Add more omics types and paths as needed\n",
    "                                }\n",
    "seed = 42\n",
    "#hyperparameter\n",
    "model_name = \"Omics_DrugESPF_Model\"\n",
    "AUCtransform = \"-log2\" #\"-log2\"\n",
    "splitType= 'byDrug' # byCCL byDrug\n",
    "kfoldCV = 5\n",
    "include_omics = ['Exp']\n",
    "max_drug_len=50 # 不夠補零補到50 / 超過取前50個subwords(index) !!!!須改方法!!!!\n",
    "drug_embedding_feature_size = 128\n",
    "ESPF = True # False True\n",
    "Drug_SelfAttention = True\n",
    "pos_emb_type = 'learned' # 'learned' 'sinusoidal'\n",
    "#需再修改-----------\n",
    "\n",
    "intermediate_size =512\n",
    "num_attention_heads = 8        \n",
    "attention_probs_dropout_prob = 0.1\n",
    "hidden_dropout_prob = 0.1\n",
    "\n",
    "if ESPF is True:\n",
    "    \n",
    "    drug_encode_dims =[1600,400,100] # 50*128\n",
    "    dense_layer_dim = sum(omics_encode_dim_dict[omic_type][2] for omic_type in include_omics) + drug_encode_dims[2] # MLPDim\n",
    "elif ESPF is False:\n",
    "    \n",
    "    drug_encode_dims =[110,55,22]\n",
    "    dense_layer_dim = sum(omics_encode_dim_dict[omic_type][2] for omic_type in include_omics) + drug_encode_dims[2] # MLPDim\n",
    "#需再修改-------------\n",
    "TrackGradient = False # False True\n",
    "\n",
    "activation_func = nn.ReLU()  # ReLU activation function # Leaky ReLu\n",
    "activation_func_final = ScaledSigmoid(scale=8) # GroundT range ( 0 ~ scale )\n",
    "#nn.Sigmoid()or ReLU() or Linear/identity(when -log2AUC)\n",
    "batch_size = 200\n",
    "num_epoch = 200 # for k fold CV \n",
    "patience = 20\n",
    "warmup_iters = 60\n",
    "Decrease_percent = 0.9\n",
    "continuous = True\n",
    "learning_rate=1e-05\n",
    "criterion = Custom_Weighted_LossFunction(loss_type=\"weighted_MSE\", loss_lambda=1.0, regular_type=\"L2\", regular_lambda=1e-05) #nn.MSELoss()#\n",
    "\"\"\" A customizable loss function class.\n",
    "    Args:\n",
    "        loss_type (str): The type of loss to use (\"RMSE\", \"MSE\", \"MAE\", \"MAE+MSE\", \"MAE+RMSE\")/(\"weighted_RMSE\", \"weighted_MSE\", \"weighted_MAE\", \"weighted_MAE+MSE\", \"weighted_MAE+RMSE\").\n",
    "        loss_lambda (float): The lambda weight for the additional loss (MSE or RMSE) if applicable. Default is MAE+ 1.0*(MSE or RMSE).\n",
    "        regular_type (str): The type of regularization to use (\"L1\", \"L2\", \"L1+L2\"), or None for no regularization.\n",
    "        regular_lambda (float): The lambda weight for regularization. Default is 1e-05.\"\"\"\n",
    "\n",
    "hyperparameter_print = f' omics_dict ={omics_dict}\\n omics_files ={omics_files}\\n TCGA_pretrain_weight_path_dict ={TCGA_pretrain_weight_path_dict}\\n seed ={seed}\\n  model_name ={model_name}\\n AUCtransform ={AUCtransform}\\n splitType ={splitType}\\n kfoldCV ={kfoldCV}\\n omics_encode_dim ={[(omic_type,omics_encode_dim_dict[omic_type]) for omic_type in include_omics]}\\n max_drug_len ={max_drug_len}\\n drug_embedding_feature_size ={drug_embedding_feature_size}\\n ESPF ={ESPF}\\n Drug_SelfAttention ={Drug_SelfAttention}\\n pos_emb_type ={pos_emb_type}\\n intermediate_size ={intermediate_size}\\n num_attention_heads ={num_attention_heads}\\n attention_probs_dropout_prob ={attention_probs_dropout_prob}\\n hidden_dropout_prob ={hidden_dropout_prob}\\n drug_encode_dims ={drug_encode_dims}\\n dense_layer_dim = {dense_layer_dim}\\n activation_func = {activation_func}\\n activation_func_final = {activation_func_final}\\n batch_size = {batch_size}\\n num_epoch = {num_epoch}\\n patience = {patience}\\n warmup_iters = {warmup_iters}\\n Decrease_percent = {Decrease_percent}\\n continuous ={continuous}\\n learning_rate = {learning_rate}\\n criterion ={criterion}\\n'\n",
    "\n",
    "__translation_table__ = str.maketrans({\n",
    "    \"*\": \"\",    \"/\": \"\",    \":\": \"-\",    \"%\": \"\",\n",
    "    \"'\": \"\",    \"\\\"\": \"\",    \"[\": \"\",    \"]\": \"\",\n",
    "    \",\": \"\" })\n",
    "\n",
    "hyperparameter_folder_part = (f'Model{model_name}_{splitType}_Omics{[omic_type for omic_type in include_omics]}_ESPF{ESPF}_Tranformer{Drug_SelfAttention}').translate(__translation_table__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "set_seed(seed)\n",
    "for omic_type in include_omics:\n",
    "    # Read the file\n",
    "    omics_data_dict[omic_type] = pd.read_csv(omics_files[omic_type], sep='\\t', index_col=0)\n",
    "\n",
    "    if test is True:\n",
    "        # Specify the index as needed\n",
    "        omics_data_dict[omic_type] = omics_data_dict[omic_type][:76]  # Adjust the row selection as needed\n",
    "    omics_data_tensor_dict[omic_type]  = torch.tensor(omics_data_dict[omic_type].values, dtype=torch.float32)\n",
    "    omics_numfeatures_dict[omic_type] = omics_data_tensor_dict[omic_type].shape[1]\n",
    "    print(f\"{omic_type} tensor shape:\", omics_data_tensor_dict[omic_type].shape)\n",
    "    print(f\"{omic_type} num_features\",omics_numfeatures_dict[omic_type])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "#load data\n",
    "drug_df= pd.read_csv(\"../data/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/MACCS(Secondary_Screen_treatment_info)_union_NOrepeat.csv\", sep=',', index_col=0)\n",
    "AUC_df = pd.read_csv(\"../data/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/Drug_sensitivity_AUC_(PRISM_Repurposing_Secondary_Screen)_subsetted_NOrepeat.csv\", sep=',', index_col=0)\n",
    "\n",
    "matched_samples = sorted(set(AUC_df.T.columns) & set(list(omics_data_dict.values())[0].T.columns))\n",
    "\n",
    "AUC_df= (AUC_df.T[matched_samples]).T\n",
    "if AUCtransform == \"-log2\":\n",
    "    AUC_df = -np.log2(AUC_df)\n",
    "if AUCtransform == \"-log10\":\n",
    "    AUC_df = -np.log10(AUC_df)\n",
    "    \n",
    "if test is True:\n",
    "    batch_size = 3\n",
    "    num_epoch = 2\n",
    "    print(\"batch_size\",batch_size,\"num_epoch:\",num_epoch)\n",
    "    drug_df=drug_df[:42]\n",
    "    AUC_df=AUC_df.iloc[:76,:42]\n",
    "    print(\"drug_df\",drug_df.shape)\n",
    "    print(\"AUC_df\",AUC_df.shape)\n",
    "    kfoldCV = 2\n",
    "    print(\"kfoldCV\",kfoldCV)\n",
    "\n",
    "if 'weighted' in criterion.loss_type :    \n",
    "    weighted_threshold = np.nanpercentile(AUC_df.values, 90)    \n",
    "    total_samples = (~np.isnan(AUC_df.values)).sum().item()\n",
    "    few_samples = (AUC_df.values > weighted_threshold).sum().item()\n",
    "    more_samples = total_samples - few_samples\n",
    "    few_weight = total_samples / (2 * few_samples)  \n",
    "    more_weight = total_samples / (2 * more_samples)   \n",
    "else:\n",
    "    weighted_threshold = None\n",
    "    few_weight = None\n",
    "    more_weight = None\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# 檢查有無重複的SMILES\n",
    "if ESPF is True:\n",
    "    drug_smiles =drug_df[\"smiles\"] # \n",
    "    drug_names =drug_df.index\n",
    "    # 挑出重複的SMILES\n",
    "    duplicate =  drug_smiles[drug_smiles.duplicated(keep=False)]\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------\n",
    "    #ESPF\n",
    "    vocab_path = \"./ESPF/drug_codes_chembl_freq_1500.txt\" # token\n",
    "    sub_csv = pd.read_csv(\"./ESPF/subword_units_map_chembl_freq_1500.csv\")# token with frequency\n",
    "\n",
    "    drug_encode = pd.Series(drug_smiles).apply(drug2emb_encoder, args=(vocab_path, sub_csv, max_drug_len))\n",
    "\n",
    "    drug_features_tensor = torch.tensor(np.array(drug_encode.values.tolist()), dtype=torch.long)\n",
    "else:\n",
    "    drug_encode = drug_df[\"MACCS166bits\"]\n",
    "    drug_encode_list = [list(map(int, item.split(','))) for item in drug_encode.values]\n",
    "    print(\"MACCS166bits_drug_encode_list type: \",type(drug_encode_list))\n",
    "    # Convert your data to tensors if they're in numpy\n",
    "    drug_features_tensor = torch.tensor(np.array(drug_encode_list), dtype=torch.long)\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "num_ccl = list(omics_data_dict.values())[0].shape[0]\n",
    "num_drug = drug_encode.shape[0]\n",
    "print(\"num_ccl,num_drug: \",num_ccl,num_drug)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Convert your data to tensors if they're in numpy\n",
    "response_matrix_tensor = torch.tensor(AUC_df.values, dtype=torch.float32)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# randomly split\n",
    "# 90% for training(10% for validation) and 10% for testing\n",
    "id_unrepeat_test, id_unrepeat_train_val = split_id(num_ccl,num_drug,splitType,kfoldCV,repeat=True)\n",
    "# repeat the test id\n",
    "if splitType == \"byCCL\":\n",
    "    repeatNum = num_drug\n",
    "elif splitType == \"byDrug\":\n",
    "    repeatNum = num_ccl\n",
    "id_test = repeat_func(id_unrepeat_test, repeatNum, setname='test')   \n",
    "\n",
    "\n",
    "#create dataset\n",
    "set_seed(seed)\n",
    "dataset = OmicsDrugDataset(omics_data_tensor_dict, drug_features_tensor, response_matrix_tensor, splitType, include_omics)\n",
    "\n",
    "test_dataset = Subset(dataset, id_test.tolist())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# k-fold run\n",
    "kfold_losses= {}\n",
    "best_fold_train_epoch_loss_list = []#  for train every epoch loss plot (best_fold)\n",
    "best_fold_val_epoch_loss_list = []#  for validation every epoch loss plot (best_fold)\n",
    "best_test_loss = float('inf')\n",
    "best_fold_best_weight=None\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_training\"):\n",
    "\n",
    "        # Define the K-fold Cross Validator\n",
    "        kfold = KFold(n_splits=kfoldCV, shuffle=True, random_state=seed) #shuffle the order of split subset\n",
    "        for fold, (id_unrepeat_train, id_unrepeat_val) in enumerate(kfold.split(id_unrepeat_train_val)):\n",
    "            print(f'FOLD {fold}')\n",
    "            print('--------------------------------------------------------------')\n",
    "            print(id_unrepeat_train.shape,id_unrepeat_val.shape)\n",
    "            # correct the id \n",
    "            id_unrepeat_train = np.array(id_unrepeat_train_val)[id_unrepeat_train.tolist()]\n",
    "            id_unrepeat_val = np.array(id_unrepeat_train_val)[id_unrepeat_val.tolist()]\n",
    "            # repeat the id \n",
    "            id_train = repeat_func(id_unrepeat_train, repeatNum, setname='train')\n",
    "            id_val = repeat_func(id_unrepeat_val, repeatNum, setname='val')\n",
    "\n",
    "            set_seed(seed)\n",
    "            train_dataset = Subset(dataset, id_train.tolist())# create dataset\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "            val_dataset = Subset(dataset, id_val.tolist())\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "            # train\n",
    "            # Init the neural network \n",
    "            set_seed(seed)\n",
    "            model = Omics_DrugESPF_Model(omics_encode_dim_dict, drug_encode_dims, activation_func, activation_func_final, dense_layer_dim, device, ESPF, Drug_SelfAttention, pos_emb_type,\n",
    "                                    drug_embedding_feature_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeatures_dict, max_drug_len,\n",
    "                                    TCGA_pretrain_weight_path_dict= TCGA_pretrain_weight_path_dict)\n",
    "            model.to(device=device)\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)# Initialize optimizer\n",
    "\n",
    "        \n",
    "            best_epoch, best_weight, best_val_loss, train_epoch_loss_list, val_epoch_loss_list,best_val_epoch_train_loss,best_epoch_attention_score_matrix , gradient_fig,gradient_norms_list = train( model,\n",
    "                optimizer,      batch_size,      num_epoch,      patience,      warmup_iters,      Decrease_percent,    continuous,\n",
    "                learning_rate,      criterion,      train_loader,      val_loader,\n",
    "                device,ESPF,Drug_SelfAttention, seed, kfoldCV ,weighted_threshold, few_weight, more_weight, TrackGradient)\n",
    "\n",
    "            print(\"best Epoch : \",best_epoch,\"best_val_loss : \",best_val_loss,\"best_val_epoch_train_loss : \",best_val_epoch_train_loss,\" batch_size : \",batch_size,\n",
    "                    \"learning_rate : \",learning_rate,\" warmup_iters :\" ,warmup_iters  ,\" with Decrease_percent : \",Decrease_percent )\n",
    "\n",
    "            kfold_losses[fold] = {\n",
    "            'train': best_val_epoch_train_loss,  # Train loss in best Validation epoch\n",
    "            'val': best_val_loss,  # best epoch\n",
    "            'test': None  # Placeholder for test loss\n",
    "            }   \n",
    "            # Evaluation on the test set for each fold's best model to pick the best fold for later inference\n",
    "            model.load_state_dict(best_weight)  \n",
    "            model.to(device=device)\n",
    "            \n",
    "            test_loss,_,_,_ = evaluation(model, None, criterion, test_loader, device,ESPF, Drug_SelfAttention,weighted_threshold, few_weight, more_weight, correlation='test')\n",
    "\n",
    "            break\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
