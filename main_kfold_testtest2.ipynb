{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfoldCV 2\n",
      "criterion: Custom_LossFunction(loss_type=MSE, loss_lambda=1.0, regular_type=None, regular_lambda=0.001)\n",
      "/root/DeepTTA\n",
      "Training on device cuda.\n",
      "batch_size 3 num_epoch: 2\n",
      "Exp tensor shape: torch.Size([76, 4692])\n",
      "Exp num_features 4692\n",
      "drug_df (1440, 9)\n",
      "AUC_df (480, 1440)\n",
      "76\n",
      "AUC_df (76, 1440)\n",
      "drug_df (42, 9)\n",
      "AUC_df (76, 42)\n"
     ]
    }
   ],
   "source": [
    "#main_kfold\n",
    "import argparse\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader, Subset\n",
    "import torch.nn.init as init\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import importlib.util\n",
    "import time\n",
    "\n",
    "from utils.ESPF_drug2emb import drug2emb_encoder\n",
    "from utils.Model import Omics_DrugESPF_Model\n",
    "from utils.split_data_id import split_id,repeat_func\n",
    "from utils.create_dataloader import OmicsDrugDataset\n",
    "from utils.train import train, evaluation\n",
    "from utils.correlation import correlation_func\n",
    "from utils.plot import loss_curve, correlation_density,Density_Plot_of_AUC_Values\n",
    "from utils.tools import get_data_value_range,set_seed,get_vram_usage\n",
    "from utils.Metrics import MetricsCalculator\n",
    "from utils.config import *\n",
    "\n",
    "\n",
    "print(\"kfoldCV\",kfoldCV)\n",
    "print(\"criterion:\",criterion)\n",
    "print(os.getcwd())\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "if test is True:\n",
    "    batch_size = 3\n",
    "    num_epoch = 2\n",
    "    print(\"batch_size\",batch_size,\"num_epoch:\",num_epoch)\n",
    "# print(\"include_omics\",include_omics)\n",
    "# print(\"dense_layer_dim\",dense_layer_dim)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "set_seed(seed)\n",
    "for omic_type in include_omics:\n",
    "    # Read the file\n",
    "    omics_data_dict[omic_type] = pd.read_csv(omics_files[omic_type], sep='\\t', index_col=0)\n",
    "\n",
    "    if test_dataset is True:\n",
    "        # Specify the index as needed\n",
    "        omics_data_dict[omic_type] = omics_data_dict[omic_type][:76]  # Adjust the row selection as needed\n",
    "    omics_data_tensor_dict[omic_type]  = torch.tensor(omics_data_dict[omic_type].values, dtype=torch.float32)\n",
    "    omics_numfeatures_dict[omic_type] = omics_data_tensor_dict[omic_type].shape[1]\n",
    "    print(f\"{omic_type} tensor shape:\", omics_data_tensor_dict[omic_type].shape)\n",
    "    print(f\"{omic_type} num_features\",omics_numfeatures_dict[omic_type])\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#load data\n",
    "# data_mut, gene_names_mut,ccl_names_mut  = load_ccl(\"/root/data/CCLE/CCLE_match_TCGAgene_PRISMandEXPsample_binary_mutation_476_6009.txt\")\n",
    "drug_df= pd.read_csv(\"../data/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/MACCS(Secondary_Screen_treatment_info)_union_NOrepeat.csv\", sep=',', index_col=0)\n",
    "AUC_df = pd.read_csv(\"../data/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/Drug_sensitivity_AUC_(PRISM_Repurposing_Secondary_Screen)_subsetted_NOrepeat.csv\", sep=',', index_col=0)\n",
    "# data_AUC_matrix, drug_names_AUC, ccl_names_AUC = load_AUC_matrix(splitType,\"/root/Winnie/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/Drug_sensitivity_AUC_(PRISM_Repurposing_Secondary_Screen)_subsetted.csv\") # splitType = \"byCCL\" or \"byDrug\" 決定AUCmatrix要不要轉置\n",
    "print(\"drug_df\",(np.shape(drug_df)))\n",
    "print(\"AUC_df\",np.shape(AUC_df))\n",
    "\n",
    "# matched AUCfile and omics_data samples\n",
    "matched_samples = sorted(set(AUC_df.T.columns) & set(list(omics_data_dict.values())[0].T.columns))\n",
    "print(len(matched_samples))\n",
    "AUC_df= (AUC_df.T[matched_samples]).T\n",
    "print(\"AUC_df\",AUC_df.shape)\n",
    "\n",
    "if test_dataset is True:\n",
    "    drug_df=drug_df[:42]\n",
    "    AUC_df=AUC_df.iloc[:76,:42]\n",
    "print(\"drug_df\",drug_df.shape)\n",
    "print(\"AUC_df\",AUC_df.shape)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_smiles.shape (42,)\n",
      "(drug_smiles.unique()).shape (42,)\n",
      "duplicate drug Series([], Name: smiles, dtype: object)\n",
      "(42,)\n",
      "(array([ 696,   61,  603,  472,  567,  302, 2058,   92,    2,   96,   63,\n",
      "       2363,  269,  744,  168,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]))\n",
      "num_ccl,num_drug:  76 42\n",
      "tensor([[[ 696,   61,  603,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 896,  452,  152,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 196, 1263,  402,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 145,  163, 1156,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[2379,  102,   89,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[  19,  625,  407,  ...,    0,    0,    0],\n",
      "         [   1,    1,    1,  ...,    0,    0,    0]]])\n",
      "cpu\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56834/3713116125.py:28: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(drug_encode[0])\n"
     ]
    }
   ],
   "source": [
    "# 檢查有無重複的SMILES\n",
    "\n",
    "if ESPF is True:\n",
    "    drug_smiles =drug_df[\"smiles\"] # \n",
    "    print(\"drug_smiles.shape\",drug_smiles.shape)\n",
    "    drug_names =drug_df.index\n",
    "    print(\"(drug_smiles.unique()).shape\",(drug_smiles.unique()).shape)\n",
    "    # 挑出重複的SMILES\n",
    "    duplicate =  drug_smiles[drug_smiles.duplicated(keep=False)]\n",
    "    print(\"duplicate drug\",duplicate)\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------\n",
    "    #ESPF\n",
    "    vocab_path = \"./dataset/ESPF/drug_codes_chembl_freq_1500.txt\" # token\n",
    "    sub_csv = pd.read_csv(\"./dataset/ESPF/subword_units_map_chembl_freq_1500.csv\")# token with frequency\n",
    "\n",
    "    # drug_encode = pd.Series(drug_smiles.unique()).apply(drug2emb_encoder, args=(vocab_path, sub_csv, max_drug_len))# 將drug_smiles 使用_drug2emb_encoder function編碼成subword vector\n",
    "    drug_encode = pd.Series(drug_smiles).apply(drug2emb_encoder, args=(vocab_path, sub_csv, max_drug_len))\n",
    "    # uniq_smile_dict = dict(zip(drug_smiles.unique(),drug_encode))# zip drug_smiles和其subword vector編碼 成字典\n",
    "\n",
    "    # print(type(smile_encode))\n",
    "    # print(smile_encode.shape)\n",
    "    # print(type(smile_encode.index))\n",
    "    print((drug_encode.index.values).shape)\n",
    "\n",
    "else:\n",
    "    drug_encode = drug_df[\"MACCS166bits\"]\n",
    "print(drug_encode[0])\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "num_ccl = list(omics_data_dict.values())[0].shape[0]\n",
    "num_drug = drug_encode.shape[0]\n",
    "print(\"num_ccl,num_drug: \",num_ccl,num_drug)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Convert your data to tensors if they're in numpy\n",
    "drug_features_tensor = torch.tensor(np.array(drug_encode.values.tolist()), dtype=torch.long)\n",
    "response_matrix_tensor = torch.tensor(AUC_df.values, dtype=torch.float32)\n",
    "print(drug_features_tensor)\n",
    "print(drug_features_tensor.device)\n",
    "print(response_matrix_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), True, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device,ESPF,Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 223, 1575,   83,  886, 1135,   43,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0],\n",
      "         [   1,    1,    1,    1,    1,    1,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0]],\n",
      "\n",
      "        [[  60,  321,   80,  459,  106,  321,  216, 2577,  179,  241,  880,\n",
      "            99,  241,  139,  238, 1034,   96,  124,   54,  434,   29,  342,\n",
      "           462, 1494,   54, 2296,   61, 1034,  202,   43,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0],\n",
      "         [   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "             1,    1,    1,    1,    1,    1,    1,    1,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0]],\n",
      "\n",
      "        [[ 196,  448,   65, 1648, 1759,  201,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0],\n",
      "         [   1,    1,    1,    1,    1,    1,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0]]])\n",
      "drug cpu\n",
      "{'Mut': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "omics_tensor_dict['Mut'] cpu\n"
     ]
    }
   ],
   "source": [
    "for batch_idx,inputs in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    omics_tensor_dict,drug = inputs[0],inputs[1]\n",
    "    target = inputs[2].to(device=device)\n",
    "    print(drug)\n",
    "    print(\"drug\",drug.device)\n",
    "    print(omics_tensor_dict)\n",
    "    print(\"omics_tensor_dict['Mut']\",omics_tensor_dict['Mut'].device)\n",
    "    outputs,attention_score_matrix = model(omics_tensor_dict, drug, device,ESPF,Transformer) #drug.to(torch.float32)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_unrepeat_train (60,)\n",
      "id_unrepeat_val (8,)\n",
      "id_unrepeat_test (8,)\n",
      "id_unrepeat_train_val (68,)\n",
      "id_test.shape (336,)\n",
      "FOLD 0\n",
      "--------------------------------------------------------------\n",
      "(34,) (34,)\n",
      "id_train.shape (1428,)\n",
      "id_val.shape (1428,)\n",
      "State_dict for Sequential(\n",
      "  (0): Linear(in_features=6009, out_features=1000, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=100, out_features=50, bias=True)\n",
      ") loaded successfully.\n",
      "\n",
      " Transformer is applied \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     62\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\u001b[38;5;66;03m# Initialize optimizer\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m best_epoch, best_weight, best_val_loss, train_epoch_loss_list, val_epoch_loss_list,best_val_epoch_train_loss,attention_score_matrix , gradient_fig,gradient_norms_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mwarmup_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mDecrease_percent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[43mcontinuous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mESPF\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkfoldCV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest Epoch : \u001b[39m\u001b[38;5;124m\"\u001b[39m,best_epoch,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_val_loss : \u001b[39m\u001b[38;5;124m\"\u001b[39m,best_val_loss,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_val_epoch_train_loss : \u001b[39m\u001b[38;5;124m\"\u001b[39m,best_val_epoch_train_loss,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m batch_size : \u001b[39m\u001b[38;5;124m\"\u001b[39m,batch_size,\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate : \u001b[39m\u001b[38;5;124m\"\u001b[39m,learning_rate,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m warmup_iters :\u001b[39m\u001b[38;5;124m\"\u001b[39m ,warmup_iters  ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with Decrease_percent : \u001b[39m\u001b[38;5;124m\"\u001b[39m,Decrease_percent )\n\u001b[1;32m     72\u001b[0m kfold_losses[fold] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: best_val_epoch_train_loss,  \u001b[38;5;66;03m# Train loss in best Validation epoch\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: best_val_loss,  \u001b[38;5;66;03m# best epoch\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Placeholder for test loss\u001b[39;00m\n\u001b[1;32m     76\u001b[0m }   \n",
      "File \u001b[0;32m~/DeepTTA/utils/train.py:160\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, batch_size, num_epoch, patience, warmup_iters, Decrease_percent, continuous, learning_rate, criterion, train_loader, val_loader, device, ESPF, Transformer, seed, kfoldCV)\u001b[0m\n\u001b[1;32m    158\u001b[0m total_train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    159\u001b[0m batch_idx_without_nan_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# if a batch has [] empty list than don't count\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx,inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    161\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    162\u001b[0m     omics_tensor_dict,drug \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m],inputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# randomly split\n",
    "# 90% for training(10% for validation) and 10% for testing\n",
    "if kfoldCV>1:\n",
    "    id_unrepeat_test, id_unrepeat_train_val = split_id(num_ccl,num_drug,splitType,kfoldCV,repeat=True)\n",
    "    # repeat the test id\n",
    "    if splitType == \"byCCL\":\n",
    "        repeatNum = num_drug\n",
    "    elif splitType == \"byDrug\":\n",
    "        repeatNum = num_ccl\n",
    "    id_test = repeat_func(id_unrepeat_test, repeatNum, setname='test')   \n",
    "elif kfoldCV==1:\n",
    "    id_unrepeat_train, id_unrepeat_val, id_unrepeat_test, _ , id_train, id_val, id_test= split_id(num_ccl,num_drug,splitType,kfoldCV,repeat=True)\n",
    "else:\n",
    "    print(\"Wrong assignment for kfoldCV\")\n",
    "\n",
    "#create dataset\n",
    "set_seed(seed)\n",
    "dataset = OmicsDrugDataset(omics_data_tensor_dict, drug_features_tensor, response_matrix_tensor, splitType, include_omics)\n",
    "\n",
    "test_dataset = Subset(dataset, id_test.tolist())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# k-fold run\n",
    "kfold_losses= {}\n",
    "best_fold_train_epoch_loss_list = []#  for train every epoch loss plot (best_fold)\n",
    "best_fold_val_epoch_loss_list = []#  for validation every epoch loss plot (best_fold)\n",
    "best_test_loss = float('inf')\n",
    "best_fold_best_weight=None\n",
    "set_seed(seed)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "if kfoldCV>1:\n",
    "    kfold = KFold(n_splits=kfoldCV, shuffle=True, random_state=seed) #shuffle the order of split subset\n",
    "    for fold, (id_unrepeat_train, id_unrepeat_val) in enumerate(kfold.split(id_unrepeat_train_val)):\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------------------------------------')\n",
    "        print(id_unrepeat_train.shape,id_unrepeat_val.shape)\n",
    "        # correct the id \n",
    "        id_unrepeat_train = np.array(id_unrepeat_train_val)[id_unrepeat_train.tolist()]\n",
    "        id_unrepeat_val = np.array(id_unrepeat_train_val)[id_unrepeat_val.tolist()]\n",
    "        # repeat the id \n",
    "        id_train = repeat_func(id_unrepeat_train, repeatNum, setname='train')\n",
    "        id_val = repeat_func(id_unrepeat_val, repeatNum, setname='val')\n",
    "        \n",
    "        set_seed(seed)\n",
    "        train_dataset = Subset(dataset, id_train.tolist())# create dataset\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataset = Subset(dataset, id_val.tolist())\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # train\n",
    "        # Init the neural network \n",
    "        set_seed(seed)\n",
    "        model = Omics_DrugESPF_Model(omics_encode_dim_dict, drug_encode_dims, activation_func, activation_func_final, dense_layer_dim, device,\n",
    "                                drug_embedding_feature_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeatures_dict, max_drug_len,\n",
    "                                TCGA_pretrain_weight_path_dict= TCGA_pretrain_weight_path_dict)\n",
    "        model.to(device=device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)# Initialize optimizer\n",
    "\n",
    "        best_epoch, best_weight, best_val_loss, train_epoch_loss_list, val_epoch_loss_list,best_val_epoch_train_loss,attention_score_matrix , gradient_fig,gradient_norms_list = train( model,\n",
    "            optimizer,      batch_size,      num_epoch,      patience,      warmup_iters,      Decrease_percent,    continuous,\n",
    "            learning_rate,      criterion,      train_loader,      val_loader,\n",
    "            device,ESPF,Transformer, seed, kfoldCV)\n",
    "\n",
    "        print(\"best Epoch : \",best_epoch,\"best_val_loss : \",best_val_loss,\"best_val_epoch_train_loss : \",best_val_epoch_train_loss,\" batch_size : \",batch_size,\n",
    "                \"learning_rate : \",learning_rate,\" warmup_iters :\" ,warmup_iters  ,\" with Decrease_percent : \",Decrease_percent )\n",
    "\n",
    "        kfold_losses[fold] = {\n",
    "        'train': best_val_epoch_train_loss,  # Train loss in best Validation epoch\n",
    "        'val': best_val_loss,  # best epoch\n",
    "        'test': None  # Placeholder for test loss\n",
    "        }   \n",
    "        # Evaluation on the test set for each fold's best model to pick the best fold for later inference\n",
    "        model.load_state_dict(best_weight)  \n",
    "        model.to(device=device)\n",
    "        test_loss = evaluation(model, val_epoch_loss_list, criterion, test_loader, device,ESPF, Transformer,kfoldCV, correlation='plotLossCurve')\n",
    "        \n",
    "        kfold_losses[fold]['test'] = test_loss\n",
    "        # save best fold testing loss model weight\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_fold_train_epoch_loss_list = train_epoch_loss_list #  for train epoch loss plot\n",
    "            best_fold_val_epoch_loss_list = val_epoch_loss_list #  for validation epoch loss plot\n",
    "            best_fold_best_epoch = best_epoch #  for validation epoch loss plot\n",
    "            best_fold_best_val_loss = best_val_loss #  for validation epoch loss plot\n",
    "            best_fold_best_weight = copy.deepcopy(best_weight) # best fold best epoch \n",
    "            best_fold = fold\n",
    "            # best_fold_id_train= id_train\n",
    "            # best_fold_id_val= id_val\n",
    "            best_fold_id_unrepeat_train= id_unrepeat_train# for correlation\n",
    "            best_fold_id_unrepeat_val= id_unrepeat_val  \n",
    "        # print(\"best_fold_best_weight\",best_fold_best_weight[\"MLP4omics_dict.Mut.0.weight\"][0])    \n",
    "        del model\n",
    "        # Set the current device\n",
    "        torch.cuda.set_device(\"cuda:0\")\n",
    "        # Optionally, force garbage collection to release memory \n",
    "        gc.collect()\n",
    "        # Empty PyTorch cache\n",
    "        torch.cuda.empty_cache() # model 會從GPU消失，所以要evaluation時要重新load model\n",
    "    \n",
    "    print(f'K-FOLD CrossValidation {criterion.loss_type} loss By {model_name}')\n",
    "    print(f\"Best fold {best_fold} , {criterion.loss_type} train Loss: {(kfold_losses[best_fold]['train']):.7f}\")\n",
    "    print(f\"Best fold {best_fold} , {criterion.loss_type} val Loss: {best_fold_best_val_loss:.7f}\") # = (kfold_losses[best_fold]['val'])\n",
    "    print(f\"Best fold {best_fold} , {criterion.loss_type} test Loss: {best_test_loss:.7f}\") # = (kfold_losses[best_fold]['test'])\n",
    "    # Calculate mean and standard deviation of the all folds loss\n",
    "    for loss_type in ['train', 'val', 'test']:\n",
    "        Folds_losses = [fold_data[loss_type] for fold_data in kfold_losses.values()]\n",
    "        print(f\"KFolds Average {loss_type.capitalize()} {criterion.loss_type}: {np.mean(Folds_losses):.7f} ± {np.std(Folds_losses):.7f}\")\n",
    "    #模型的超參數比較的時候應該用平均值和標準差來比較，而不是單個fold的結果\n",
    "\n",
    "    # Saving the model weughts\n",
    "    hyperparameter_folder_path = f'./results/BestFold{best_fold}_test_loss{best_test_loss:.7f}_BestValEpo{best_fold_best_epoch}_{hyperparameter_folder_part}'\n",
    "    os.makedirs(hyperparameter_folder_path, exist_ok=True)\n",
    "    save_path = os.path.join(hyperparameter_folder_path, f'BestValWeight.pt')\n",
    "    torch.save(best_fold_best_weight, save_path)\n",
    "    #-----------------------------------------------------------------------------------------------------------------\n",
    "best_fold_train_epoch_loss_list = [float(value) if isinstance(value, (np.float32, np.float64)) else value for value in best_fold_train_epoch_loss_list]\n",
    "best_fold_val_epoch_loss_list = [float(value) if isinstance(value, (np.float32, np.float64)) else value for value in best_fold_val_epoch_loss_list]\n",
    "import json\n",
    "epoch_loss_dict = {\"best_fold_train_epoch_loss_list\": best_fold_train_epoch_loss_list, \"best_fold_val_epoch_loss_list\": best_fold_val_epoch_loss_list}\n",
    "json_data = json.dumps(epoch_loss_dict, indent=0)\n",
    "with open(f\"{hyperparameter_folder_path}/BestFold{best_fold}_epoch-loss.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "if gradient_norms_list:\n",
    "    with open(f\"{hyperparameter_folder_path}/BestFold{best_fold}_gradient-norms-list.txt\", \"w\") as txt_file:\n",
    "        txt_file.write(\"\\n\".join(map(str, gradient_norms_list)))\n",
    "if gradient_fig:\n",
    "    gradient_fig.savefig(f'{hyperparameter_folder_path}/BestFold{best_fold}_Gradient-Norms-Over-Epochs')\n",
    "    \n",
    "loss_curve(model_name, best_fold_train_epoch_loss_list, best_fold_val_epoch_loss_list, best_fold_best_epoch, best_fold_best_val_loss,hyperparameter_folder_path, ylim_top=0.035)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "#Evaluation on best fold best split id (train, val) with best_fold_best_weight \n",
    "best_fold_id_train = repeat_func(best_fold_id_unrepeat_train, repeatNum, setname='train')\n",
    "best_fold_id_val = repeat_func(best_fold_id_unrepeat_val, repeatNum, setname='val')\n",
    "set_seed(seed)\n",
    "train_dataset = Subset(dataset, best_fold_id_train.tolist())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = Subset(dataset, best_fold_id_val.tolist())\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# set_seed(seed)\n",
    "model = Omics_DrugESPF_Model(omics_encode_dim_dict, drug_encode_dims, activation_func, activation_func_final, dense_layer_dim, device,\n",
    "                        drug_embedding_feature_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeatures_dict, max_drug_len,\n",
    "                        TCGA_pretrain_weight_path_dict= TCGA_pretrain_weight_path_dict).to(device=device)\n",
    "num_param = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of parameter: %.2fK\" % (num_param/1e3))\n",
    "\n",
    "# Compute and print all metrics\n",
    "metrics_calculator = MetricsCalculator()\n",
    "# Evaluation on the train set\n",
    "model.load_state_dict(best_fold_best_weight)  \n",
    "model.to(device=device)\n",
    "train_loss, train_targets, train_outputs = evaluation(model, val_epoch_loss_list, criterion, train_loader, device,ESPF, Transformer,kfoldCV, correlation='train')\n",
    "train_metrics= metrics_calculator.compute_all_metrics(np.concatenate(train_targets), np.concatenate(train_outputs),set_name='train_set')\n",
    "metrics_calculator.print_results(set_name='train_set')\n",
    "# Evaluation on the validation set\n",
    "val_loss, val_targets, val_outputs = evaluation(model, val_epoch_loss_list, criterion, val_loader, device,ESPF, Transformer,kfoldCV, correlation='val')\n",
    "val_metrics= metrics_calculator.compute_all_metrics(np.concatenate(val_targets), np.concatenate(val_outputs),set_name='val_set')\n",
    "metrics_calculator.print_results(set_name='val_set')\n",
    "# Evaluation on the test set\n",
    "test_loss, test_targets, test_outputs = evaluation(model, val_epoch_loss_list, criterion, test_loader, device,ESPF, Transformer,kfoldCV, correlation='test')\n",
    "test_metrics= metrics_calculator.compute_all_metrics(np.concatenate(test_targets), np.concatenate(test_outputs),set_name='test_set')\n",
    "metrics_calculator.print_results(set_name='test_set')\n",
    "\n",
    "print(f\"compare id repeat or store {time.time() - start:.6f} seconds\")\n",
    "'''\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Correlation\n",
    "train_pearson, train_spearman = correlation_func(splitType, AUC_df.values,AUC_df.index,AUC_df.columns,best_fold_id_unrepeat_train,train_targets,train_outputs)\n",
    "# print(\"\\n\")\n",
    "# print(\"val set\"+\"=\"*20)\n",
    "# print(\"val set\"+\"=\"*20)\n",
    "val_pearson, val_spearman = correlation_func(splitType, AUC_df.values,AUC_df.index,AUC_df.columns,best_fold_id_unrepeat_val,val_targets,val_outputs)\n",
    "# print(\"\\n\")\n",
    "# print(\"test set\"+\"=\"*20)\n",
    "# print(\"test set\"+\"=\"*20)\n",
    "test_pearson, test_spearman = correlation_func(splitType, AUC_df.values,AUC_df.index,AUC_df.columns,id_unrepeat_test,test_targets,test_outputs)\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#plot correlation_density\n",
    "correlation_density(model_name,train_pearson,val_pearson,test_pearson,train_spearman,val_spearman,test_spearman, hyperparameter_folder_path)\n",
    "\n",
    "# plot GroundTruth AUC and predicted AUC distribution\n",
    "predicted_AUC = np.concatenate(train_outputs + val_outputs + test_outputs).tolist()\n",
    "# print(predicted_AUC[:10])\n",
    "print(\"predicted_AUC\",np.array(predicted_AUC).shape)\n",
    "GroundTruth_AUC = np.concatenate(train_targets + val_targets + test_targets).tolist()\n",
    "print(\"GroundTruth_AUC\",np.array(GroundTruth_AUC).shape)\n",
    "# print(GroundTruth_AUC[:10])\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "datas = [(train_targets, train_outputs, 'Train', 'red'),\n",
    "                (val_targets, val_outputs, 'Validation', 'green'),\n",
    "                (test_targets, test_outputs, 'Test', 'purple')]\n",
    "# plot Density_Plot_of_AUC_Values of train val test datasets\n",
    "Density_Plot_of_AUC_Values(datas,hyperparameter_folder_path)\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "output_file = f\"{hyperparameter_folder_path}/BestFold{best_fold}_result_performance.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    # data range\n",
    "    get_data_value_range(GroundTruth_AUC,\"GroundTruth_AUC\", file=file)\n",
    "    get_data_value_range(predicted_AUC,\"predicted_AUC\", file=file)\n",
    "    print('hyperparameter_print',hyperparameter_print, file=file)\n",
    "    print('best_fold_best_epoch: ',best_fold_best_epoch, file=file)\n",
    "    print('BestFold: ',best_fold, file=file)\n",
    "    print('----------------After Training-------------- ', file=file)\n",
    "    print(f\"Best fold {best_fold} , {criterion.loss_type} train Loss: {(kfold_losses[best_fold]['train']):.7f}\", file=file)\n",
    "    print(f\"Best fold {best_fold} , {criterion.loss_type} val Loss: {best_fold_best_val_loss:.7f}\", file=file) # = (kfold_losses[best_fold]['val'])\n",
    "    print(f\"Best fold {best_fold} , {criterion.loss_type} test Loss: {best_test_loss:.7f}\", file=file) # = (kfold_losses[best_fold]['test'])\n",
    "    # Calculate mean and standard deviation of the all folds loss\n",
    "    for loss_type in ['train', 'val', 'test']:\n",
    "        Folds_losses = [fold_data[loss_type] for fold_data in kfold_losses.values()]\n",
    "        print(f\"KFolds Average {loss_type.capitalize()} {criterion.loss_type}: {np.mean(Folds_losses):.6f} ± {np.std(Folds_losses):.6f}\", file=file)\n",
    "    #模型的超參數比較的時候應該用平均值和標準差來比較，而不是單個fold的結果\n",
    "    print('----------------After Evaluation-------------- ', file=file)\n",
    "    print(f'Evaluation Train Loss: {train_loss:.6f}', file=file)\n",
    "    print(f'Evaluation validation Loss: {val_loss:.6f}', file=file)\n",
    "    print(f'Evaluation Test Loss: {test_loss:.6f}', file=file)\n",
    "    # Metrics\n",
    "    for name, metrics in [(\"Train_set\", train_metrics),(\"val_set\", val_metrics),(\"test_set\", test_metrics)]:\n",
    "        for key, value in metrics.items():\n",
    "            if key != 'Evaluation':\n",
    "                print(f\"Metrics {name} {key} : {value:.6f}\", file=file)\n",
    "# Pearson and Spearman statistics\n",
    "    for name, pearson in [(\"Train\", train_pearson),\n",
    "                                    (\"Validation\", val_pearson),\n",
    "                                    (\"Test\", test_pearson)]:\n",
    "        print(f\"Mean {name} Pearson {model_name}: {np.mean(pearson):.6f}\", file=file)\n",
    "        print(f\"Median {name} Pearson {model_name}: {np.median(pearson):.6f}\", file=file)\n",
    "        print(f\"Mode {name} Pearson {model_name}: {stats.mode(np.round(pearson,2))[0]}, count={stats.mode(np.round(pearson,2))[1]}\", file=file)\n",
    "    for name, spearman in [(\"Train\", train_spearman),\n",
    "                                    (\"Validation\", val_spearman),\n",
    "                                    (\"Test\", test_spearman)]:\n",
    "        print(f\"Mean {name} Spearman {model_name}: {np.mean(spearman):.6f}\", file=file)\n",
    "        print(f\"Median {name} Spearman {model_name}: {np.median(spearman):.6f}\", file=file)\n",
    "        print(f\"Mode {name} Spearman {model_name}: {stats.mode(np.round(spearman,2))[0]}, count={stats.mode(np.round(spearman,2))[1]}\", file=file)\n",
    "    for name, pearson in [(\"Train\", train_pearson),\n",
    "                                    (\"Validation\", val_pearson),\n",
    "                                    (\"Test\", test_pearson)]:\n",
    "        print(f\"Mean Median Mode {name} Pearson {model_name}:\\t{np.mean(pearson):.6f}\\t{np.median(pearson):.6f}\\t{stats.mode(np.round(pearson,2))}\", file=file)\n",
    "    for name, spearman in [(\"Train\", train_spearman),\n",
    "                                    (\"Validation\", val_spearman),\n",
    "                                    (\"Test\", test_spearman)]:\n",
    "        print(f\"Mean Median Mode {name} Spearman {model_name}:\\t{np.mean(spearman):.6f}\\t{np.median(spearman):.6f}\\t{stats.mode(np.round(spearman,2))}\", file=file)\n",
    "    print(\"Output saved to:\", output_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(criterion.loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
