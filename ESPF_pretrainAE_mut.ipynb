{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdb568c-1ac2-42c9-8b80-f8777900b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "            else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571a0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information\n",
    "filename     = \"ESPF_PretrainAE_exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea8469b-addf-4351-85d0-81ccf2469ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "\n",
    "class AE_dense_3layers(nn.Module):\n",
    "    def __init__(self, input_dim, mut_encode_dim, activation_func):\n",
    "        super(AE_dense_3layers, self).__init__()##__init__初始化父類別的屬性和方法\n",
    "        print('input_dim = ', input_dim)\n",
    "        print('first_layer_dim = ', mut_encode_dim[0])\n",
    "        print('second_layer_dim = ', mut_encode_dim[1])\n",
    "        print('third_layer_dim = ', mut_encode_dim[2])\n",
    "        # nn.Module 自己將batch 裡的samples 拆開一個一個送入self.encoder nn.Sequential\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, mut_encode_dim[0]),\n",
    "            activation_func,\n",
    "            nn.Linear(mut_encode_dim[0], mut_encode_dim[1]),\n",
    "            activation_func,\n",
    "            nn.Linear(mut_encode_dim[1], mut_encode_dim[2]),\n",
    "            activation_func\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(mut_encode_dim[2], mut_encode_dim[1]),\n",
    "            activation_func,\n",
    "            nn.Linear(mut_encode_dim[1], mut_encode_dim[0]),\n",
    "            activation_func,\n",
    "            nn.Linear(mut_encode_dim[0], input_dim),\n",
    "            activation_func\n",
    "        )\n",
    "        # Initialize weights using He uniform initialization\n",
    "        for layer in [self.encoder[0], self.encoder[2], self.encoder[4], self.decoder[0], self.decoder[2], self.decoder[4]]:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):#加self才能呼叫self裡其他的屬性和method\n",
    "        x_encoded = self.encoder(x)\n",
    "        x_decoded = self.decoder(x_encoded)\n",
    "        return x_decoded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1529df-e3c1-4fcb-896f-ccdbd112f79f",
   "metadata": {},
   "source": [
    "## def warmup_lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3fb3a00-aaae-4bee-8d99-df3a0d194ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_scheduler\n",
    "def warmup_lr_scheduler(optimizer, warmup_iters,Decrease_percent):\n",
    "    def f(epoch):\n",
    "        if epoch >= warmup_iters:\n",
    "          return Decrease_percent ** (epoch-warmup_iters+1)\n",
    "        return 1\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acebfb61-8400-48ce-916e-9e9a21f6d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function to set the seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# Set the seed\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "635fdbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Datasets successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# load TCGA mutation data, substitute here with other genomics\n",
    "data_mut_tcga= pd.read_csv(\"../data/TCGA/TCGA_exp_matchCCLEgenes_8238samples_4692genes.txt\", sep='\\t', index_col=0)\n",
    "\n",
    "print(\"\\n\\nDatasets successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ab1ae7-7b89-4269-a15a-5b5e7af1e51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8238, 4692)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mut_tcga.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8f33e0-e21f-4b4c-8bf2-3569e9700b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #exchange the row and column to match the format of doctor chiu's data (row is gene ;column is cancer)\n",
    "# data_mut_tcga=data_mut_tcga.transpose()\n",
    "# data_mut_tcga.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0151a3-f0e7-445a-aa74-0e117bba5246",
   "metadata": {},
   "source": [
    "## 也許應該要分train validation set，這樣才能更學到真正重要的特徵，而不是背答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01c799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim =  4692\n",
      "first_layer_dim =  1000\n",
      "second_layer_dim =  100\n",
      "third_layer_dim =  50\n",
      "Epoch 1/300 - Train Loss: 5855.17918555\n",
      "Epoch 1/300 - Test Loss: 14.61074477\n",
      "lr of epoch 1 => [0.01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300 - Train Loss: 9.79639777\n",
      "Epoch 2/300 - Test Loss: 6.60033567\n",
      "lr of epoch 2 => [0.01]\n",
      "Epoch 3/300 - Train Loss: 6.63009643\n",
      "Epoch 3/300 - Test Loss: 6.60052602\n",
      "lr of epoch 3 => [0.01]\n",
      "Epoch 4/300 - Train Loss: 6.62921285\n",
      "Epoch 4/300 - Test Loss: 6.59884704\n",
      "lr of epoch 4 => [0.01]\n",
      "Epoch 5/300 - Train Loss: 6.62834160\n",
      "Epoch 5/300 - Test Loss: 6.59523124\n",
      "lr of epoch 5 => [0.01]\n",
      "Epoch 6/300 - Train Loss: 6.62781306\n",
      "Epoch 6/300 - Test Loss: 6.59944272\n",
      "lr of epoch 6 => [0.01]\n",
      "Epoch 7/300 - Train Loss: 6.61783409\n",
      "Epoch 7/300 - Test Loss: 6.58560021\n",
      "lr of epoch 7 => [0.01]\n",
      "Epoch 8/300 - Train Loss: 6.61905097\n",
      "Epoch 8/300 - Test Loss: 6.59662813\n",
      "lr of epoch 8 => [0.01]\n",
      "Epoch 9/300 - Train Loss: 6.61971090\n",
      "Epoch 9/300 - Test Loss: 6.58845153\n",
      "lr of epoch 9 => [0.01]\n",
      "Epoch 10/300 - Train Loss: 6.61735777\n",
      "Epoch 10/300 - Test Loss: 6.59171297\n",
      "lr of epoch 10 => [0.01]\n",
      "Epoch 11/300 - Train Loss: 6.61695895\n",
      "Epoch 11/300 - Test Loss: 6.58675302\n",
      "lr of epoch 11 => [0.01]\n",
      "Epoch 12/300 - Train Loss: 6.61556445\n",
      "Epoch 12/300 - Test Loss: 6.58518806\n",
      "lr of epoch 12 => [0.01]\n",
      "Epoch 13/300 - Train Loss: 6.61398630\n",
      "Epoch 13/300 - Test Loss: 6.58688103\n",
      "lr of epoch 13 => [0.01]\n",
      "Epoch 14/300 - Train Loss: 6.61193419\n",
      "Epoch 14/300 - Test Loss: 6.58221813\n",
      "lr of epoch 14 => [0.01]\n",
      "Epoch 15/300 - Train Loss: 6.61064278\n",
      "Epoch 15/300 - Test Loss: 6.58867286\n",
      "lr of epoch 15 => [0.01]\n",
      "Epoch 16/300 - Train Loss: 6.60934022\n",
      "Epoch 16/300 - Test Loss: 6.57646179\n",
      "lr of epoch 16 => [0.01]\n",
      "Epoch 17/300 - Train Loss: 6.60828568\n",
      "Epoch 17/300 - Test Loss: 6.57709296\n",
      "lr of epoch 17 => [0.01]\n",
      "Epoch 18/300 - Train Loss: 6.60781429\n",
      "Epoch 18/300 - Test Loss: 6.58723299\n",
      "lr of epoch 18 => [0.01]\n",
      "Epoch 19/300 - Train Loss: 6.60668979\n",
      "Epoch 19/300 - Test Loss: 6.58050192\n",
      "lr of epoch 19 => [0.01]\n",
      "Epoch 20/300 - Train Loss: 6.60360140\n",
      "Epoch 20/300 - Test Loss: 6.57342227\n",
      "lr of epoch 20 => [0.01]\n",
      "Epoch 21/300 - Train Loss: 6.60317637\n",
      "Epoch 21/300 - Test Loss: 6.56597878\n",
      "lr of epoch 21 => [0.01]\n",
      "Epoch 22/300 - Train Loss: 6.59898793\n",
      "Epoch 22/300 - Test Loss: 6.57000441\n",
      "lr of epoch 22 => [0.01]\n",
      "Epoch 23/300 - Train Loss: 6.59796993\n",
      "Epoch 23/300 - Test Loss: 6.56811729\n",
      "lr of epoch 23 => [0.01]\n",
      "Epoch 24/300 - Train Loss: 6.59663711\n",
      "Epoch 24/300 - Test Loss: 6.56880742\n",
      "lr of epoch 24 => [0.01]\n",
      "Epoch 25/300 - Train Loss: 6.59560599\n",
      "Epoch 25/300 - Test Loss: 6.56340417\n",
      "lr of epoch 25 => [0.01]\n",
      "Epoch 26/300 - Train Loss: 6.59380110\n",
      "Epoch 26/300 - Test Loss: 6.56423525\n",
      "lr of epoch 26 => [0.01]\n",
      "Epoch 27/300 - Train Loss: 6.59389866\n",
      "Epoch 27/300 - Test Loss: 6.56220726\n",
      "lr of epoch 27 => [0.01]\n",
      "Epoch 28/300 - Train Loss: 6.59507145\n",
      "Epoch 28/300 - Test Loss: 6.56907837\n",
      "lr of epoch 28 => [0.01]\n",
      "Epoch 29/300 - Train Loss: 6.59467608\n",
      "Epoch 29/300 - Test Loss: 6.56285192\n",
      "lr of epoch 29 => [0.01]\n",
      "Epoch 30/300 - Train Loss: 6.59287599\n",
      "Epoch 30/300 - Test Loss: 6.55905190\n",
      "lr of epoch 30 => [0.01]\n",
      "Epoch 31/300 - Train Loss: 6.58974788\n",
      "Epoch 31/300 - Test Loss: 6.55928793\n",
      "lr of epoch 31 => [0.01]\n",
      "Epoch 32/300 - Train Loss: 6.58852530\n",
      "Epoch 32/300 - Test Loss: 6.55844811\n",
      "lr of epoch 32 => [0.01]\n",
      "Epoch 33/300 - Train Loss: 6.58969153\n",
      "Epoch 33/300 - Test Loss: 6.56134734\n",
      "lr of epoch 33 => [0.01]\n",
      "Epoch 34/300 - Train Loss: 6.58925809\n",
      "Epoch 34/300 - Test Loss: 6.56353923\n",
      "lr of epoch 34 => [0.01]\n",
      "Epoch 35/300 - Train Loss: 6.58953393\n",
      "Epoch 35/300 - Test Loss: 6.55980752\n",
      "lr of epoch 35 => [0.01]\n",
      "Epoch 36/300 - Train Loss: 6.58719842\n",
      "Epoch 36/300 - Test Loss: 6.56476081\n",
      "lr of epoch 36 => [0.01]\n",
      "Epoch 37/300 - Train Loss: 6.56387073\n",
      "Epoch 37/300 - Test Loss: 6.53377982\n",
      "lr of epoch 37 => [0.01]\n",
      "Epoch 38/300 - Train Loss: 6.56059205\n",
      "Epoch 38/300 - Test Loss: 6.53093325\n",
      "lr of epoch 38 => [0.01]\n",
      "Epoch 39/300 - Train Loss: 6.55834017\n",
      "Epoch 39/300 - Test Loss: 6.52986435\n",
      "lr of epoch 39 => [0.01]\n",
      "Epoch 40/300 - Train Loss: 6.55865796\n",
      "Epoch 40/300 - Test Loss: 6.53153970\n",
      "lr of epoch 40 => [0.01]\n",
      "Epoch 41/300 - Train Loss: 6.55779845\n",
      "Epoch 41/300 - Test Loss: 6.53133895\n",
      "lr of epoch 41 => [0.01]\n",
      "Epoch 42/300 - Train Loss: 6.55765283\n",
      "Epoch 42/300 - Test Loss: 6.53202967\n",
      "lr of epoch 42 => [0.01]\n",
      "Epoch 43/300 - Train Loss: 6.55780206\n",
      "Epoch 43/300 - Test Loss: 6.53451076\n",
      "lr of epoch 43 => [0.01]\n",
      "Epoch 44/300 - Train Loss: 6.55793150\n",
      "Epoch 44/300 - Test Loss: 6.52926733\n",
      "lr of epoch 44 => [0.01]\n",
      "Epoch 45/300 - Train Loss: 6.55906614\n",
      "Epoch 45/300 - Test Loss: 6.52741885\n",
      "lr of epoch 45 => [0.01]\n",
      "Epoch 46/300 - Train Loss: 6.55832708\n",
      "Epoch 46/300 - Test Loss: 6.53027954\n",
      "lr of epoch 46 => [0.01]\n",
      "Epoch 47/300 - Train Loss: 6.55873884\n",
      "Epoch 47/300 - Test Loss: 6.52883260\n",
      "lr of epoch 47 => [0.01]\n",
      "Epoch 48/300 - Train Loss: 6.55779134\n",
      "Epoch 48/300 - Test Loss: 6.53167235\n",
      "lr of epoch 48 => [0.01]\n",
      "Epoch 49/300 - Train Loss: 6.55816557\n",
      "Epoch 49/300 - Test Loss: 6.53122907\n",
      "lr of epoch 49 => [0.01]\n",
      "Epoch 50/300 - Train Loss: 6.55851615\n",
      "Epoch 50/300 - Test Loss: 6.52730267\n",
      "lr of epoch 50 => [0.01]\n",
      "Epoch 51/300 - Train Loss: 6.55843619\n",
      "Epoch 51/300 - Test Loss: 6.53170367\n",
      "lr of epoch 51 => [0.01]\n",
      "Epoch 52/300 - Train Loss: 6.55602653\n",
      "Epoch 52/300 - Test Loss: 6.52453105\n",
      "lr of epoch 52 => [0.01]\n",
      "Epoch 53/300 - Train Loss: 6.55622528\n",
      "Epoch 53/300 - Test Loss: 6.52468478\n",
      "lr of epoch 53 => [0.01]\n",
      "Epoch 54/300 - Train Loss: 6.55555819\n",
      "Epoch 54/300 - Test Loss: 6.52498964\n",
      "lr of epoch 54 => [0.01]\n",
      "Epoch 55/300 - Train Loss: 6.55514014\n",
      "Epoch 55/300 - Test Loss: 6.52630624\n",
      "lr of epoch 55 => [0.01]\n",
      "Epoch 56/300 - Train Loss: 6.55657080\n",
      "Epoch 56/300 - Test Loss: 6.52636106\n",
      "lr of epoch 56 => [0.01]\n",
      "Epoch 57/300 - Train Loss: 6.55483360\n",
      "Epoch 57/300 - Test Loss: 6.52600943\n",
      "lr of epoch 57 => [0.01]\n",
      "Epoch 58/300 - Train Loss: 6.55546959\n",
      "Epoch 58/300 - Test Loss: 6.52820952\n",
      "lr of epoch 58 => [0.01]\n",
      "Epoch 59/300 - Train Loss: 6.55678322\n",
      "Epoch 59/300 - Test Loss: 6.52580314\n",
      "lr of epoch 59 => [0.01]\n",
      "Epoch 60/300 - Train Loss: 6.55365085\n",
      "Epoch 60/300 - Test Loss: 6.52450068\n",
      "lr of epoch 60 => [0.01]\n",
      "Epoch 61/300 - Train Loss: 6.55276489\n",
      "Epoch 61/300 - Test Loss: 6.52530382\n",
      "lr of epoch 61 => [0.01]\n",
      "Epoch 62/300 - Train Loss: 6.55148045\n",
      "Epoch 62/300 - Test Loss: 6.52197346\n",
      "lr of epoch 62 => [0.01]\n",
      "Epoch 63/300 - Train Loss: 6.55289981\n",
      "Epoch 63/300 - Test Loss: 6.52283085\n",
      "lr of epoch 63 => [0.01]\n",
      "Epoch 64/300 - Train Loss: 6.54897417\n",
      "Epoch 64/300 - Test Loss: 6.52013513\n",
      "lr of epoch 64 => [0.01]\n",
      "Epoch 65/300 - Train Loss: 6.54953676\n",
      "Epoch 65/300 - Test Loss: 6.51933426\n",
      "lr of epoch 65 => [0.01]\n",
      "Epoch 66/300 - Train Loss: 6.55098689\n",
      "Epoch 66/300 - Test Loss: 6.53067323\n",
      "lr of epoch 66 => [0.01]\n",
      "Epoch 67/300 - Train Loss: 6.55003578\n",
      "Epoch 67/300 - Test Loss: 6.51967518\n",
      "lr of epoch 67 => [0.01]\n",
      "Epoch 68/300 - Train Loss: 6.55033396\n",
      "Epoch 68/300 - Test Loss: 6.52472956\n",
      "lr of epoch 68 => [0.01]\n",
      "Epoch 69/300 - Train Loss: 6.54900898\n",
      "Epoch 69/300 - Test Loss: 6.52404174\n",
      "lr of epoch 69 => [0.01]\n",
      "Epoch 70/300 - Train Loss: 6.54822712\n",
      "Epoch 70/300 - Test Loss: 6.51920104\n",
      "lr of epoch 70 => [0.01]\n",
      "Epoch 71/300 - Train Loss: 6.54712395\n",
      "Epoch 71/300 - Test Loss: 6.51374278\n",
      "lr of epoch 71 => [0.01]\n",
      "Epoch 72/300 - Train Loss: 6.54490472\n",
      "Epoch 72/300 - Test Loss: 6.51672484\n",
      "lr of epoch 72 => [0.01]\n",
      "Epoch 73/300 - Train Loss: 6.54116660\n",
      "Epoch 73/300 - Test Loss: 6.51166052\n",
      "lr of epoch 73 => [0.01]\n",
      "Epoch 74/300 - Train Loss: 6.54048735\n",
      "Epoch 74/300 - Test Loss: 6.50773373\n",
      "lr of epoch 74 => [0.01]\n",
      "Epoch 75/300 - Train Loss: 6.53820489\n",
      "Epoch 75/300 - Test Loss: 6.50901794\n",
      "lr of epoch 75 => [0.01]\n",
      "Epoch 76/300 - Train Loss: 6.53755794\n",
      "Epoch 76/300 - Test Loss: 6.50957735\n",
      "lr of epoch 76 => [0.01]\n",
      "Epoch 77/300 - Train Loss: 6.53675876\n",
      "Epoch 77/300 - Test Loss: 6.50906157\n",
      "lr of epoch 77 => [0.01]\n",
      "Epoch 78/300 - Train Loss: 6.53710139\n",
      "Epoch 78/300 - Test Loss: 6.51052513\n",
      "lr of epoch 78 => [0.01]\n",
      "Epoch 79/300 - Train Loss: 6.53697612\n",
      "Epoch 79/300 - Test Loss: 6.50745177\n",
      "lr of epoch 79 => [0.01]\n",
      "Epoch 80/300 - Train Loss: 6.53749613\n",
      "Epoch 80/300 - Test Loss: 6.51173753\n",
      "lr of epoch 80 => [0.01]\n",
      "Epoch 81/300 - Train Loss: 6.53700613\n",
      "Epoch 81/300 - Test Loss: 6.50630399\n",
      "lr of epoch 81 => [0.01]\n",
      "Epoch 82/300 - Train Loss: 6.53569809\n",
      "Epoch 82/300 - Test Loss: 6.50531374\n",
      "lr of epoch 82 => [0.01]\n",
      "Epoch 83/300 - Train Loss: 6.53532977\n",
      "Epoch 83/300 - Test Loss: 6.50525950\n",
      "lr of epoch 83 => [0.01]\n",
      "Epoch 84/300 - Train Loss: 6.53359696\n",
      "Epoch 84/300 - Test Loss: 6.50281224\n",
      "lr of epoch 84 => [0.01]\n",
      "Epoch 85/300 - Train Loss: 6.53015153\n",
      "Epoch 85/300 - Test Loss: 6.50112488\n",
      "lr of epoch 85 => [0.01]\n",
      "Epoch 86/300 - Train Loss: 6.52929579\n",
      "Epoch 86/300 - Test Loss: 6.49931227\n",
      "lr of epoch 86 => [0.01]\n",
      "Epoch 87/300 - Train Loss: 6.52903950\n",
      "Epoch 87/300 - Test Loss: 6.49762044\n",
      "lr of epoch 87 => [0.01]\n",
      "Epoch 88/300 - Train Loss: 6.52595945\n",
      "Epoch 88/300 - Test Loss: 6.49399957\n",
      "lr of epoch 88 => [0.01]\n",
      "Epoch 89/300 - Train Loss: 6.52451836\n",
      "Epoch 89/300 - Test Loss: 6.49541638\n",
      "lr of epoch 89 => [0.01]\n",
      "Epoch 90/300 - Train Loss: 6.52401372\n",
      "Epoch 90/300 - Test Loss: 6.49267694\n",
      "lr of epoch 90 => [0.01]\n",
      "Epoch 91/300 - Train Loss: 6.51222590\n",
      "Epoch 91/300 - Test Loss: 6.48320754\n",
      "lr of epoch 91 => [0.01]\n",
      "Epoch 92/300 - Train Loss: 6.50919453\n",
      "Epoch 92/300 - Test Loss: 6.47868940\n",
      "lr of epoch 92 => [0.01]\n",
      "Epoch 93/300 - Train Loss: 6.50708440\n",
      "Epoch 93/300 - Test Loss: 6.47324993\n",
      "lr of epoch 93 => [0.01]\n",
      "Epoch 94/300 - Train Loss: 6.50085223\n",
      "Epoch 94/300 - Test Loss: 6.47478883\n",
      "lr of epoch 94 => [0.01]\n",
      "Epoch 95/300 - Train Loss: 6.50045875\n",
      "Epoch 95/300 - Test Loss: 6.46933121\n",
      "lr of epoch 95 => [0.01]\n",
      "Epoch 96/300 - Train Loss: 6.49896003\n",
      "Epoch 96/300 - Test Loss: 6.47096230\n",
      "lr of epoch 96 => [0.01]\n",
      "Epoch 97/300 - Train Loss: 6.49796210\n",
      "Epoch 97/300 - Test Loss: 6.47108683\n",
      "lr of epoch 97 => [0.01]\n",
      "Epoch 98/300 - Train Loss: 6.49528751\n",
      "Epoch 98/300 - Test Loss: 6.46759576\n",
      "lr of epoch 98 => [0.01]\n",
      "Epoch 99/300 - Train Loss: 6.49466717\n",
      "Epoch 99/300 - Test Loss: 6.46712048\n",
      "lr of epoch 99 => [0.01]\n",
      "Epoch 100/300 - Train Loss: 6.49344742\n",
      "Epoch 100/300 - Test Loss: 6.46476661\n",
      "lr of epoch 100 => [0.01]\n",
      "Epoch 101/300 - Train Loss: 6.49417885\n",
      "Epoch 101/300 - Test Loss: 6.46396938\n",
      "lr of epoch 101 => [0.01]\n",
      "Epoch 102/300 - Train Loss: 6.49068159\n",
      "Epoch 102/300 - Test Loss: 6.46078807\n",
      "lr of epoch 102 => [0.01]\n",
      "Epoch 103/300 - Train Loss: 6.48792573\n",
      "Epoch 103/300 - Test Loss: 6.45586434\n",
      "lr of epoch 103 => [0.01]\n",
      "Epoch 104/300 - Train Loss: 6.48491476\n",
      "Epoch 104/300 - Test Loss: 6.46015270\n",
      "lr of epoch 104 => [0.01]\n",
      "Epoch 105/300 - Train Loss: 6.48242227\n",
      "Epoch 105/300 - Test Loss: 6.45444501\n",
      "lr of epoch 105 => [0.01]\n",
      "Epoch 106/300 - Train Loss: 6.47913494\n",
      "Epoch 106/300 - Test Loss: 6.45063795\n",
      "lr of epoch 106 => [0.01]\n",
      "Epoch 107/300 - Train Loss: 6.47405373\n",
      "Epoch 107/300 - Test Loss: 6.44417213\n",
      "lr of epoch 107 => [0.01]\n",
      "Epoch 108/300 - Train Loss: 6.47264017\n",
      "Epoch 108/300 - Test Loss: 6.44671033\n",
      "lr of epoch 108 => [0.01]\n",
      "Epoch 109/300 - Train Loss: 6.47262002\n",
      "Epoch 109/300 - Test Loss: 6.44602554\n",
      "lr of epoch 109 => [0.01]\n",
      "Epoch 110/300 - Train Loss: 6.47305184\n",
      "Epoch 110/300 - Test Loss: 6.44397549\n",
      "lr of epoch 110 => [0.01]\n",
      "Epoch 111/300 - Train Loss: 6.47176723\n",
      "Epoch 111/300 - Test Loss: 6.44142277\n",
      "lr of epoch 111 => [0.01]\n",
      "Epoch 112/300 - Train Loss: 6.46860567\n",
      "Epoch 112/300 - Test Loss: 6.43747728\n",
      "lr of epoch 112 => [0.01]\n",
      "Epoch 113/300 - Train Loss: 6.46498107\n",
      "Epoch 113/300 - Test Loss: 6.43837868\n",
      "lr of epoch 113 => [0.01]\n",
      "Epoch 114/300 - Train Loss: 6.45993360\n",
      "Epoch 114/300 - Test Loss: 6.42843234\n",
      "lr of epoch 114 => [0.01]\n",
      "Epoch 115/300 - Train Loss: 6.45552688\n",
      "Epoch 115/300 - Test Loss: 6.42932683\n",
      "lr of epoch 115 => [0.01]\n",
      "Epoch 116/300 - Train Loss: 6.45453816\n",
      "Epoch 116/300 - Test Loss: 6.42741634\n",
      "lr of epoch 116 => [0.01]\n",
      "Epoch 117/300 - Train Loss: 6.45320089\n",
      "Epoch 117/300 - Test Loss: 6.42485604\n",
      "lr of epoch 117 => [0.01]\n",
      "Epoch 118/300 - Train Loss: 6.45233086\n",
      "Epoch 118/300 - Test Loss: 6.42325031\n",
      "lr of epoch 118 => [0.01]\n",
      "Epoch 119/300 - Train Loss: 6.45231214\n",
      "Epoch 119/300 - Test Loss: 6.42073866\n",
      "lr of epoch 119 => [0.01]\n",
      "Epoch 120/300 - Train Loss: 6.44658143\n",
      "Epoch 120/300 - Test Loss: 6.41832720\n",
      "lr of epoch 120 => [0.01]\n",
      "Epoch 121/300 - Train Loss: 6.44418584\n",
      "Epoch 121/300 - Test Loss: 6.41414233\n",
      "lr of epoch 121 => [0.01]\n",
      "Epoch 122/300 - Train Loss: 6.44239771\n",
      "Epoch 122/300 - Test Loss: 6.41242405\n",
      "lr of epoch 122 => [0.01]\n",
      "Epoch 123/300 - Train Loss: 6.44021656\n",
      "Epoch 123/300 - Test Loss: 6.41253686\n",
      "lr of epoch 123 => [0.01]\n",
      "Epoch 124/300 - Train Loss: 6.43963417\n",
      "Epoch 124/300 - Test Loss: 6.41282634\n",
      "lr of epoch 124 => [0.01]\n",
      "Epoch 125/300 - Train Loss: 6.44059852\n",
      "Epoch 125/300 - Test Loss: 6.41144448\n",
      "lr of epoch 125 => [0.01]\n",
      "Epoch 126/300 - Train Loss: 6.43960868\n",
      "Epoch 126/300 - Test Loss: 6.41331904\n",
      "lr of epoch 126 => [0.01]\n",
      "Epoch 127/300 - Train Loss: 6.44020144\n",
      "Epoch 127/300 - Test Loss: 6.41245882\n",
      "lr of epoch 127 => [0.01]\n",
      "Epoch 128/300 - Train Loss: 6.43952934\n",
      "Epoch 128/300 - Test Loss: 6.41310675\n",
      "lr of epoch 128 => [0.01]\n",
      "Epoch 129/300 - Train Loss: 6.44111115\n",
      "Epoch 129/300 - Test Loss: 6.41598122\n",
      "lr of epoch 129 => [0.01]\n",
      "Epoch 130/300 - Train Loss: 6.43910794\n",
      "Epoch 130/300 - Test Loss: 6.41233771\n",
      "lr of epoch 130 => [0.01]\n",
      "Epoch 131/300 - Train Loss: 6.43048659\n",
      "Epoch 131/300 - Test Loss: 6.39495400\n",
      "lr of epoch 131 => [0.01]\n",
      "Epoch 132/300 - Train Loss: 6.42214318\n",
      "Epoch 132/300 - Test Loss: 6.39683993\n",
      "lr of epoch 132 => [0.01]\n",
      "Epoch 133/300 - Train Loss: 6.42088066\n",
      "Epoch 133/300 - Test Loss: 6.40000219\n",
      "lr of epoch 133 => [0.01]\n",
      "Epoch 134/300 - Train Loss: 6.41885280\n",
      "Epoch 134/300 - Test Loss: 6.40493961\n",
      "lr of epoch 134 => [0.01]\n",
      "Epoch 135/300 - Train Loss: 6.41746255\n",
      "Epoch 135/300 - Test Loss: 6.38861682\n",
      "lr of epoch 135 => [0.01]\n",
      "Epoch 136/300 - Train Loss: 6.41750628\n",
      "Epoch 136/300 - Test Loss: 6.38956360\n",
      "lr of epoch 136 => [0.01]\n",
      "Epoch 137/300 - Train Loss: 6.41764422\n",
      "Epoch 137/300 - Test Loss: 6.39508211\n",
      "lr of epoch 137 => [0.01]\n",
      "Epoch 138/300 - Train Loss: 6.41702783\n",
      "Epoch 138/300 - Test Loss: 6.39011700\n",
      "lr of epoch 138 => [0.01]\n",
      "Epoch 139/300 - Train Loss: 6.41640887\n",
      "Epoch 139/300 - Test Loss: 6.38879717\n",
      "lr of epoch 139 => [0.01]\n",
      "Epoch 140/300 - Train Loss: 6.41660197\n",
      "Epoch 140/300 - Test Loss: 6.38740006\n",
      "lr of epoch 140 => [0.01]\n",
      "Epoch 141/300 - Train Loss: 6.41472747\n",
      "Epoch 141/300 - Test Loss: 6.38589938\n",
      "lr of epoch 141 => [0.01]\n",
      "Epoch 142/300 - Train Loss: 6.41447750\n",
      "Epoch 142/300 - Test Loss: 6.38710304\n",
      "lr of epoch 142 => [0.01]\n",
      "Epoch 143/300 - Train Loss: 6.41077041\n",
      "Epoch 143/300 - Test Loss: 6.37733203\n",
      "lr of epoch 143 => [0.01]\n",
      "Epoch 144/300 - Train Loss: 6.40373230\n",
      "Epoch 144/300 - Test Loss: 6.37720035\n",
      "lr of epoch 144 => [0.01]\n",
      "Epoch 145/300 - Train Loss: 6.40236299\n",
      "Epoch 145/300 - Test Loss: 6.37617027\n",
      "lr of epoch 145 => [0.01]\n",
      "Epoch 146/300 - Train Loss: 6.39666931\n",
      "Epoch 146/300 - Test Loss: 6.36286935\n",
      "lr of epoch 146 => [0.01]\n",
      "Epoch 147/300 - Train Loss: 6.38974671\n",
      "Epoch 147/300 - Test Loss: 6.36539573\n",
      "lr of epoch 147 => [0.01]\n",
      "Epoch 148/300 - Train Loss: 6.39091537\n",
      "Epoch 148/300 - Test Loss: 6.36296855\n",
      "lr of epoch 148 => [0.01]\n",
      "Epoch 149/300 - Train Loss: 6.38904034\n",
      "Epoch 149/300 - Test Loss: 6.36392757\n",
      "lr of epoch 149 => [0.01]\n",
      "Epoch 150/300 - Train Loss: 6.38840736\n",
      "Epoch 150/300 - Test Loss: 6.36663384\n",
      "lr of epoch 150 => [0.01]\n",
      "Epoch 151/300 - Train Loss: 6.38886982\n",
      "Epoch 151/300 - Test Loss: 6.36285826\n",
      "lr of epoch 151 => [0.01]\n",
      "Epoch 152/300 - Train Loss: 6.38768336\n",
      "Epoch 152/300 - Test Loss: 6.35961712\n",
      "lr of epoch 152 => [0.01]\n",
      "Epoch 153/300 - Train Loss: 6.38646819\n",
      "Epoch 153/300 - Test Loss: 6.36060225\n",
      "lr of epoch 153 => [0.01]\n",
      "Epoch 154/300 - Train Loss: 6.38562182\n",
      "Epoch 154/300 - Test Loss: 6.35856726\n",
      "lr of epoch 154 => [0.01]\n",
      "Epoch 155/300 - Train Loss: 6.37517114\n",
      "Epoch 155/300 - Test Loss: 6.34784728\n",
      "lr of epoch 155 => [0.01]\n",
      "Epoch 156/300 - Train Loss: 6.37554049\n",
      "Epoch 156/300 - Test Loss: 6.34590140\n",
      "lr of epoch 156 => [0.01]\n",
      "Epoch 157/300 - Train Loss: 6.37292963\n",
      "Epoch 157/300 - Test Loss: 6.34591196\n",
      "lr of epoch 157 => [0.01]\n",
      "Epoch 158/300 - Train Loss: 6.37323098\n",
      "Epoch 158/300 - Test Loss: 6.34667156\n",
      "lr of epoch 158 => [0.01]\n",
      "Epoch 159/300 - Train Loss: 6.37309194\n",
      "Epoch 159/300 - Test Loss: 6.34618594\n",
      "lr of epoch 159 => [0.01]\n",
      "Epoch 160/300 - Train Loss: 6.36991051\n",
      "Epoch 160/300 - Test Loss: 6.34077621\n",
      "lr of epoch 160 => [0.01]\n",
      "Epoch 161/300 - Train Loss: 6.35913552\n",
      "Epoch 161/300 - Test Loss: 6.32994441\n",
      "lr of epoch 161 => [0.01]\n",
      "Epoch 162/300 - Train Loss: 6.35231597\n",
      "Epoch 162/300 - Test Loss: 6.33279690\n",
      "lr of epoch 162 => [0.01]\n",
      "Epoch 163/300 - Train Loss: 6.34850620\n",
      "Epoch 163/300 - Test Loss: 6.32065786\n",
      "lr of epoch 163 => [0.01]\n",
      "Epoch 164/300 - Train Loss: 6.34681639\n",
      "Epoch 164/300 - Test Loss: 6.32000555\n",
      "lr of epoch 164 => [0.01]\n",
      "Epoch 165/300 - Train Loss: 6.34347374\n",
      "Epoch 165/300 - Test Loss: 6.31751884\n",
      "lr of epoch 165 => [0.01]\n",
      "Epoch 166/300 - Train Loss: 6.34103798\n",
      "Epoch 166/300 - Test Loss: 6.31657929\n",
      "lr of epoch 166 => [0.01]\n",
      "Epoch 167/300 - Train Loss: 6.34105348\n",
      "Epoch 167/300 - Test Loss: 6.31670735\n",
      "lr of epoch 167 => [0.01]\n",
      "Epoch 168/300 - Train Loss: 6.34023689\n",
      "Epoch 168/300 - Test Loss: 6.31477127\n",
      "lr of epoch 168 => [0.01]\n",
      "Epoch 169/300 - Train Loss: 6.34070651\n",
      "Epoch 169/300 - Test Loss: 6.31380600\n",
      "lr of epoch 169 => [0.01]\n",
      "Epoch 170/300 - Train Loss: 6.34067483\n",
      "Epoch 170/300 - Test Loss: 6.31805919\n",
      "lr of epoch 170 => [0.01]\n",
      "Epoch 171/300 - Train Loss: 6.33865806\n",
      "Epoch 171/300 - Test Loss: 6.30917819\n",
      "lr of epoch 171 => [0.01]\n",
      "Epoch 172/300 - Train Loss: 6.33650874\n",
      "Epoch 172/300 - Test Loss: 6.30950849\n",
      "lr of epoch 172 => [0.01]\n",
      "Epoch 173/300 - Train Loss: 6.33621411\n",
      "Epoch 173/300 - Test Loss: 6.31259581\n",
      "lr of epoch 173 => [0.01]\n",
      "Epoch 174/300 - Train Loss: 6.33657205\n",
      "Epoch 174/300 - Test Loss: 6.31071289\n",
      "lr of epoch 174 => [0.01]\n",
      "Epoch 175/300 - Train Loss: 6.33672247\n",
      "Epoch 175/300 - Test Loss: 6.31328442\n",
      "lr of epoch 175 => [0.01]\n",
      "Epoch 176/300 - Train Loss: 6.33551815\n",
      "Epoch 176/300 - Test Loss: 6.30530948\n",
      "lr of epoch 176 => [0.01]\n",
      "Epoch 177/300 - Train Loss: 6.33192279\n",
      "Epoch 177/300 - Test Loss: 6.30353429\n",
      "lr of epoch 177 => [0.01]\n",
      "Epoch 178/300 - Train Loss: 6.33048227\n",
      "Epoch 178/300 - Test Loss: 6.30367230\n",
      "lr of epoch 178 => [0.01]\n",
      "Epoch 179/300 - Train Loss: 6.33062165\n",
      "Epoch 179/300 - Test Loss: 6.30886696\n",
      "lr of epoch 179 => [0.01]\n",
      "Epoch 180/300 - Train Loss: 6.32723292\n",
      "Epoch 180/300 - Test Loss: 6.29644812\n",
      "lr of epoch 180 => [0.01]\n",
      "Epoch 181/300 - Train Loss: 6.32226020\n",
      "Epoch 181/300 - Test Loss: 6.29627982\n",
      "lr of epoch 181 => [0.01]\n",
      "Epoch 182/300 - Train Loss: 6.32207518\n",
      "Epoch 182/300 - Test Loss: 6.29533755\n",
      "lr of epoch 182 => [0.01]\n",
      "Epoch 183/300 - Train Loss: 6.32133076\n",
      "Epoch 183/300 - Test Loss: 6.29995973\n",
      "lr of epoch 183 => [0.01]\n",
      "Epoch 184/300 - Train Loss: 6.31970228\n",
      "Epoch 184/300 - Test Loss: 6.29182007\n",
      "lr of epoch 184 => [0.01]\n",
      "Epoch 185/300 - Train Loss: 6.31460169\n",
      "Epoch 185/300 - Test Loss: 6.28319104\n",
      "lr of epoch 185 => [0.01]\n",
      "Epoch 186/300 - Train Loss: 6.30924407\n",
      "Epoch 186/300 - Test Loss: 6.28704282\n",
      "lr of epoch 186 => [0.01]\n",
      "Epoch 187/300 - Train Loss: 6.30322041\n",
      "Epoch 187/300 - Test Loss: 6.27469934\n",
      "lr of epoch 187 => [0.01]\n",
      "Epoch 188/300 - Train Loss: 6.29992970\n",
      "Epoch 188/300 - Test Loss: 6.27115094\n",
      "lr of epoch 188 => [0.01]\n",
      "Epoch 189/300 - Train Loss: 6.29742293\n",
      "Epoch 189/300 - Test Loss: 6.27291254\n",
      "lr of epoch 189 => [0.01]\n",
      "Epoch 190/300 - Train Loss: 6.29576556\n",
      "Epoch 190/300 - Test Loss: 6.26939830\n",
      "lr of epoch 190 => [0.01]\n",
      "Epoch 191/300 - Train Loss: 6.29502752\n",
      "Epoch 191/300 - Test Loss: 6.26890280\n",
      "lr of epoch 191 => [0.01]\n",
      "Epoch 192/300 - Train Loss: 6.29387165\n",
      "Epoch 192/300 - Test Loss: 6.27366517\n",
      "lr of epoch 192 => [0.01]\n",
      "Epoch 193/300 - Train Loss: 6.29588636\n",
      "Epoch 193/300 - Test Loss: 6.26882047\n",
      "lr of epoch 193 => [0.01]\n",
      "Epoch 194/300 - Train Loss: 6.29463221\n",
      "Epoch 194/300 - Test Loss: 6.27157833\n",
      "lr of epoch 194 => [0.01]\n",
      "Epoch 195/300 - Train Loss: 6.29436083\n",
      "Epoch 195/300 - Test Loss: 6.27062317\n",
      "lr of epoch 195 => [0.01]\n",
      "Epoch 196/300 - Train Loss: 6.29413603\n",
      "Epoch 196/300 - Test Loss: 6.27178522\n",
      "lr of epoch 196 => [0.01]\n",
      "Epoch 197/300 - Train Loss: 6.29434322\n",
      "Epoch 197/300 - Test Loss: 6.26890652\n",
      "lr of epoch 197 => [0.01]\n",
      "Epoch 198/300 - Train Loss: 6.29379250\n",
      "Epoch 198/300 - Test Loss: 6.26979879\n",
      "lr of epoch 198 => [0.01]\n",
      "Epoch 199/300 - Train Loss: 6.29354765\n",
      "Epoch 199/300 - Test Loss: 6.26804610\n",
      "lr of epoch 199 => [0.01]\n",
      "Epoch 200/300 - Train Loss: 6.29416087\n",
      "Epoch 200/300 - Test Loss: 6.26791261\n",
      "lr of epoch 200 => [0.01]\n",
      "Epoch 201/300 - Train Loss: 6.29434430\n",
      "Epoch 201/300 - Test Loss: 6.26937395\n",
      "lr of epoch 201 => [0.01]\n",
      "Epoch 202/300 - Train Loss: 6.29453501\n",
      "Epoch 202/300 - Test Loss: 6.26848300\n",
      "lr of epoch 202 => [0.01]\n",
      "Epoch 203/300 - Train Loss: 6.29370404\n",
      "Epoch 203/300 - Test Loss: 6.26748298\n",
      "lr of epoch 203 => [0.01]\n",
      "Epoch 204/300 - Train Loss: 6.29257344\n",
      "Epoch 204/300 - Test Loss: 6.26892625\n",
      "lr of epoch 204 => [0.01]\n",
      "Epoch 205/300 - Train Loss: 6.29238997\n",
      "Epoch 205/300 - Test Loss: 6.26787459\n",
      "lr of epoch 205 => [0.01]\n",
      "Epoch 206/300 - Train Loss: 6.29318943\n",
      "Epoch 206/300 - Test Loss: 6.26780591\n",
      "lr of epoch 206 => [0.01]\n",
      "Epoch 207/300 - Train Loss: 6.29304550\n",
      "Epoch 207/300 - Test Loss: 6.26853384\n",
      "lr of epoch 207 => [0.01]\n",
      "Epoch 208/300 - Train Loss: 6.29359799\n",
      "Epoch 208/300 - Test Loss: 6.26727064\n",
      "lr of epoch 208 => [0.01]\n",
      "Epoch 209/300 - Train Loss: 6.29351740\n",
      "Epoch 209/300 - Test Loss: 6.26753409\n",
      "lr of epoch 209 => [0.01]\n",
      "Epoch 210/300 - Train Loss: 6.29466466\n",
      "Epoch 210/300 - Test Loss: 6.26768292\n",
      "lr of epoch 210 => [0.01]\n",
      "Epoch 211/300 - Train Loss: 6.29240067\n",
      "Epoch 211/300 - Test Loss: 6.26732892\n",
      "lr of epoch 211 => [0.01]\n",
      "Epoch 212/300 - Train Loss: 6.29300499\n",
      "Epoch 212/300 - Test Loss: 6.27018270\n",
      "lr of epoch 212 => [0.01]\n",
      "Epoch 213/300 - Train Loss: 6.29284526\n",
      "Epoch 213/300 - Test Loss: 6.27154314\n",
      "lr of epoch 213 => [0.01]\n",
      "Epoch 214/300 - Train Loss: 6.29232762\n",
      "Epoch 214/300 - Test Loss: 6.26405177\n",
      "lr of epoch 214 => [0.01]\n",
      "Epoch 215/300 - Train Loss: 6.29032982\n",
      "Epoch 215/300 - Test Loss: 6.26536542\n",
      "lr of epoch 215 => [0.01]\n",
      "Epoch 216/300 - Train Loss: 6.29126474\n",
      "Epoch 216/300 - Test Loss: 6.26826571\n",
      "lr of epoch 216 => [0.01]\n",
      "Epoch 217/300 - Train Loss: 6.29033256\n",
      "Epoch 217/300 - Test Loss: 6.26646396\n",
      "lr of epoch 217 => [0.01]\n",
      "Epoch 218/300 - Train Loss: 6.29041973\n",
      "Epoch 218/300 - Test Loss: 6.26434880\n",
      "lr of epoch 218 => [0.01]\n",
      "Epoch 219/300 - Train Loss: 6.28963561\n",
      "Epoch 219/300 - Test Loss: 6.26520593\n",
      "lr of epoch 219 => [0.01]\n",
      "Epoch 220/300 - Train Loss: 6.29058144\n",
      "Epoch 220/300 - Test Loss: 6.26192357\n",
      "lr of epoch 220 => [0.01]\n",
      "Epoch 221/300 - Train Loss: 6.28755545\n",
      "Epoch 221/300 - Test Loss: 6.26102281\n",
      "lr of epoch 221 => [0.01]\n",
      "Epoch 222/300 - Train Loss: 6.28685172\n",
      "Epoch 222/300 - Test Loss: 6.26347085\n",
      "lr of epoch 222 => [0.01]\n",
      "Epoch 223/300 - Train Loss: 6.28757480\n",
      "Epoch 223/300 - Test Loss: 6.26243822\n",
      "lr of epoch 223 => [0.01]\n",
      "Epoch 224/300 - Train Loss: 6.28463297\n",
      "Epoch 224/300 - Test Loss: 6.25688646\n",
      "lr of epoch 224 => [0.01]\n",
      "Epoch 225/300 - Train Loss: 6.28218269\n",
      "Epoch 225/300 - Test Loss: 6.25911472\n",
      "lr of epoch 225 => [0.01]\n",
      "Epoch 226/300 - Train Loss: 6.28184165\n",
      "Epoch 226/300 - Test Loss: 6.25609889\n",
      "lr of epoch 226 => [0.01]\n",
      "Epoch 227/300 - Train Loss: 6.28197657\n",
      "Epoch 227/300 - Test Loss: 6.26113453\n",
      "lr of epoch 227 => [0.01]\n",
      "Epoch 228/300 - Train Loss: 6.28268142\n",
      "Epoch 228/300 - Test Loss: 6.25876955\n",
      "lr of epoch 228 => [0.01]\n",
      "Epoch 229/300 - Train Loss: 6.28209899\n",
      "Epoch 229/300 - Test Loss: 6.26176313\n",
      "lr of epoch 229 => [0.01]\n",
      "Epoch 230/300 - Train Loss: 6.28055286\n",
      "Epoch 230/300 - Test Loss: 6.25470319\n",
      "lr of epoch 230 => [0.01]\n",
      "Epoch 231/300 - Train Loss: 6.28018003\n",
      "Epoch 231/300 - Test Loss: 6.25783693\n",
      "lr of epoch 231 => [0.01]\n",
      "Epoch 232/300 - Train Loss: 6.28015057\n",
      "Epoch 232/300 - Test Loss: 6.25653918\n",
      "lr of epoch 232 => [0.01]\n",
      "Epoch 233/300 - Train Loss: 6.28081167\n",
      "Epoch 233/300 - Test Loss: 6.25484536\n",
      "lr of epoch 233 => [0.01]\n",
      "Epoch 234/300 - Train Loss: 6.28035306\n",
      "Epoch 234/300 - Test Loss: 6.25352671\n",
      "lr of epoch 234 => [0.01]\n",
      "Epoch 235/300 - Train Loss: 6.27998238\n",
      "Epoch 235/300 - Test Loss: 6.25534962\n",
      "lr of epoch 235 => [0.01]\n",
      "Epoch 236/300 - Train Loss: 6.27597565\n",
      "Epoch 236/300 - Test Loss: 6.24686316\n",
      "lr of epoch 236 => [0.01]\n",
      "Epoch 237/300 - Train Loss: 6.27190976\n",
      "Epoch 237/300 - Test Loss: 6.24781401\n",
      "lr of epoch 237 => [0.01]\n",
      "Epoch 238/300 - Train Loss: 6.27196662\n",
      "Epoch 238/300 - Test Loss: 6.24641855\n",
      "lr of epoch 238 => [0.01]\n",
      "Epoch 239/300 - Train Loss: 6.27198555\n",
      "Epoch 239/300 - Test Loss: 6.24717316\n",
      "lr of epoch 239 => [0.01]\n",
      "Epoch 240/300 - Train Loss: 6.27257217\n",
      "Epoch 240/300 - Test Loss: 6.24790432\n",
      "lr of epoch 240 => [0.01]\n",
      "Epoch 241/300 - Train Loss: 6.27178256\n",
      "Epoch 241/300 - Test Loss: 6.24563021\n",
      "lr of epoch 241 => [0.01]\n",
      "Epoch 242/300 - Train Loss: 6.27174131\n",
      "Epoch 242/300 - Test Loss: 6.24627542\n",
      "lr of epoch 242 => [0.01]\n",
      "Epoch 243/300 - Train Loss: 6.27211806\n",
      "Epoch 243/300 - Test Loss: 6.24588367\n",
      "lr of epoch 243 => [0.01]\n",
      "Epoch 244/300 - Train Loss: 6.27204135\n",
      "Epoch 244/300 - Test Loss: 6.24658574\n",
      "lr of epoch 244 => [0.01]\n",
      "Epoch 245/300 - Train Loss: 6.27121494\n",
      "Epoch 245/300 - Test Loss: 6.24608944\n",
      "lr of epoch 245 => [0.01]\n",
      "Epoch 246/300 - Train Loss: 6.27141229\n",
      "Epoch 246/300 - Test Loss: 6.24618182\n",
      "lr of epoch 246 => [0.01]\n",
      "Epoch 247/300 - Train Loss: 6.27153862\n",
      "Epoch 247/300 - Test Loss: 6.24712783\n",
      "lr of epoch 247 => [0.01]\n",
      "Epoch 248/300 - Train Loss: 6.27101987\n",
      "Epoch 248/300 - Test Loss: 6.24966084\n",
      "lr of epoch 248 => [0.01]\n",
      "Epoch 249/300 - Train Loss: 6.27066042\n",
      "Epoch 249/300 - Test Loss: 6.24513945\n",
      "lr of epoch 249 => [0.01]\n",
      "Epoch 250/300 - Train Loss: 6.27041688\n",
      "Epoch 250/300 - Test Loss: 6.24464494\n",
      "lr of epoch 250 => [0.01]\n",
      "Epoch 251/300 - Train Loss: 6.26903265\n",
      "Epoch 251/300 - Test Loss: 6.24495316\n",
      "lr of epoch 251 => [0.01]\n",
      "Epoch 252/300 - Train Loss: 6.26849268\n",
      "Epoch 252/300 - Test Loss: 6.24478624\n",
      "lr of epoch 252 => [0.01]\n",
      "Epoch 253/300 - Train Loss: 6.26800959\n",
      "Epoch 253/300 - Test Loss: 6.24277775\n",
      "lr of epoch 253 => [0.01]\n",
      "Epoch 254/300 - Train Loss: 6.26395634\n",
      "Epoch 254/300 - Test Loss: 6.24131982\n",
      "lr of epoch 254 => [0.01]\n",
      "Epoch 255/300 - Train Loss: 6.26334336\n",
      "Epoch 255/300 - Test Loss: 6.24021945\n",
      "lr of epoch 255 => [0.01]\n",
      "Epoch 256/300 - Train Loss: 6.26234176\n",
      "Epoch 256/300 - Test Loss: 6.24070470\n",
      "lr of epoch 256 => [0.01]\n",
      "Epoch 257/300 - Train Loss: 6.25843046\n",
      "Epoch 257/300 - Test Loss: 6.23244513\n",
      "lr of epoch 257 => [0.01]\n",
      "Epoch 258/300 - Train Loss: 6.25545197\n",
      "Epoch 258/300 - Test Loss: 6.23067806\n",
      "lr of epoch 258 => [0.01]\n",
      "Epoch 259/300 - Train Loss: 6.25560967\n",
      "Epoch 259/300 - Test Loss: 6.23249758\n",
      "lr of epoch 259 => [0.01]\n",
      "Epoch 260/300 - Train Loss: 6.25561629\n",
      "Epoch 260/300 - Test Loss: 6.23010799\n",
      "lr of epoch 260 => [0.01]\n",
      "Epoch 261/300 - Train Loss: 6.25459003\n",
      "Epoch 261/300 - Test Loss: 6.22938196\n",
      "lr of epoch 261 => [0.01]\n",
      "Epoch 262/300 - Train Loss: 6.25342017\n",
      "Epoch 262/300 - Test Loss: 6.22905155\n",
      "lr of epoch 262 => [0.01]\n",
      "Epoch 263/300 - Train Loss: 6.25285590\n",
      "Epoch 263/300 - Test Loss: 6.22760833\n",
      "lr of epoch 263 => [0.01]\n",
      "Epoch 264/300 - Train Loss: 6.25239444\n",
      "Epoch 264/300 - Test Loss: 6.22778905\n",
      "lr of epoch 264 => [0.01]\n",
      "Epoch 265/300 - Train Loss: 6.25184884\n",
      "Epoch 265/300 - Test Loss: 6.22692655\n",
      "lr of epoch 265 => [0.01]\n",
      "Epoch 266/300 - Train Loss: 6.25060706\n",
      "Epoch 266/300 - Test Loss: 6.22148492\n",
      "lr of epoch 266 => [0.01]\n",
      "Epoch 267/300 - Train Loss: 6.24798429\n",
      "Epoch 267/300 - Test Loss: 6.22207511\n",
      "lr of epoch 267 => [0.01]\n",
      "Epoch 268/300 - Train Loss: 6.24775318\n",
      "Epoch 268/300 - Test Loss: 6.22285362\n",
      "lr of epoch 268 => [0.01]\n",
      "Epoch 269/300 - Train Loss: 6.24778889\n",
      "Epoch 269/300 - Test Loss: 6.22257502\n",
      "lr of epoch 269 => [0.01]\n",
      "Epoch 270/300 - Train Loss: 6.24787603\n",
      "Epoch 270/300 - Test Loss: 6.22239205\n",
      "lr of epoch 270 => [0.01]\n",
      "Epoch 271/300 - Train Loss: 6.24794104\n",
      "Epoch 271/300 - Test Loss: 6.22278454\n",
      "lr of epoch 271 => [0.01]\n",
      "Epoch 272/300 - Train Loss: 6.24782687\n",
      "Epoch 272/300 - Test Loss: 6.22358564\n",
      "lr of epoch 272 => [0.01]\n",
      "Epoch 273/300 - Train Loss: 6.24779363\n",
      "Epoch 273/300 - Test Loss: 6.22377111\n",
      "lr of epoch 273 => [0.01]\n",
      "Epoch 274/300 - Train Loss: 6.24688595\n",
      "Epoch 274/300 - Test Loss: 6.22258140\n",
      "lr of epoch 274 => [0.01]\n",
      "Epoch 275/300 - Train Loss: 6.24704138\n",
      "Epoch 275/300 - Test Loss: 6.22243296\n",
      "lr of epoch 275 => [0.01]\n",
      "Epoch 276/300 - Train Loss: 6.24665077\n",
      "Epoch 276/300 - Test Loss: 6.22252202\n",
      "lr of epoch 276 => [0.01]\n",
      "Epoch 277/300 - Train Loss: 6.24553166\n",
      "Epoch 277/300 - Test Loss: 6.22054195\n",
      "lr of epoch 277 => [0.01]\n",
      "Epoch 278/300 - Train Loss: 6.24612646\n",
      "Epoch 278/300 - Test Loss: 6.22097426\n",
      "lr of epoch 278 => [0.01]\n",
      "Epoch 279/300 - Train Loss: 6.24677285\n",
      "Epoch 279/300 - Test Loss: 6.22380081\n",
      "lr of epoch 279 => [0.01]\n",
      "Epoch 280/300 - Train Loss: 6.24575289\n",
      "Epoch 280/300 - Test Loss: 6.21867334\n",
      "lr of epoch 280 => [0.01]\n",
      "Epoch 281/300 - Train Loss: 6.24487695\n",
      "Epoch 281/300 - Test Loss: 6.22025574\n",
      "lr of epoch 281 => [0.01]\n",
      "Epoch 282/300 - Train Loss: 6.24331910\n",
      "Epoch 282/300 - Test Loss: 6.21771130\n",
      "lr of epoch 282 => [0.01]\n",
      "Epoch 283/300 - Train Loss: 6.24430007\n",
      "Epoch 283/300 - Test Loss: 6.21733667\n",
      "lr of epoch 283 => [0.01]\n",
      "Epoch 284/300 - Train Loss: 6.24283571\n",
      "Epoch 284/300 - Test Loss: 6.21850645\n",
      "lr of epoch 284 => [0.01]\n",
      "Epoch 285/300 - Train Loss: 6.24317732\n",
      "Epoch 285/300 - Test Loss: 6.21679185\n",
      "lr of epoch 285 => [0.01]\n",
      "Epoch 286/300 - Train Loss: 6.24121166\n",
      "Epoch 286/300 - Test Loss: 6.21843144\n",
      "lr of epoch 286 => [0.01]\n",
      "Epoch 287/300 - Train Loss: 6.24064487\n",
      "Epoch 287/300 - Test Loss: 6.21915115\n",
      "lr of epoch 287 => [0.01]\n",
      "Epoch 288/300 - Train Loss: 6.23877294\n",
      "Epoch 288/300 - Test Loss: 6.21461577\n",
      "lr of epoch 288 => [0.01]\n",
      "Epoch 289/300 - Train Loss: 6.23856304\n",
      "Epoch 289/300 - Test Loss: 6.21375801\n",
      "lr of epoch 289 => [0.01]\n",
      "Epoch 290/300 - Train Loss: 6.23842130\n",
      "Epoch 290/300 - Test Loss: 6.21488786\n",
      "lr of epoch 290 => [0.01]\n",
      "Epoch 291/300 - Train Loss: 6.23845140\n",
      "Epoch 291/300 - Test Loss: 6.21286911\n",
      "lr of epoch 291 => [0.01]\n",
      "Epoch 292/300 - Train Loss: 6.23866171\n",
      "Epoch 292/300 - Test Loss: 6.21318568\n",
      "lr of epoch 292 => [0.01]\n",
      "Epoch 293/300 - Train Loss: 6.23755726\n",
      "Epoch 293/300 - Test Loss: 6.21252847\n",
      "lr of epoch 293 => [0.01]\n",
      "Epoch 294/300 - Train Loss: 6.23768764\n",
      "Epoch 294/300 - Test Loss: 6.21264436\n",
      "lr of epoch 294 => [0.01]\n",
      "Epoch 295/300 - Train Loss: 6.23774531\n",
      "Epoch 295/300 - Test Loss: 6.21553359\n",
      "lr of epoch 295 => [0.01]\n",
      "Epoch 296/300 - Train Loss: 6.23739717\n",
      "Epoch 296/300 - Test Loss: 6.21353076\n",
      "lr of epoch 296 => [0.01]\n",
      "Epoch 297/300 - Train Loss: 6.23527573\n",
      "Epoch 297/300 - Test Loss: 6.20900849\n",
      "lr of epoch 297 => [0.01]\n",
      "Epoch 298/300 - Train Loss: 6.23466616\n",
      "Epoch 298/300 - Test Loss: 6.21077462\n",
      "lr of epoch 298 => [0.01]\n",
      "Epoch 299/300 - Train Loss: 6.23492778\n",
      "Epoch 299/300 - Test Loss: 6.21029879\n",
      "lr of epoch 299 => [0.01]\n",
      "Epoch 300/300 - Train Loss: 6.23146460\n",
      "Epoch 300/300 - Test Loss: 6.20520407\n",
      "lr of epoch 300 => [0.01]\n",
      "batch_size: 64   activation_function: ReLU()   learning_rate: 0.01   warmup_iters: 100\n",
      "best Epoch :  300  , best_val_loss :  6.205204065029438\n",
      "\n",
      "Autoencoder training completed in 9.3 mins.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':#被別人呼叫的時候不會執行main以下的程式，只會執行\n",
    "\n",
    "    input_dim = data_mut_tcga.shape[1]# (8238sample[0], 2649gene[1])\n",
    "    mut_encode_dim =[1000,100,50]\n",
    "    batch_size = 64\n",
    "    epoch_size = 300 #100\n",
    "    activation_function = nn.ReLU()\n",
    "    model_save_name = \"tcga_exp_%d_%d_%d\" % (mut_encode_dim[0], mut_encode_dim[1], mut_encode_dim[2])\n",
    "    learning_rate=0.01\n",
    "    warmup_iters = 100\n",
    "    patience = 50\n",
    "    Decrease_percent = 1\n",
    "    seed=42\n",
    "    \n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    train_size = int(0.8 * len(data_mut_tcga))\n",
    "    test_size = len(data_mut_tcga) - train_size\n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = random_split(data_mut_tcga.values, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders for training and testing\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    t = time.time()\n",
    "    torch.manual_seed(seed)\n",
    "    model = AE_dense_3layers(input_dim=input_dim, mut_encode_dim=mut_encode_dim, activation_func=activation_function).to(device=device)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if warmup_iters is not None:\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters,Decrease_percent)\n",
    "    \n",
    "    # Training with early stopping (assuming you've defined the EarlyStopping logic)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_weight=None\n",
    "    counter = 0\n",
    "    train_epoch_loss_list = []#  for train every epoch loss plot\n",
    "    test_epoch_loss_list=[]#  for validation every epoch loss plot\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    for epoch in range(epoch_size):\n",
    "        model.train()\n",
    "        model.requires_grad = True\n",
    "        total_train_loss = 0.0\n",
    "        for batch_idx,inputs in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs =inputs.float().to(device=device) # change torch.Tensor int64 to float32\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() # sum every batch's loss to compute average loss of every batch for one epoch\n",
    "            #print(f'Epoch {epoch + 1}/{epoch_size} - Batch {batch_idx+1}/{len(dataloader_mut_tcga)} - Loss: {loss.item():.4f}')#batch loss()\n",
    "        # Calculate and print the average loss of batch for the epoch\n",
    "        average_loss = total_train_loss / len(train_loader) # 一個 epoch 的 loss 是 batch 的 average loss\n",
    "        train_epoch_loss_list.append(average_loss)# for loss plot'\n",
    "        print(f'Epoch {epoch + 1}/{epoch_size} - Train Loss: {average_loss:.8f}')\n",
    "        \n",
    "        model.eval()\n",
    "        model.requires_grad = False\n",
    "        total_test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx,inputs in enumerate(test_loader):\n",
    "                inputs =inputs.float().to(device=device) # change torch.Tensor int64 to float32\n",
    "                outputs = model(inputs)\n",
    "                test_loss = criterion(outputs, inputs)\n",
    "                total_test_loss += test_loss.item() \n",
    "        test_average_loss = total_test_loss / len(test_loader)\n",
    "        test_epoch_loss_list.append(test_average_loss)# for loss plot'\n",
    "        print(f'Epoch {epoch + 1}/{epoch_size} - Test Loss: {test_average_loss:.8f}')\n",
    "\n",
    "        if warmup_iters:\n",
    "            print(\"lr of epoch\", epoch + 1, \"=>\", lr_scheduler.get_lr()) \n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        if test_average_loss < best_val_loss:\n",
    "            best_val_loss = test_average_loss\n",
    "            best_weight = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch+1\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping after {patience} epochs of no improvement.')\n",
    "                break\n",
    "                \n",
    "    encoder_best_weight = {key: value for key, value in best_weight.items() if key.startswith('encoder')} # only store the encoder part without decoder part\n",
    "    torch.save(encoder_best_weight, f'/root/DeepTTA/results/Encoder_{model_save_name}_best_loss_{best_val_loss:.8}.pt')\n",
    "    \n",
    "    print('batch_size:',batch_size,\" \",'activation_function:',activation_function,\" \",\n",
    "          'learning_rate:',learning_rate,\" \",'warmup_iters:',warmup_iters)\n",
    "    print(\"best Epoch : \",best_epoch,\" ,\" ,\"best_val_loss : \",best_val_loss)\n",
    "    print('\\nAutoencoder training completed in %.1f mins.\\n' % ((time.time() - t) / 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e20d41d-42cc-4ed3-b07f-ae6ae338783d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create a plot of the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_epoch_loss_list) + 1), train_epoch_loss_list, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "# Plot validation loss\n",
    "plt.plot(range(1, len(test_epoch_loss_list) + 1), test_epoch_loss_list, marker='o', linestyle='-', color='r', label='Validation Loss')\n",
    "# Mark the best epoch\n",
    "plt.plot(best_epoch, best_val_loss , marker='o', markersize=8, linestyle='', color='c', label=f'Best Epoch:{best_epoch}')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Pretain AE Loss Curve', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(bottom=0,top=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1e1a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3aca4-0867-4bfd-b80b-ffb0f590d4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
