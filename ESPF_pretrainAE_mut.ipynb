{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ec8bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "            else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f0ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information\n",
    "filename = \"DAPL_PretrainAE_exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f211e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "\n",
    "def create_mlpEncoder(dimList, activation_func):\n",
    "    layers = []\n",
    "    for i in range(len(dimList) - 1):  \n",
    "        layers.append(nn.Linear(dimList[i], dimList[i + 1]))\n",
    "        if i < len(dimList) - 2:  \n",
    "            layers.append(activation_func)\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class AE_dense_layers(nn.Module):\n",
    "    def __init__(self, input_dim, mut_encode_dim, activation_func):\n",
    "        super(AE_dense_layers, self).__init__()##__init__初始化父類別的屬性和方法\n",
    "        print('input_dim = ', input_dim)\n",
    "        print('first_layer_dim = ', mut_encode_dim[0])\n",
    "        print('second_layer_dim = ', mut_encode_dim[1])\n",
    "\n",
    "        self.encoder = create_mlpEncoder([input_dim] + mut_encode_dim, activation_func\n",
    "            )\n",
    "        self._init_weights(self.encoder)\n",
    "\n",
    "        self.decoder = create_mlpEncoder(([input_dim] + mut_encode_dim)[::-1], activation_func\n",
    "            )\n",
    "        self._init_weights(self.decoder)\n",
    "\n",
    "    def _init_weights(self, model):\n",
    "        if isinstance(model, nn.Linear):  # 直接初始化 nn.Linear 層\n",
    "            init.kaiming_uniform_(model.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "            if model.bias is not None:\n",
    "                init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.LayerNorm):\n",
    "            init.ones_(model.weight)\n",
    "            init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.ModuleList) or isinstance(model, nn.Sequential):  # 遍歷子層\n",
    "            for layer in model:\n",
    "                self._init_weights(layer)\n",
    "\n",
    "    def forward(self, x):#加self才能呼叫self裡其他的屬性和method\n",
    "        x_encoded = self.encoder(x)\n",
    "        x_decoded = self.decoder(x_encoded)\n",
    "        return x_decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "950ec938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, LR: 0.0001\n",
      "Epoch 1, LR: 0.0001\n",
      "Epoch 2, LR: 0.0001\n",
      "Epoch 3, LR: 0.0001\n",
      "Epoch 4, LR: 0.001\n",
      "Epoch 5, LR: 0.001\n",
      "Epoch 6, LR: 0.001\n",
      "Epoch 7, LR: 0.001\n",
      "Epoch 8, LR: 0.001\n",
      "Epoch 9, LR: 8e-05\n",
      "Epoch 10, LR: 8e-05\n",
      "Epoch 11, LR: 8e-05\n",
      "Epoch 12, LR: 8e-05\n",
      "Epoch 13, LR: 8e-05\n",
      "Epoch 14, LR: 8e-05\n",
      "Epoch 15, LR: 8e-05\n",
      "Epoch 16, LR: 8e-05\n",
      "Epoch 17, LR: 8e-05\n",
      "Epoch 18, LR: 8e-05\n",
      "Epoch 19, LR: 8e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "def warmup_lr_scheduler(optimizer, warmup_iters1, Decrease_percent1, warmup_iters2, Decrease_percent2, continuous=True):\n",
    "    def f(epoch):  \n",
    "        if epoch < warmup_iters1:\n",
    "            return 1  # No decrease during first warmup phase\n",
    "        elif epoch < warmup_iters2:\n",
    "            if continuous:\n",
    "                return Decrease_percent1 ** (epoch - warmup_iters1 + 1)\n",
    "            else:\n",
    "                return Decrease_percent1\n",
    "        else:\n",
    "            if continuous:\n",
    "                return (Decrease_percent1 ** (warmup_iters2 - warmup_iters1)) * (Decrease_percent2 ** (epoch - warmup_iters2 + 1))\n",
    "            else:\n",
    "                return Decrease_percent2\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = warmup_lr_scheduler(optimizer, warmup_iters1=5, Decrease_percent1=10, warmup_iters2=10, Decrease_percent2=0.8, continuous=False)\n",
    "\n",
    "for epoch in range(20):\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch}, LR: {scheduler.get_last_lr()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c7b35",
   "metadata": {},
   "source": [
    "## def warmup_lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3c57499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_lr_scheduler(optimizer, warmup_iters, Decrease_percent,continuous=True):\n",
    "    def f(epoch):  \n",
    "        if epoch >= warmup_iters:\n",
    "            if continuous is True:\n",
    "                return Decrease_percent ** (epoch-warmup_iters+1)\n",
    "            elif continuous is not True:\n",
    "                return Decrease_percent\n",
    "        return 1\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "\n",
    "# def warmup_lr_scheduler(optimizer, warmup_iters1, Decrease_percent1, warmup_iters2, Decrease_percent2, continuous=True):\n",
    "#     def f(epoch):  \n",
    "#         if epoch < warmup_iters1:\n",
    "#             return 1  # No decrease during first warmup phase\n",
    "#         elif epoch < warmup_iters2:\n",
    "#             if continuous:\n",
    "#                 return Decrease_percent1 ** (epoch - warmup_iters1 + 1)\n",
    "#             else:\n",
    "#                 return Decrease_percent1\n",
    "#         else:\n",
    "#             if continuous:\n",
    "#                 return (Decrease_percent1 ** (warmup_iters2 - warmup_iters1)) * (Decrease_percent2 ** (epoch - warmup_iters2 + 1))\n",
    "#             else:\n",
    "#                 return Decrease_percent2\n",
    "#     return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "\n",
    "import random\n",
    "# Function to set the seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# Set the seed\n",
    "seed = 42\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5f77cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Datasets successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# load TCGA mutation data, substitute here with other genomics\n",
    "data_mut_tcga= pd.read_csv(\"../data/DAPL/share/pretrain_tcga.csv\", sep=',', index_col=0)\n",
    "\n",
    "print(\"\\n\\nDatasets successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70928f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9808, 1426)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mut_tcga.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e0448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcga_EXP : \n",
      "Range: 22.04906963\n",
      "Minimum: -13.71211562\n",
      "Maximum: 8.33695401\n",
      "Mean: 0.00000000\n",
      "Median: 0.01146598\n",
      "Standard Deviation: 1.00000000\n",
      "Skewness: -0.02756315\n",
      "binary data:False\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_data_value_range\n\u001b[1;32m      2\u001b[0m get_data_value_range(data_mut_tcga\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtcga_EXP\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mkdeplot(np\u001b[38;5;241m.\u001b[39mconcatenate(np\u001b[38;5;241m.\u001b[39marray(omics_data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())), fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "from utils.tools import get_data_value_range\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "get_data_value_range(data_mut_tcga.values.tolist(),\"tcga_EXP\", file=None)\n",
    "sns.kdeplot(np.concatenate(np.array(data_mut_tcga.values.tolist())), fill=True, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba8afd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #exchange the row and column to match the format of doctor chiu's data (row is gene ;column is cancer)\n",
    "# data_mut_tcga=data_mut_tcga.transpose()\n",
    "# data_mut_tcga.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75196629",
   "metadata": {},
   "source": [
    "## 也許應該要分train validation set，這樣才能更學到真正重要的特徵，而不是背答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e452a702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim =  1426\n",
      "first_layer_dim =  128\n",
      "second_layer_dim =  32\n",
      "Epoch 1/500 - Train Loss: 1.27612219\n",
      "Epoch 1/500 - Test Loss: 0.90643192\n",
      "lr of epoch 1 => [0.001]\n",
      "Epoch 2/500 - Train Loss: 0.76061652\n",
      "Epoch 2/500 - Test Loss: 0.64880209\n",
      "lr of epoch 2 => [0.001]\n",
      "Epoch 4/500 - Train Loss: 0.46102953\n",
      "Epoch 4/500 - Test Loss: 0.44013021\n",
      "lr of epoch 4 => [0.001]\n",
      "Epoch 5/500 - Train Loss: 0.40969900\n",
      "Epoch 5/500 - Test Loss: 0.40168567\n",
      "lr of epoch 5 => [0.001]\n",
      "Epoch 6/500 - Train Loss: 0.37722669\n",
      "Epoch 6/500 - Test Loss: 0.37320868\n",
      "lr of epoch 6 => [0.001]\n",
      "Epoch 7/500 - Train Loss: 0.35379454\n",
      "Epoch 7/500 - Test Loss: 0.35446100\n",
      "lr of epoch 7 => [0.001]\n",
      "Epoch 8/500 - Train Loss: 0.33675984\n",
      "Epoch 8/500 - Test Loss: 0.33948755\n",
      "lr of epoch 8 => [0.001]\n",
      "Epoch 9/500 - Train Loss: 0.32380581\n",
      "Epoch 9/500 - Test Loss: 0.32781486\n",
      "lr of epoch 9 => [0.001]\n",
      "Epoch 10/500 - Train Loss: 0.31348281\n",
      "Epoch 10/500 - Test Loss: 0.31904828\n",
      "lr of epoch 10 => [0.001]\n",
      "Epoch 11/500 - Train Loss: 0.30530168\n",
      "Epoch 11/500 - Test Loss: 0.31170885\n",
      "lr of epoch 11 => [0.001]\n",
      "Epoch 12/500 - Train Loss: 0.29875222\n",
      "Epoch 12/500 - Test Loss: 0.30526889\n",
      "lr of epoch 12 => [0.001]\n",
      "Epoch 13/500 - Train Loss: 0.29290556\n",
      "Epoch 13/500 - Test Loss: 0.30030330\n",
      "lr of epoch 13 => [0.001]\n",
      "Epoch 14/500 - Train Loss: 0.28805253\n",
      "Epoch 14/500 - Test Loss: 0.29588151\n",
      "lr of epoch 14 => [0.001]\n",
      "Epoch 15/500 - Train Loss: 0.28392667\n",
      "Epoch 15/500 - Test Loss: 0.29114116\n",
      "lr of epoch 15 => [0.001]\n",
      "Epoch 16/500 - Train Loss: 0.28029043\n",
      "Epoch 16/500 - Test Loss: 0.28771036\n",
      "lr of epoch 16 => [0.001]\n",
      "Epoch 17/500 - Train Loss: 0.27648811\n",
      "Epoch 17/500 - Test Loss: 0.28464473\n",
      "lr of epoch 17 => [0.001]\n",
      "Epoch 19/500 - Train Loss: 0.27065082\n",
      "Epoch 19/500 - Test Loss: 0.27825437\n",
      "lr of epoch 19 => [0.001]\n",
      "Epoch 20/500 - Train Loss: 0.26810824\n",
      "Epoch 20/500 - Test Loss: 0.27587948\n",
      "lr of epoch 20 => [0.001]\n",
      "Epoch 21/500 - Train Loss: 0.26558409\n",
      "Epoch 21/500 - Test Loss: 0.27370538\n",
      "lr of epoch 21 => [0.001]\n",
      "Epoch 22/500 - Train Loss: 0.26340302\n",
      "Epoch 22/500 - Test Loss: 0.27140254\n",
      "lr of epoch 22 => [0.001]\n",
      "Epoch 23/500 - Train Loss: 0.26159651\n",
      "Epoch 23/500 - Test Loss: 0.26934731\n",
      "lr of epoch 23 => [0.001]\n",
      "Epoch 24/500 - Train Loss: 0.25978908\n",
      "Epoch 24/500 - Test Loss: 0.26830520\n",
      "lr of epoch 24 => [0.001]\n",
      "Epoch 25/500 - Train Loss: 0.25831235\n",
      "Epoch 25/500 - Test Loss: 0.26607388\n",
      "lr of epoch 25 => [0.001]\n",
      "Epoch 26/500 - Train Loss: 0.25684012\n",
      "Epoch 26/500 - Test Loss: 0.26557803\n",
      "lr of epoch 26 => [0.001]\n",
      "Epoch 27/500 - Train Loss: 0.25560518\n",
      "Epoch 27/500 - Test Loss: 0.26333762\n",
      "lr of epoch 27 => [0.001]\n",
      "Epoch 28/500 - Train Loss: 0.25425140\n",
      "Epoch 28/500 - Test Loss: 0.26227959\n",
      "lr of epoch 28 => [0.001]\n",
      "Epoch 29/500 - Train Loss: 0.25319270\n",
      "Epoch 29/500 - Test Loss: 0.26196390\n",
      "lr of epoch 29 => [0.001]\n",
      "Epoch 30/500 - Train Loss: 0.25225926\n",
      "Epoch 30/500 - Test Loss: 0.26017330\n",
      "lr of epoch 30 => [0.001]\n",
      "Epoch 31/500 - Train Loss: 0.25105708\n",
      "Epoch 31/500 - Test Loss: 0.25894170\n",
      "lr of epoch 31 => [0.001]\n",
      "Epoch 32/500 - Train Loss: 0.25035825\n",
      "Epoch 32/500 - Test Loss: 0.25900533\n",
      "lr of epoch 32 => [0.001]\n",
      "Epoch 33/500 - Train Loss: 0.24975363\n",
      "Epoch 33/500 - Test Loss: 0.25776835\n",
      "lr of epoch 33 => [0.001]\n",
      "Epoch 34/500 - Train Loss: 0.24877509\n",
      "Epoch 34/500 - Test Loss: 0.25640293\n",
      "lr of epoch 34 => [0.001]\n",
      "Epoch 35/500 - Train Loss: 0.24780332\n",
      "Epoch 35/500 - Test Loss: 0.25741677\n",
      "lr of epoch 35 => [0.001]\n",
      "Epoch 36/500 - Train Loss: 0.24753537\n",
      "Epoch 36/500 - Test Loss: 0.25554545\n",
      "lr of epoch 36 => [0.001]\n",
      "Epoch 37/500 - Train Loss: 0.24648231\n",
      "Epoch 37/500 - Test Loss: 0.25392191\n",
      "lr of epoch 37 => [0.001]\n",
      "Epoch 38/500 - Train Loss: 0.24591904\n",
      "Epoch 38/500 - Test Loss: 0.25380840\n",
      "lr of epoch 38 => [0.001]\n",
      "Epoch 39/500 - Train Loss: 0.24545400\n",
      "Epoch 39/500 - Test Loss: 0.25350130\n",
      "lr of epoch 39 => [0.001]\n",
      "Epoch 40/500 - Train Loss: 0.24521057\n",
      "Epoch 40/500 - Test Loss: 0.25319374\n",
      "lr of epoch 40 => [0.001]\n",
      "Epoch 41/500 - Train Loss: 0.24484491\n",
      "Epoch 41/500 - Test Loss: 0.25341716\n",
      "lr of epoch 41 => [0.001]\n",
      "Epoch 42/500 - Train Loss: 0.24642745\n",
      "Epoch 42/500 - Test Loss: 0.25194947\n",
      "lr of epoch 42 => [0.001]\n",
      "Epoch 43/500 - Train Loss: 0.24347448\n",
      "Epoch 43/500 - Test Loss: 0.25108258\n",
      "lr of epoch 43 => [0.001]\n",
      "Epoch 44/500 - Train Loss: 0.24276199\n",
      "Epoch 44/500 - Test Loss: 0.25077016\n",
      "lr of epoch 44 => [0.001]\n",
      "Epoch 45/500 - Train Loss: 0.24231233\n",
      "Epoch 45/500 - Test Loss: 0.25057914\n",
      "lr of epoch 45 => [0.001]\n",
      "Epoch 46/500 - Train Loss: 0.24199469\n",
      "Epoch 46/500 - Test Loss: 0.25000267\n",
      "lr of epoch 46 => [0.001]\n",
      "Epoch 47/500 - Train Loss: 0.24222378\n",
      "Epoch 47/500 - Test Loss: 0.24973584\n",
      "lr of epoch 47 => [0.001]\n",
      "Epoch 48/500 - Train Loss: 0.24174406\n",
      "Epoch 48/500 - Test Loss: 0.25027273\n",
      "lr of epoch 48 => [0.001]\n",
      "Epoch 49/500 - Train Loss: 0.24141499\n",
      "Epoch 49/500 - Test Loss: 0.24893264\n",
      "lr of epoch 49 => [0.001]\n",
      "Epoch 50/500 - Train Loss: 0.24083933\n",
      "Epoch 50/500 - Test Loss: 0.24870135\n",
      "lr of epoch 50 => [0.001]\n",
      "Epoch 51/500 - Train Loss: 0.24028666\n",
      "Epoch 51/500 - Test Loss: 0.24977543\n",
      "lr of epoch 51 => [0.001]\n",
      "Epoch 52/500 - Train Loss: 0.24043270\n",
      "Epoch 52/500 - Test Loss: 0.24767717\n",
      "lr of epoch 52 => [0.001]\n",
      "Epoch 53/500 - Train Loss: 0.23970139\n",
      "Epoch 53/500 - Test Loss: 0.24880100\n",
      "lr of epoch 53 => [0.001]\n",
      "Epoch 54/500 - Train Loss: 0.24010648\n",
      "Epoch 54/500 - Test Loss: 0.24785479\n",
      "lr of epoch 54 => [0.001]\n",
      "Epoch 55/500 - Train Loss: 0.23938429\n",
      "Epoch 55/500 - Test Loss: 0.24668461\n",
      "lr of epoch 55 => [0.001]\n",
      "Epoch 56/500 - Train Loss: 0.23892817\n",
      "Epoch 56/500 - Test Loss: 0.24655975\n",
      "lr of epoch 56 => [0.001]\n",
      "Epoch 57/500 - Train Loss: 0.23839388\n",
      "Epoch 57/500 - Test Loss: 0.24629032\n",
      "lr of epoch 57 => [0.001]\n",
      "Epoch 58/500 - Train Loss: 0.23812671\n",
      "Epoch 58/500 - Test Loss: 0.24580289\n",
      "lr of epoch 58 => [0.001]\n",
      "Epoch 59/500 - Train Loss: 0.23770857\n",
      "Epoch 59/500 - Test Loss: 0.24579954\n",
      "lr of epoch 59 => [0.001]\n",
      "Epoch 60/500 - Train Loss: 0.23756372\n",
      "Epoch 60/500 - Test Loss: 0.24556270\n",
      "lr of epoch 60 => [0.001]\n",
      "Epoch 61/500 - Train Loss: 0.23719216\n",
      "Epoch 61/500 - Test Loss: 0.24551595\n",
      "lr of epoch 61 => [0.001]\n",
      "Epoch 62/500 - Train Loss: 0.23699367\n",
      "Epoch 62/500 - Test Loss: 0.24542995\n",
      "lr of epoch 62 => [0.001]\n",
      "Epoch 63/500 - Train Loss: 0.23673562\n",
      "Epoch 63/500 - Test Loss: 0.24517562\n",
      "lr of epoch 63 => [0.001]\n",
      "Epoch 64/500 - Train Loss: 0.23629926\n",
      "Epoch 64/500 - Test Loss: 0.24503024\n",
      "lr of epoch 64 => [0.001]\n",
      "Epoch 65/500 - Train Loss: 0.23609079\n",
      "Epoch 65/500 - Test Loss: 0.24470125\n",
      "lr of epoch 65 => [0.001]\n",
      "Epoch 66/500 - Train Loss: 0.23605842\n",
      "Epoch 66/500 - Test Loss: 0.24431589\n",
      "lr of epoch 66 => [0.001]\n",
      "Epoch 67/500 - Train Loss: 0.23562043\n",
      "Epoch 67/500 - Test Loss: 0.24408021\n",
      "lr of epoch 67 => [0.001]\n",
      "Epoch 68/500 - Train Loss: 0.23534450\n",
      "Epoch 68/500 - Test Loss: 0.24403676\n",
      "lr of epoch 68 => [0.001]\n",
      "Epoch 69/500 - Train Loss: 0.23520608\n",
      "Epoch 69/500 - Test Loss: 0.24320779\n",
      "lr of epoch 69 => [0.001]\n",
      "Epoch 70/500 - Train Loss: 0.23563977\n",
      "Epoch 70/500 - Test Loss: 0.24358752\n",
      "lr of epoch 70 => [0.001]\n",
      "Epoch 71/500 - Train Loss: 0.23508809\n",
      "Epoch 71/500 - Test Loss: 0.24403475\n",
      "lr of epoch 71 => [0.001]\n",
      "Epoch 72/500 - Train Loss: 0.23457695\n",
      "Epoch 72/500 - Test Loss: 0.24264685\n",
      "lr of epoch 72 => [0.001]\n",
      "Epoch 73/500 - Train Loss: 0.23427333\n",
      "Epoch 73/500 - Test Loss: 0.24265712\n",
      "lr of epoch 73 => [0.001]\n",
      "Epoch 74/500 - Train Loss: 0.23373798\n",
      "Epoch 74/500 - Test Loss: 0.24188123\n",
      "lr of epoch 74 => [0.001]\n",
      "Epoch 75/500 - Train Loss: 0.23378653\n",
      "Epoch 75/500 - Test Loss: 0.24262063\n",
      "lr of epoch 75 => [0.001]\n",
      "Epoch 76/500 - Train Loss: 0.23352412\n",
      "Epoch 76/500 - Test Loss: 0.24144727\n",
      "lr of epoch 76 => [0.001]\n",
      "Epoch 77/500 - Train Loss: 0.23314338\n",
      "Epoch 77/500 - Test Loss: 0.24135624\n",
      "lr of epoch 77 => [0.001]\n",
      "Epoch 78/500 - Train Loss: 0.23235053\n",
      "Epoch 78/500 - Test Loss: 0.24051688\n",
      "lr of epoch 78 => [0.001]\n",
      "Epoch 79/500 - Train Loss: 0.23184218\n",
      "Epoch 79/500 - Test Loss: 0.24035116\n",
      "lr of epoch 79 => [0.001]\n",
      "Epoch 80/500 - Train Loss: 0.23182937\n",
      "Epoch 80/500 - Test Loss: 0.24103028\n",
      "lr of epoch 80 => [0.001]\n",
      "Epoch 81/500 - Train Loss: 0.23246050\n",
      "Epoch 81/500 - Test Loss: 0.24162741\n",
      "lr of epoch 81 => [0.001]\n",
      "Epoch 82/500 - Train Loss: 0.23178905\n",
      "Epoch 82/500 - Test Loss: 0.24016380\n",
      "lr of epoch 82 => [0.001]\n",
      "Epoch 83/500 - Train Loss: 0.23166310\n",
      "Epoch 83/500 - Test Loss: 0.24109819\n",
      "lr of epoch 83 => [0.001]\n",
      "Epoch 84/500 - Train Loss: 0.23158631\n",
      "Epoch 84/500 - Test Loss: 0.23974734\n",
      "lr of epoch 84 => [0.001]\n",
      "Epoch 85/500 - Train Loss: 0.23067680\n",
      "Epoch 85/500 - Test Loss: 0.23950734\n",
      "lr of epoch 85 => [0.001]\n",
      "Epoch 86/500 - Train Loss: 0.23064165\n",
      "Epoch 86/500 - Test Loss: 0.23953721\n",
      "lr of epoch 86 => [0.001]\n",
      "Epoch 87/500 - Train Loss: 0.23006196\n",
      "Epoch 87/500 - Test Loss: 0.23929065\n",
      "lr of epoch 87 => [0.001]\n",
      "Epoch 88/500 - Train Loss: 0.22982994\n",
      "Epoch 88/500 - Test Loss: 0.23913973\n",
      "lr of epoch 88 => [0.001]\n",
      "Epoch 89/500 - Train Loss: 0.22915656\n",
      "Epoch 89/500 - Test Loss: 0.23865184\n",
      "lr of epoch 89 => [0.001]\n",
      "Epoch 90/500 - Train Loss: 0.22928530\n",
      "Epoch 90/500 - Test Loss: 0.23887030\n",
      "lr of epoch 90 => [0.001]\n",
      "Epoch 91/500 - Train Loss: 0.22906257\n",
      "Epoch 91/500 - Test Loss: 0.23761396\n",
      "lr of epoch 91 => [0.001]\n",
      "Epoch 92/500 - Train Loss: 0.22847473\n",
      "Epoch 92/500 - Test Loss: 0.23762509\n",
      "lr of epoch 92 => [0.001]\n",
      "Epoch 93/500 - Train Loss: 0.22800651\n",
      "Epoch 93/500 - Test Loss: 0.23739421\n",
      "lr of epoch 93 => [0.001]\n",
      "Epoch 94/500 - Train Loss: 0.22779669\n",
      "Epoch 94/500 - Test Loss: 0.23741438\n",
      "lr of epoch 94 => [0.001]\n",
      "Epoch 95/500 - Train Loss: 0.22802526\n",
      "Epoch 95/500 - Test Loss: 0.23713943\n",
      "lr of epoch 95 => [0.001]\n",
      "Epoch 96/500 - Train Loss: 0.22796344\n",
      "Epoch 96/500 - Test Loss: 0.23749495\n",
      "lr of epoch 96 => [0.001]\n",
      "Epoch 97/500 - Train Loss: 0.22723691\n",
      "Epoch 97/500 - Test Loss: 0.23688183\n",
      "lr of epoch 97 => [0.001]\n",
      "Epoch 98/500 - Train Loss: 0.22700027\n",
      "Epoch 98/500 - Test Loss: 0.23843260\n",
      "lr of epoch 98 => [0.001]\n",
      "Epoch 99/500 - Train Loss: 0.22777756\n",
      "Epoch 99/500 - Test Loss: 0.23651891\n",
      "lr of epoch 99 => [0.001]\n",
      "Epoch 100/500 - Train Loss: 0.22695352\n",
      "Epoch 100/500 - Test Loss: 0.23664456\n",
      "lr of epoch 100 => [0.001]\n",
      "Epoch 101/500 - Train Loss: 0.22659248\n",
      "Epoch 101/500 - Test Loss: 0.23673095\n",
      "lr of epoch 101 => [0.001]\n",
      "Epoch 102/500 - Train Loss: 0.22633307\n",
      "Epoch 102/500 - Test Loss: 0.23587280\n",
      "lr of epoch 102 => [0.001]\n",
      "Epoch 103/500 - Train Loss: 0.22675570\n",
      "Epoch 103/500 - Test Loss: 0.23755217\n",
      "lr of epoch 103 => [0.001]\n",
      "Epoch 104/500 - Train Loss: 0.22654526\n",
      "Epoch 104/500 - Test Loss: 0.23615837\n",
      "lr of epoch 104 => [0.001]\n",
      "Epoch 105/500 - Train Loss: 0.22578720\n",
      "Epoch 105/500 - Test Loss: 0.23555993\n",
      "lr of epoch 105 => [0.001]\n",
      "Epoch 106/500 - Train Loss: 0.22534049\n",
      "Epoch 106/500 - Test Loss: 0.23594575\n",
      "lr of epoch 106 => [0.001]\n",
      "Epoch 107/500 - Train Loss: 0.22575892\n",
      "Epoch 107/500 - Test Loss: 0.23573535\n",
      "lr of epoch 107 => [0.001]\n",
      "Epoch 108/500 - Train Loss: 0.22572321\n",
      "Epoch 108/500 - Test Loss: 0.23593430\n",
      "lr of epoch 108 => [0.001]\n",
      "Epoch 109/500 - Train Loss: 0.22546851\n",
      "Epoch 109/500 - Test Loss: 0.23517629\n",
      "lr of epoch 109 => [0.001]\n",
      "Epoch 110/500 - Train Loss: 0.22532050\n",
      "Epoch 110/500 - Test Loss: 0.23563578\n",
      "lr of epoch 110 => [0.001]\n",
      "Epoch 111/500 - Train Loss: 0.22532680\n",
      "Epoch 111/500 - Test Loss: 0.23588803\n",
      "lr of epoch 111 => [0.001]\n",
      "Epoch 112/500 - Train Loss: 0.22512978\n",
      "Epoch 112/500 - Test Loss: 0.23535935\n",
      "lr of epoch 112 => [0.001]\n",
      "Epoch 113/500 - Train Loss: 0.22543137\n",
      "Epoch 113/500 - Test Loss: 0.23603753\n",
      "lr of epoch 113 => [0.001]\n",
      "Epoch 114/500 - Train Loss: 0.22531952\n",
      "Epoch 114/500 - Test Loss: 0.23562374\n",
      "lr of epoch 114 => [0.001]\n",
      "Epoch 115/500 - Train Loss: 0.22572217\n",
      "Epoch 115/500 - Test Loss: 0.23560579\n",
      "lr of epoch 115 => [0.001]\n",
      "Epoch 116/500 - Train Loss: 0.22478613\n",
      "Epoch 116/500 - Test Loss: 0.23469942\n",
      "lr of epoch 116 => [0.001]\n",
      "Epoch 117/500 - Train Loss: 0.22466536\n",
      "Epoch 117/500 - Test Loss: 0.23474455\n",
      "lr of epoch 117 => [0.001]\n",
      "Epoch 118/500 - Train Loss: 0.22480257\n",
      "Epoch 118/500 - Test Loss: 0.23679890\n",
      "lr of epoch 118 => [0.001]\n",
      "Epoch 119/500 - Train Loss: 0.22487157\n",
      "Epoch 119/500 - Test Loss: 0.23474347\n",
      "lr of epoch 119 => [0.001]\n",
      "Epoch 120/500 - Train Loss: 0.22445111\n",
      "Epoch 120/500 - Test Loss: 0.23447268\n",
      "lr of epoch 120 => [0.001]\n",
      "Epoch 121/500 - Train Loss: 0.22431067\n",
      "Epoch 121/500 - Test Loss: 0.23455749\n",
      "lr of epoch 121 => [0.001]\n",
      "Epoch 122/500 - Train Loss: 0.22378507\n",
      "Epoch 122/500 - Test Loss: 0.23384324\n",
      "lr of epoch 122 => [0.001]\n",
      "Epoch 123/500 - Train Loss: 0.22367943\n",
      "Epoch 123/500 - Test Loss: 0.23388554\n",
      "lr of epoch 123 => [0.001]\n",
      "Epoch 124/500 - Train Loss: 0.22323656\n",
      "Epoch 124/500 - Test Loss: 0.23415500\n",
      "lr of epoch 124 => [0.001]\n",
      "Epoch 125/500 - Train Loss: 0.22315436\n",
      "Epoch 125/500 - Test Loss: 0.23408224\n",
      "lr of epoch 125 => [0.001]\n",
      "Epoch 126/500 - Train Loss: 0.22310414\n",
      "Epoch 126/500 - Test Loss: 0.23330191\n",
      "lr of epoch 126 => [0.001]\n",
      "Epoch 127/500 - Train Loss: 0.22299432\n",
      "Epoch 127/500 - Test Loss: 0.23354790\n",
      "lr of epoch 127 => [0.001]\n",
      "Epoch 128/500 - Train Loss: 0.22291201\n",
      "Epoch 128/500 - Test Loss: 0.23404971\n",
      "lr of epoch 128 => [0.001]\n",
      "Epoch 129/500 - Train Loss: 0.22270136\n",
      "Epoch 129/500 - Test Loss: 0.23343197\n",
      "lr of epoch 129 => [0.001]\n",
      "Epoch 130/500 - Train Loss: 0.22300694\n",
      "Epoch 130/500 - Test Loss: 0.23386788\n",
      "lr of epoch 130 => [0.001]\n",
      "Epoch 131/500 - Train Loss: 0.22347008\n",
      "Epoch 131/500 - Test Loss: 0.23342708\n",
      "lr of epoch 131 => [0.001]\n",
      "Epoch 132/500 - Train Loss: 0.22317079\n",
      "Epoch 132/500 - Test Loss: 0.23474109\n",
      "lr of epoch 132 => [0.001]\n",
      "Epoch 133/500 - Train Loss: 0.22286528\n",
      "Epoch 133/500 - Test Loss: 0.23347183\n",
      "lr of epoch 133 => [0.001]\n",
      "Epoch 134/500 - Train Loss: 0.22249743\n",
      "Epoch 134/500 - Test Loss: 0.23281223\n",
      "lr of epoch 134 => [0.001]\n",
      "Epoch 135/500 - Train Loss: 0.22177910\n",
      "Epoch 135/500 - Test Loss: 0.23242564\n",
      "lr of epoch 135 => [0.001]\n",
      "Epoch 136/500 - Train Loss: 0.22200369\n",
      "Epoch 136/500 - Test Loss: 0.23264770\n",
      "lr of epoch 136 => [0.001]\n",
      "Epoch 137/500 - Train Loss: 0.22190878\n",
      "Epoch 137/500 - Test Loss: 0.23305455\n",
      "lr of epoch 137 => [0.001]\n",
      "Epoch 138/500 - Train Loss: 0.22169368\n",
      "Epoch 138/500 - Test Loss: 0.23309839\n",
      "lr of epoch 138 => [0.001]\n",
      "Epoch 139/500 - Train Loss: 0.22172054\n",
      "Epoch 139/500 - Test Loss: 0.23274496\n",
      "lr of epoch 139 => [0.001]\n",
      "Epoch 140/500 - Train Loss: 0.22145958\n",
      "Epoch 140/500 - Test Loss: 0.23202018\n",
      "lr of epoch 140 => [0.001]\n",
      "Epoch 141/500 - Train Loss: 0.22130575\n",
      "Epoch 141/500 - Test Loss: 0.23293548\n",
      "lr of epoch 141 => [0.001]\n",
      "Epoch 142/500 - Train Loss: 0.22087845\n",
      "Epoch 142/500 - Test Loss: 0.23226444\n",
      "lr of epoch 142 => [0.001]\n",
      "Epoch 143/500 - Train Loss: 0.22074183\n",
      "Epoch 143/500 - Test Loss: 0.23219526\n",
      "lr of epoch 143 => [0.001]\n",
      "Epoch 144/500 - Train Loss: 0.22099753\n",
      "Epoch 144/500 - Test Loss: 0.23231572\n",
      "lr of epoch 144 => [0.001]\n",
      "Epoch 145/500 - Train Loss: 0.22078162\n",
      "Epoch 145/500 - Test Loss: 0.23204858\n",
      "lr of epoch 145 => [0.001]\n",
      "Epoch 146/500 - Train Loss: 0.22048730\n",
      "Epoch 146/500 - Test Loss: 0.23193677\n",
      "lr of epoch 146 => [0.001]\n",
      "Epoch 147/500 - Train Loss: 0.22056159\n",
      "Epoch 147/500 - Test Loss: 0.23179595\n",
      "lr of epoch 147 => [0.001]\n",
      "Epoch 148/500 - Train Loss: 0.22022434\n",
      "Epoch 148/500 - Test Loss: 0.23192819\n",
      "lr of epoch 148 => [0.001]\n",
      "Epoch 149/500 - Train Loss: 0.22009777\n",
      "Epoch 149/500 - Test Loss: 0.23170509\n",
      "lr of epoch 149 => [0.001]\n",
      "Epoch 150/500 - Train Loss: 0.22003451\n",
      "Epoch 150/500 - Test Loss: 0.23199555\n",
      "lr of epoch 150 => [0.001]\n",
      "Epoch 151/500 - Train Loss: 0.22026962\n",
      "Epoch 151/500 - Test Loss: 0.23172908\n",
      "lr of epoch 151 => [0.001]\n",
      "Epoch 152/500 - Train Loss: 0.21996713\n",
      "Epoch 152/500 - Test Loss: 0.23192314\n",
      "lr of epoch 152 => [0.001]\n",
      "Epoch 154/500 - Train Loss: 0.21961060\n",
      "Epoch 154/500 - Test Loss: 0.23184373\n",
      "lr of epoch 154 => [0.001]\n",
      "Epoch 155/500 - Train Loss: 0.21943814\n",
      "Epoch 155/500 - Test Loss: 0.23070979\n",
      "lr of epoch 155 => [0.001]\n",
      "Epoch 156/500 - Train Loss: 0.21893941\n",
      "Epoch 156/500 - Test Loss: 0.23113079\n",
      "lr of epoch 156 => [0.001]\n",
      "Epoch 157/500 - Train Loss: 0.21897926\n",
      "Epoch 157/500 - Test Loss: 0.23107907\n",
      "lr of epoch 157 => [0.001]\n",
      "Epoch 158/500 - Train Loss: 0.21917940\n",
      "Epoch 158/500 - Test Loss: 0.23119896\n",
      "lr of epoch 158 => [0.001]\n",
      "Epoch 159/500 - Train Loss: 0.21945834\n",
      "Epoch 159/500 - Test Loss: 0.23117489\n",
      "lr of epoch 159 => [0.001]\n",
      "Epoch 160/500 - Train Loss: 0.21914083\n",
      "Epoch 160/500 - Test Loss: 0.23056269\n",
      "lr of epoch 160 => [0.001]\n",
      "Epoch 161/500 - Train Loss: 0.21876878\n",
      "Epoch 161/500 - Test Loss: 0.23094423\n",
      "lr of epoch 161 => [0.001]\n",
      "Epoch 162/500 - Train Loss: 0.21862592\n",
      "Epoch 162/500 - Test Loss: 0.23048969\n",
      "lr of epoch 162 => [0.001]\n",
      "Epoch 163/500 - Train Loss: 0.21884454\n",
      "Epoch 163/500 - Test Loss: 0.23063533\n",
      "lr of epoch 163 => [0.001]\n",
      "Epoch 164/500 - Train Loss: 0.21882178\n",
      "Epoch 164/500 - Test Loss: 0.23174938\n",
      "lr of epoch 164 => [0.001]\n",
      "Epoch 165/500 - Train Loss: 0.21873607\n",
      "Epoch 165/500 - Test Loss: 0.23022230\n",
      "lr of epoch 165 => [0.001]\n",
      "Epoch 166/500 - Train Loss: 0.21865169\n",
      "Epoch 166/500 - Test Loss: 0.23080825\n",
      "lr of epoch 166 => [0.001]\n",
      "Epoch 167/500 - Train Loss: 0.21799852\n",
      "Epoch 167/500 - Test Loss: 0.22993554\n",
      "lr of epoch 167 => [0.001]\n",
      "Epoch 168/500 - Train Loss: 0.21841774\n",
      "Epoch 168/500 - Test Loss: 0.22996890\n",
      "lr of epoch 168 => [0.001]\n",
      "Epoch 169/500 - Train Loss: 0.21807949\n",
      "Epoch 169/500 - Test Loss: 0.23023831\n",
      "lr of epoch 169 => [0.001]\n",
      "Epoch 170/500 - Train Loss: 0.21839309\n",
      "Epoch 170/500 - Test Loss: 0.22996137\n",
      "lr of epoch 170 => [0.001]\n",
      "Epoch 171/500 - Train Loss: 0.21833951\n",
      "Epoch 171/500 - Test Loss: 0.23011799\n",
      "lr of epoch 171 => [0.001]\n",
      "Epoch 172/500 - Train Loss: 0.21792609\n",
      "Epoch 172/500 - Test Loss: 0.23061152\n",
      "lr of epoch 172 => [0.001]\n",
      "Epoch 173/500 - Train Loss: 0.21833698\n",
      "Epoch 173/500 - Test Loss: 0.23016484\n",
      "lr of epoch 173 => [0.001]\n",
      "Epoch 174/500 - Train Loss: 0.21769043\n",
      "Epoch 174/500 - Test Loss: 0.23014551\n",
      "lr of epoch 174 => [0.001]\n",
      "Epoch 175/500 - Train Loss: 0.21787028\n",
      "Epoch 175/500 - Test Loss: 0.23008134\n",
      "lr of epoch 175 => [0.001]\n",
      "Epoch 176/500 - Train Loss: 0.21751128\n",
      "Epoch 176/500 - Test Loss: 0.22977399\n",
      "lr of epoch 176 => [0.001]\n",
      "Epoch 177/500 - Train Loss: 0.21731908\n",
      "Epoch 177/500 - Test Loss: 0.22954369\n",
      "lr of epoch 177 => [0.001]\n",
      "Epoch 178/500 - Train Loss: 0.21716272\n",
      "Epoch 178/500 - Test Loss: 0.22959084\n",
      "lr of epoch 178 => [0.001]\n",
      "Epoch 179/500 - Train Loss: 0.21758104\n",
      "Epoch 179/500 - Test Loss: 0.22956193\n",
      "lr of epoch 179 => [0.001]\n",
      "Epoch 180/500 - Train Loss: 0.21768672\n",
      "Epoch 180/500 - Test Loss: 0.22967957\n",
      "lr of epoch 180 => [0.001]\n",
      "Epoch 181/500 - Train Loss: 0.21732696\n",
      "Epoch 181/500 - Test Loss: 0.23018145\n",
      "lr of epoch 181 => [0.001]\n",
      "Epoch 182/500 - Train Loss: 0.21714411\n",
      "Epoch 182/500 - Test Loss: 0.22938737\n",
      "lr of epoch 182 => [0.001]\n",
      "Epoch 183/500 - Train Loss: 0.21760475\n",
      "Epoch 183/500 - Test Loss: 0.22980932\n",
      "lr of epoch 183 => [0.001]\n",
      "Epoch 184/500 - Train Loss: 0.21719565\n",
      "Epoch 184/500 - Test Loss: 0.23074322\n",
      "lr of epoch 184 => [0.001]\n",
      "Epoch 185/500 - Train Loss: 0.21703235\n",
      "Epoch 185/500 - Test Loss: 0.22909043\n",
      "lr of epoch 185 => [0.001]\n",
      "Epoch 186/500 - Train Loss: 0.21647862\n",
      "Epoch 186/500 - Test Loss: 0.22897261\n",
      "lr of epoch 186 => [0.001]\n",
      "Epoch 187/500 - Train Loss: 0.21622203\n",
      "Epoch 187/500 - Test Loss: 0.22899802\n",
      "lr of epoch 187 => [0.001]\n",
      "Epoch 188/500 - Train Loss: 0.21645130\n",
      "Epoch 188/500 - Test Loss: 0.22920984\n",
      "lr of epoch 188 => [0.001]\n",
      "Epoch 189/500 - Train Loss: 0.21572881\n",
      "Epoch 189/500 - Test Loss: 0.22842207\n",
      "lr of epoch 189 => [0.001]\n",
      "Epoch 190/500 - Train Loss: 0.21549034\n",
      "Epoch 190/500 - Test Loss: 0.22909193\n",
      "lr of epoch 190 => [0.001]\n",
      "Epoch 191/500 - Train Loss: 0.21561219\n",
      "Epoch 191/500 - Test Loss: 0.22805475\n",
      "lr of epoch 191 => [0.001]\n",
      "Epoch 192/500 - Train Loss: 0.21527722\n",
      "Epoch 192/500 - Test Loss: 0.22815471\n",
      "lr of epoch 192 => [0.001]\n",
      "Epoch 193/500 - Train Loss: 0.21534134\n",
      "Epoch 193/500 - Test Loss: 0.22845353\n",
      "lr of epoch 193 => [0.001]\n",
      "Epoch 194/500 - Train Loss: 0.21559796\n",
      "Epoch 194/500 - Test Loss: 0.22779381\n",
      "lr of epoch 194 => [0.001]\n",
      "Epoch 195/500 - Train Loss: 0.21480240\n",
      "Epoch 195/500 - Test Loss: 0.22729842\n",
      "lr of epoch 195 => [0.001]\n",
      "Epoch 196/500 - Train Loss: 0.21455615\n",
      "Epoch 196/500 - Test Loss: 0.22723518\n",
      "lr of epoch 196 => [0.001]\n",
      "Epoch 197/500 - Train Loss: 0.21457828\n",
      "Epoch 197/500 - Test Loss: 0.22849688\n",
      "lr of epoch 197 => [0.001]\n",
      "Epoch 198/500 - Train Loss: 0.21451563\n",
      "Epoch 198/500 - Test Loss: 0.22767692\n",
      "lr of epoch 198 => [0.001]\n",
      "Epoch 199/500 - Train Loss: 0.21427638\n",
      "Epoch 199/500 - Test Loss: 0.22784832\n",
      "lr of epoch 199 => [0.001]\n",
      "Epoch 200/500 - Train Loss: 0.21484266\n",
      "Epoch 200/500 - Test Loss: 0.22753484\n",
      "lr of epoch 200 => [0.001]\n",
      "Epoch 201/500 - Train Loss: 0.21452516\n",
      "Epoch 201/500 - Test Loss: 0.22695665\n",
      "lr of epoch 201 => [0.001]\n",
      "Epoch 202/500 - Train Loss: 0.21357217\n",
      "Epoch 202/500 - Test Loss: 0.22662554\n",
      "lr of epoch 202 => [0.001]\n",
      "Epoch 203/500 - Train Loss: 0.21342759\n",
      "Epoch 203/500 - Test Loss: 0.22656371\n",
      "lr of epoch 203 => [0.001]\n",
      "Epoch 204/500 - Train Loss: 0.21356904\n",
      "Epoch 204/500 - Test Loss: 0.22660169\n",
      "lr of epoch 204 => [0.001]\n",
      "Epoch 205/500 - Train Loss: 0.21360500\n",
      "Epoch 205/500 - Test Loss: 0.22742207\n",
      "lr of epoch 205 => [0.001]\n",
      "Epoch 206/500 - Train Loss: 0.21365072\n",
      "Epoch 206/500 - Test Loss: 0.22728824\n",
      "lr of epoch 206 => [0.001]\n",
      "Epoch 207/500 - Train Loss: 0.21347816\n",
      "Epoch 207/500 - Test Loss: 0.22728864\n",
      "lr of epoch 207 => [0.001]\n",
      "Epoch 208/500 - Train Loss: 0.21435767\n",
      "Epoch 208/500 - Test Loss: 0.22764449\n",
      "lr of epoch 208 => [0.001]\n",
      "Epoch 209/500 - Train Loss: 0.21337343\n",
      "Epoch 209/500 - Test Loss: 0.22643032\n",
      "lr of epoch 209 => [0.001]\n",
      "Epoch 210/500 - Train Loss: 0.21277833\n",
      "Epoch 210/500 - Test Loss: 0.22574025\n",
      "lr of epoch 210 => [0.001]\n",
      "Epoch 211/500 - Train Loss: 0.21233554\n",
      "Epoch 211/500 - Test Loss: 0.22578779\n",
      "lr of epoch 211 => [0.001]\n",
      "Epoch 212/500 - Train Loss: 0.21200591\n",
      "Epoch 212/500 - Test Loss: 0.22551285\n",
      "lr of epoch 212 => [0.001]\n",
      "Epoch 213/500 - Train Loss: 0.21210169\n",
      "Epoch 213/500 - Test Loss: 0.22567244\n",
      "lr of epoch 213 => [0.001]\n",
      "Epoch 214/500 - Train Loss: 0.21186213\n",
      "Epoch 214/500 - Test Loss: 0.22588535\n",
      "lr of epoch 214 => [0.001]\n",
      "Epoch 215/500 - Train Loss: 0.21214832\n",
      "Epoch 215/500 - Test Loss: 0.22615686\n",
      "lr of epoch 215 => [0.001]\n",
      "Epoch 216/500 - Train Loss: 0.21217563\n",
      "Epoch 216/500 - Test Loss: 0.22578816\n",
      "lr of epoch 216 => [0.001]\n",
      "Epoch 217/500 - Train Loss: 0.21228632\n",
      "Epoch 217/500 - Test Loss: 0.22543935\n",
      "lr of epoch 217 => [0.001]\n",
      "Epoch 218/500 - Train Loss: 0.21194672\n",
      "Epoch 218/500 - Test Loss: 0.22638556\n",
      "lr of epoch 218 => [0.001]\n",
      "Epoch 219/500 - Train Loss: 0.21199158\n",
      "Epoch 219/500 - Test Loss: 0.22533059\n",
      "lr of epoch 219 => [0.001]\n",
      "Epoch 220/500 - Train Loss: 0.21181787\n",
      "Epoch 220/500 - Test Loss: 0.22488385\n",
      "lr of epoch 220 => [0.001]\n",
      "Epoch 221/500 - Train Loss: 0.21168287\n",
      "Epoch 221/500 - Test Loss: 0.22566976\n",
      "lr of epoch 221 => [0.001]\n",
      "Epoch 222/500 - Train Loss: 0.21124916\n",
      "Epoch 222/500 - Test Loss: 0.22478474\n",
      "lr of epoch 222 => [0.001]\n",
      "Epoch 223/500 - Train Loss: 0.21114918\n",
      "Epoch 223/500 - Test Loss: 0.22527221\n",
      "lr of epoch 223 => [0.001]\n",
      "Epoch 224/500 - Train Loss: 0.21139186\n",
      "Epoch 224/500 - Test Loss: 0.22487751\n",
      "lr of epoch 224 => [0.001]\n",
      "Epoch 225/500 - Train Loss: 0.21105692\n",
      "Epoch 225/500 - Test Loss: 0.22464698\n",
      "lr of epoch 225 => [0.001]\n",
      "Epoch 226/500 - Train Loss: 0.21139735\n",
      "Epoch 226/500 - Test Loss: 0.22522009\n",
      "lr of epoch 226 => [0.001]\n",
      "Epoch 227/500 - Train Loss: 0.21120191\n",
      "Epoch 227/500 - Test Loss: 0.22490802\n",
      "lr of epoch 227 => [0.001]\n",
      "Epoch 229/500 - Train Loss: 0.21134647\n",
      "Epoch 229/500 - Test Loss: 0.22470431\n",
      "lr of epoch 229 => [0.001]\n",
      "Epoch 230/500 - Train Loss: 0.21081829\n",
      "Epoch 230/500 - Test Loss: 0.22462318\n",
      "lr of epoch 230 => [0.001]\n",
      "Epoch 232/500 - Train Loss: 0.21068526\n",
      "Epoch 232/500 - Test Loss: 0.22514701\n",
      "lr of epoch 232 => [0.001]\n",
      "Epoch 233/500 - Train Loss: 0.21098271\n",
      "Epoch 233/500 - Test Loss: 0.22466752\n",
      "lr of epoch 233 => [0.001]\n",
      "Epoch 234/500 - Train Loss: 0.21045012\n",
      "Epoch 234/500 - Test Loss: 0.22402304\n",
      "lr of epoch 234 => [0.001]\n",
      "Epoch 235/500 - Train Loss: 0.21023026\n",
      "Epoch 235/500 - Test Loss: 0.22710675\n",
      "lr of epoch 235 => [0.001]\n",
      "Epoch 236/500 - Train Loss: 0.21136322\n",
      "Epoch 236/500 - Test Loss: 0.22426184\n",
      "lr of epoch 236 => [0.001]\n",
      "Epoch 237/500 - Train Loss: 0.21023056\n",
      "Epoch 237/500 - Test Loss: 0.22418443\n",
      "lr of epoch 237 => [0.001]\n",
      "Epoch 238/500 - Train Loss: 0.21002796\n",
      "Epoch 238/500 - Test Loss: 0.22421109\n",
      "lr of epoch 238 => [0.001]\n",
      "Epoch 239/500 - Train Loss: 0.21036353\n",
      "Epoch 239/500 - Test Loss: 0.22452425\n",
      "lr of epoch 239 => [0.001]\n",
      "Epoch 240/500 - Train Loss: 0.21020597\n",
      "Epoch 240/500 - Test Loss: 0.22409839\n",
      "lr of epoch 240 => [0.001]\n",
      "Epoch 241/500 - Train Loss: 0.20986134\n",
      "Epoch 241/500 - Test Loss: 0.22445877\n",
      "lr of epoch 241 => [0.001]\n",
      "Epoch 242/500 - Train Loss: 0.20961083\n",
      "Epoch 242/500 - Test Loss: 0.22409428\n",
      "lr of epoch 242 => [0.001]\n",
      "Epoch 243/500 - Train Loss: 0.20979528\n",
      "Epoch 243/500 - Test Loss: 0.22432518\n",
      "lr of epoch 243 => [0.001]\n",
      "Epoch 244/500 - Train Loss: 0.20984115\n",
      "Epoch 244/500 - Test Loss: 0.22442281\n",
      "lr of epoch 244 => [0.001]\n",
      "Epoch 245/500 - Train Loss: 0.20953921\n",
      "Epoch 245/500 - Test Loss: 0.22417268\n",
      "lr of epoch 245 => [0.001]\n",
      "Epoch 246/500 - Train Loss: 0.21002923\n",
      "Epoch 246/500 - Test Loss: 0.22496756\n",
      "lr of epoch 246 => [0.001]\n",
      "Epoch 247/500 - Train Loss: 0.20990538\n",
      "Epoch 247/500 - Test Loss: 0.22436996\n",
      "lr of epoch 247 => [0.001]\n",
      "Epoch 248/500 - Train Loss: 0.20987451\n",
      "Epoch 248/500 - Test Loss: 0.22447036\n",
      "lr of epoch 248 => [0.001]\n",
      "Epoch 249/500 - Train Loss: 0.20958493\n",
      "Epoch 249/500 - Test Loss: 0.22441569\n",
      "lr of epoch 249 => [0.001]\n",
      "Epoch 250/500 - Train Loss: 0.20939644\n",
      "Epoch 250/500 - Test Loss: 0.22417700\n",
      "lr of epoch 250 => [0.001]\n",
      "Epoch 251/500 - Train Loss: 0.20973665\n",
      "Epoch 251/500 - Test Loss: 0.22409186\n",
      "lr of epoch 251 => [0.001]\n",
      "Epoch 252/500 - Train Loss: 0.20959081\n",
      "Epoch 252/500 - Test Loss: 0.22410831\n",
      "lr of epoch 252 => [0.001]\n",
      "Epoch 253/500 - Train Loss: 0.20934139\n",
      "Epoch 253/500 - Test Loss: 0.22496613\n",
      "lr of epoch 253 => [0.001]\n",
      "Epoch 254/500 - Train Loss: 0.20954131\n",
      "Epoch 254/500 - Test Loss: 0.22404414\n",
      "lr of epoch 254 => [0.001]\n",
      "Epoch 255/500 - Train Loss: 0.20928112\n",
      "Epoch 255/500 - Test Loss: 0.22441729\n",
      "lr of epoch 255 => [0.001]\n",
      "Epoch 256/500 - Train Loss: 0.20976341\n",
      "Epoch 256/500 - Test Loss: 0.22410745\n",
      "lr of epoch 256 => [0.001]\n",
      "Epoch 257/500 - Train Loss: 0.20933612\n",
      "Epoch 257/500 - Test Loss: 0.22354144\n",
      "lr of epoch 257 => [0.001]\n",
      "Epoch 258/500 - Train Loss: 0.20917170\n",
      "Epoch 258/500 - Test Loss: 0.22383668\n",
      "lr of epoch 258 => [0.001]\n",
      "Epoch 259/500 - Train Loss: 0.20911562\n",
      "Epoch 259/500 - Test Loss: 0.22453554\n",
      "lr of epoch 259 => [0.001]\n",
      "Epoch 260/500 - Train Loss: 0.20937028\n",
      "Epoch 260/500 - Test Loss: 0.22393052\n",
      "lr of epoch 260 => [0.001]\n",
      "Epoch 261/500 - Train Loss: 0.20897861\n",
      "Epoch 261/500 - Test Loss: 0.22380667\n",
      "lr of epoch 261 => [0.001]\n",
      "Epoch 263/500 - Train Loss: 0.20889120\n",
      "Epoch 263/500 - Test Loss: 0.22420748\n",
      "lr of epoch 263 => [0.001]\n",
      "Epoch 264/500 - Train Loss: 0.20873983\n",
      "Epoch 264/500 - Test Loss: 0.22363509\n",
      "lr of epoch 264 => [0.001]\n",
      "Epoch 265/500 - Train Loss: 0.20857393\n",
      "Epoch 265/500 - Test Loss: 0.22333922\n",
      "lr of epoch 265 => [0.001]\n",
      "Epoch 266/500 - Train Loss: 0.20854844\n",
      "Epoch 266/500 - Test Loss: 0.22363145\n",
      "lr of epoch 266 => [0.001]\n",
      "Epoch 267/500 - Train Loss: 0.20842908\n",
      "Epoch 267/500 - Test Loss: 0.22350461\n",
      "lr of epoch 267 => [0.001]\n",
      "Epoch 268/500 - Train Loss: 0.20977719\n",
      "Epoch 268/500 - Test Loss: 0.22346927\n",
      "lr of epoch 268 => [0.001]\n",
      "Epoch 270/500 - Train Loss: 0.20847593\n",
      "Epoch 270/500 - Test Loss: 0.22382365\n",
      "lr of epoch 270 => [0.001]\n",
      "Epoch 271/500 - Train Loss: 0.20880509\n",
      "Epoch 271/500 - Test Loss: 0.22417241\n",
      "lr of epoch 271 => [0.001]\n",
      "Epoch 272/500 - Train Loss: 0.20810793\n",
      "Epoch 272/500 - Test Loss: 0.22285627\n",
      "lr of epoch 272 => [0.001]\n",
      "Epoch 273/500 - Train Loss: 0.20796679\n",
      "Epoch 273/500 - Test Loss: 0.22310046\n",
      "lr of epoch 273 => [0.001]\n",
      "Epoch 274/500 - Train Loss: 0.20832877\n",
      "Epoch 274/500 - Test Loss: 0.22341360\n",
      "lr of epoch 274 => [0.001]\n",
      "Epoch 275/500 - Train Loss: 0.20834235\n",
      "Epoch 275/500 - Test Loss: 0.22403488\n",
      "lr of epoch 275 => [0.001]\n",
      "Epoch 276/500 - Train Loss: 0.20888560\n",
      "Epoch 276/500 - Test Loss: 0.22365613\n",
      "lr of epoch 276 => [0.001]\n",
      "Epoch 277/500 - Train Loss: 0.20840630\n",
      "Epoch 277/500 - Test Loss: 0.22336973\n",
      "lr of epoch 277 => [0.001]\n",
      "Epoch 278/500 - Train Loss: 0.20804231\n",
      "Epoch 278/500 - Test Loss: 0.22313493\n",
      "lr of epoch 278 => [0.001]\n",
      "Epoch 279/500 - Train Loss: 0.20772122\n",
      "Epoch 279/500 - Test Loss: 0.22303792\n",
      "lr of epoch 279 => [0.001]\n",
      "Epoch 280/500 - Train Loss: 0.20779291\n",
      "Epoch 280/500 - Test Loss: 0.22291524\n",
      "lr of epoch 280 => [0.001]\n",
      "Epoch 281/500 - Train Loss: 0.20772191\n",
      "Epoch 281/500 - Test Loss: 0.22345904\n",
      "lr of epoch 281 => [0.001]\n",
      "Epoch 282/500 - Train Loss: 0.20785249\n",
      "Epoch 282/500 - Test Loss: 0.22350669\n",
      "lr of epoch 282 => [0.001]\n",
      "Epoch 283/500 - Train Loss: 0.20841721\n",
      "Epoch 283/500 - Test Loss: 0.22329392\n",
      "lr of epoch 283 => [0.001]\n",
      "Epoch 284/500 - Train Loss: 0.20788301\n",
      "Epoch 284/500 - Test Loss: 0.22318075\n",
      "lr of epoch 284 => [0.001]\n",
      "Epoch 285/500 - Train Loss: 0.20889293\n",
      "Epoch 285/500 - Test Loss: 0.22515280\n",
      "lr of epoch 285 => [0.001]\n",
      "Epoch 286/500 - Train Loss: 0.20793874\n",
      "Epoch 286/500 - Test Loss: 0.22311794\n",
      "lr of epoch 286 => [0.001]\n",
      "Epoch 287/500 - Train Loss: 0.20858763\n",
      "Epoch 287/500 - Test Loss: 0.22380553\n",
      "lr of epoch 287 => [0.001]\n",
      "Epoch 288/500 - Train Loss: 0.20776005\n",
      "Epoch 288/500 - Test Loss: 0.22285640\n",
      "lr of epoch 288 => [0.001]\n",
      "Epoch 289/500 - Train Loss: 0.20765933\n",
      "Epoch 289/500 - Test Loss: 0.22295202\n",
      "lr of epoch 289 => [0.001]\n",
      "Epoch 290/500 - Train Loss: 0.20696403\n",
      "Epoch 290/500 - Test Loss: 0.22236742\n",
      "lr of epoch 290 => [0.001]\n",
      "Epoch 291/500 - Train Loss: 0.20695171\n",
      "Epoch 291/500 - Test Loss: 0.22263176\n",
      "lr of epoch 291 => [0.001]\n",
      "Epoch 292/500 - Train Loss: 0.20757865\n",
      "Epoch 292/500 - Test Loss: 0.22435186\n",
      "lr of epoch 292 => [0.001]\n",
      "Epoch 293/500 - Train Loss: 0.20757308\n",
      "Epoch 293/500 - Test Loss: 0.22292664\n",
      "lr of epoch 293 => [0.001]\n",
      "Epoch 294/500 - Train Loss: 0.20737827\n",
      "Epoch 294/500 - Test Loss: 0.22307449\n",
      "lr of epoch 294 => [0.001]\n",
      "Epoch 295/500 - Train Loss: 0.20695496\n",
      "Epoch 295/500 - Test Loss: 0.22279147\n",
      "lr of epoch 295 => [0.001]\n",
      "Epoch 296/500 - Train Loss: 0.20729572\n",
      "Epoch 296/500 - Test Loss: 0.22260072\n",
      "lr of epoch 296 => [0.001]\n",
      "Epoch 297/500 - Train Loss: 0.20728805\n",
      "Epoch 297/500 - Test Loss: 0.22256262\n",
      "lr of epoch 297 => [0.001]\n",
      "Epoch 298/500 - Train Loss: 0.20721826\n",
      "Epoch 298/500 - Test Loss: 0.22306832\n",
      "lr of epoch 298 => [0.001]\n",
      "Epoch 299/500 - Train Loss: 0.20755922\n",
      "Epoch 299/500 - Test Loss: 0.22359581\n",
      "lr of epoch 299 => [0.001]\n",
      "Epoch 300/500 - Train Loss: 0.20748141\n",
      "Epoch 300/500 - Test Loss: 0.22286013\n",
      "lr of epoch 300 => [0.001]\n",
      "Epoch 301/500 - Train Loss: 0.20703000\n",
      "Epoch 301/500 - Test Loss: 0.22306014\n",
      "lr of epoch 301 => [0.001]\n",
      "Epoch 302/500 - Train Loss: 0.20708679\n",
      "Epoch 302/500 - Test Loss: 0.22328628\n",
      "lr of epoch 302 => [0.001]\n",
      "Epoch 303/500 - Train Loss: 0.20693555\n",
      "Epoch 303/500 - Test Loss: 0.22257601\n",
      "lr of epoch 303 => [0.001]\n",
      "Epoch 304/500 - Train Loss: 0.20688198\n",
      "Epoch 304/500 - Test Loss: 0.22248735\n",
      "lr of epoch 304 => [0.001]\n",
      "Epoch 305/500 - Train Loss: 0.20762319\n",
      "Epoch 305/500 - Test Loss: 0.22337505\n",
      "lr of epoch 305 => [0.001]\n",
      "Epoch 306/500 - Train Loss: 0.20705783\n",
      "Epoch 306/500 - Test Loss: 0.22250863\n",
      "lr of epoch 306 => [0.001]\n",
      "Epoch 308/500 - Train Loss: 0.20722411\n",
      "Epoch 308/500 - Test Loss: 0.22322707\n",
      "lr of epoch 308 => [0.001]\n",
      "Epoch 309/500 - Train Loss: 0.20665643\n",
      "Epoch 309/500 - Test Loss: 0.22230788\n",
      "lr of epoch 309 => [0.001]\n",
      "Epoch 310/500 - Train Loss: 0.20639778\n",
      "Epoch 310/500 - Test Loss: 0.22331441\n",
      "lr of epoch 310 => [0.001]\n",
      "Epoch 311/500 - Train Loss: 0.20668609\n",
      "Epoch 311/500 - Test Loss: 0.22270910\n",
      "lr of epoch 311 => [0.001]\n",
      "Epoch 312/500 - Train Loss: 0.20669191\n",
      "Epoch 312/500 - Test Loss: 0.22251189\n",
      "lr of epoch 312 => [0.001]\n",
      "Epoch 313/500 - Train Loss: 0.20667854\n",
      "Epoch 313/500 - Test Loss: 0.22244662\n",
      "lr of epoch 313 => [0.001]\n",
      "Epoch 314/500 - Train Loss: 0.20732783\n",
      "Epoch 314/500 - Test Loss: 0.22343715\n",
      "lr of epoch 314 => [0.001]\n",
      "Epoch 315/500 - Train Loss: 0.20656902\n",
      "Epoch 315/500 - Test Loss: 0.22212315\n",
      "lr of epoch 315 => [0.001]\n",
      "Epoch 317/500 - Train Loss: 0.20644016\n",
      "Epoch 317/500 - Test Loss: 0.22238982\n",
      "lr of epoch 317 => [0.001]\n",
      "Epoch 318/500 - Train Loss: 0.20623167\n",
      "Epoch 318/500 - Test Loss: 0.22225955\n",
      "lr of epoch 318 => [0.001]\n",
      "Epoch 319/500 - Train Loss: 0.20597022\n",
      "Epoch 319/500 - Test Loss: 0.22174886\n",
      "lr of epoch 319 => [0.001]\n",
      "Epoch 320/500 - Train Loss: 0.20634420\n",
      "Epoch 320/500 - Test Loss: 0.22248758\n",
      "lr of epoch 320 => [0.001]\n",
      "Epoch 321/500 - Train Loss: 0.20619968\n",
      "Epoch 321/500 - Test Loss: 0.22297833\n",
      "lr of epoch 321 => [0.001]\n",
      "Epoch 322/500 - Train Loss: 0.20616817\n",
      "Epoch 322/500 - Test Loss: 0.22198349\n",
      "lr of epoch 322 => [0.001]\n",
      "Epoch 323/500 - Train Loss: 0.20567697\n",
      "Epoch 323/500 - Test Loss: 0.22195761\n",
      "lr of epoch 323 => [0.001]\n",
      "Epoch 324/500 - Train Loss: 0.20581861\n",
      "Epoch 324/500 - Test Loss: 0.22189944\n",
      "lr of epoch 324 => [0.001]\n",
      "Epoch 325/500 - Train Loss: 0.20622903\n",
      "Epoch 325/500 - Test Loss: 0.22172642\n",
      "lr of epoch 325 => [0.001]\n",
      "Epoch 326/500 - Train Loss: 0.20579994\n",
      "Epoch 326/500 - Test Loss: 0.22243047\n",
      "lr of epoch 326 => [0.001]\n",
      "Epoch 327/500 - Train Loss: 0.20556903\n",
      "Epoch 327/500 - Test Loss: 0.22204731\n",
      "lr of epoch 327 => [0.001]\n",
      "Epoch 328/500 - Train Loss: 0.20563218\n",
      "Epoch 328/500 - Test Loss: 0.22198412\n",
      "lr of epoch 328 => [0.001]\n",
      "Epoch 329/500 - Train Loss: 0.20553650\n",
      "Epoch 329/500 - Test Loss: 0.22237258\n",
      "lr of epoch 329 => [0.001]\n",
      "Epoch 330/500 - Train Loss: 0.20621242\n",
      "Epoch 330/500 - Test Loss: 0.22243768\n",
      "lr of epoch 330 => [0.001]\n",
      "Epoch 331/500 - Train Loss: 0.20640353\n",
      "Epoch 331/500 - Test Loss: 0.22319587\n",
      "lr of epoch 331 => [0.001]\n",
      "Epoch 332/500 - Train Loss: 0.20802349\n",
      "Epoch 332/500 - Test Loss: 0.22249898\n",
      "lr of epoch 332 => [0.001]\n",
      "Epoch 333/500 - Train Loss: 0.20614745\n",
      "Epoch 333/500 - Test Loss: 0.22167686\n",
      "lr of epoch 333 => [0.001]\n",
      "Epoch 334/500 - Train Loss: 0.20552143\n",
      "Epoch 334/500 - Test Loss: 0.22252422\n",
      "lr of epoch 334 => [0.001]\n",
      "Epoch 335/500 - Train Loss: 0.20572956\n",
      "Epoch 335/500 - Test Loss: 0.22180959\n",
      "lr of epoch 335 => [0.001]\n",
      "Epoch 336/500 - Train Loss: 0.20524480\n",
      "Epoch 336/500 - Test Loss: 0.22192297\n",
      "lr of epoch 336 => [0.001]\n",
      "Epoch 337/500 - Train Loss: 0.20554653\n",
      "Epoch 337/500 - Test Loss: 0.22222265\n",
      "lr of epoch 337 => [0.001]\n",
      "Epoch 338/500 - Train Loss: 0.20540256\n",
      "Epoch 338/500 - Test Loss: 0.22250272\n",
      "lr of epoch 338 => [0.001]\n",
      "Epoch 339/500 - Train Loss: 0.20548111\n",
      "Epoch 339/500 - Test Loss: 0.22145514\n",
      "lr of epoch 339 => [0.001]\n",
      "Epoch 340/500 - Train Loss: 0.20526815\n",
      "Epoch 340/500 - Test Loss: 0.22158143\n",
      "lr of epoch 340 => [0.001]\n",
      "Epoch 341/500 - Train Loss: 0.20529820\n",
      "Epoch 341/500 - Test Loss: 0.22189447\n",
      "lr of epoch 341 => [0.001]\n",
      "Epoch 342/500 - Train Loss: 0.20532569\n",
      "Epoch 342/500 - Test Loss: 0.22205500\n",
      "lr of epoch 342 => [0.001]\n",
      "Epoch 343/500 - Train Loss: 0.20625175\n",
      "Epoch 343/500 - Test Loss: 0.22251268\n",
      "lr of epoch 343 => [0.001]\n",
      "Epoch 344/500 - Train Loss: 0.20591000\n",
      "Epoch 344/500 - Test Loss: 0.22223595\n",
      "lr of epoch 344 => [0.001]\n",
      "Epoch 345/500 - Train Loss: 0.20568720\n",
      "Epoch 345/500 - Test Loss: 0.22197917\n",
      "lr of epoch 345 => [0.001]\n",
      "Epoch 346/500 - Train Loss: 0.20535818\n",
      "Epoch 346/500 - Test Loss: 0.22185998\n",
      "lr of epoch 346 => [0.001]\n",
      "Epoch 347/500 - Train Loss: 0.20519302\n",
      "Epoch 347/500 - Test Loss: 0.22156746\n",
      "lr of epoch 347 => [0.001]\n",
      "Epoch 348/500 - Train Loss: 0.20508539\n",
      "Epoch 348/500 - Test Loss: 0.22154417\n",
      "lr of epoch 348 => [0.001]\n",
      "Epoch 349/500 - Train Loss: 0.20498401\n",
      "Epoch 349/500 - Test Loss: 0.22184581\n",
      "lr of epoch 349 => [0.001]\n",
      "Epoch 350/500 - Train Loss: 0.20583595\n",
      "Epoch 350/500 - Test Loss: 0.22202274\n",
      "lr of epoch 350 => [0.001]\n",
      "Epoch 351/500 - Train Loss: 0.20614126\n",
      "Epoch 351/500 - Test Loss: 0.22145907\n",
      "lr of epoch 351 => [0.001]\n",
      "Epoch 352/500 - Train Loss: 0.20523287\n",
      "Epoch 352/500 - Test Loss: 0.22198136\n",
      "lr of epoch 352 => [0.001]\n",
      "Epoch 353/500 - Train Loss: 0.20509560\n",
      "Epoch 353/500 - Test Loss: 0.22155091\n",
      "lr of epoch 353 => [0.001]\n",
      "Epoch 354/500 - Train Loss: 0.20504832\n",
      "Epoch 354/500 - Test Loss: 0.22157345\n",
      "lr of epoch 354 => [0.001]\n",
      "Epoch 355/500 - Train Loss: 0.20560577\n",
      "Epoch 355/500 - Test Loss: 0.22312261\n",
      "lr of epoch 355 => [0.001]\n",
      "Epoch 356/500 - Train Loss: 0.20558994\n",
      "Epoch 356/500 - Test Loss: 0.22144116\n",
      "lr of epoch 356 => [0.001]\n",
      "Epoch 357/500 - Train Loss: 0.20534102\n",
      "Epoch 357/500 - Test Loss: 0.22142484\n",
      "lr of epoch 357 => [0.001]\n",
      "Epoch 358/500 - Train Loss: 0.20534253\n",
      "Epoch 358/500 - Test Loss: 0.22120280\n",
      "lr of epoch 358 => [0.001]\n",
      "Epoch 359/500 - Train Loss: 0.20498100\n",
      "Epoch 359/500 - Test Loss: 0.22222012\n",
      "lr of epoch 359 => [0.001]\n",
      "Epoch 360/500 - Train Loss: 0.20507916\n",
      "Epoch 360/500 - Test Loss: 0.22142415\n",
      "lr of epoch 360 => [0.001]\n",
      "Epoch 361/500 - Train Loss: 0.20484164\n",
      "Epoch 361/500 - Test Loss: 0.22157856\n",
      "lr of epoch 361 => [0.001]\n",
      "Epoch 362/500 - Train Loss: 0.20435208\n",
      "Epoch 362/500 - Test Loss: 0.22161385\n",
      "lr of epoch 362 => [0.001]\n",
      "Epoch 363/500 - Train Loss: 0.20451096\n",
      "Epoch 363/500 - Test Loss: 0.22097980\n",
      "lr of epoch 363 => [0.001]\n",
      "Epoch 364/500 - Train Loss: 0.20582786\n",
      "Epoch 364/500 - Test Loss: 0.22811501\n",
      "lr of epoch 364 => [0.001]\n",
      "Epoch 365/500 - Train Loss: 0.20662326\n",
      "Epoch 365/500 - Test Loss: 0.22131476\n",
      "lr of epoch 365 => [0.001]\n",
      "Epoch 366/500 - Train Loss: 0.20440900\n",
      "Epoch 366/500 - Test Loss: 0.22140456\n",
      "lr of epoch 366 => [0.001]\n",
      "Epoch 367/500 - Train Loss: 0.20427056\n",
      "Epoch 367/500 - Test Loss: 0.22106428\n",
      "lr of epoch 367 => [0.001]\n",
      "Epoch 368/500 - Train Loss: 0.20427703\n",
      "Epoch 368/500 - Test Loss: 0.22153081\n",
      "lr of epoch 368 => [0.001]\n",
      "Epoch 369/500 - Train Loss: 0.20429377\n",
      "Epoch 369/500 - Test Loss: 0.22158549\n",
      "lr of epoch 369 => [0.001]\n",
      "Epoch 370/500 - Train Loss: 0.20424352\n",
      "Epoch 370/500 - Test Loss: 0.22162485\n",
      "lr of epoch 370 => [0.001]\n",
      "Epoch 371/500 - Train Loss: 0.20427417\n",
      "Epoch 371/500 - Test Loss: 0.22125701\n",
      "lr of epoch 371 => [0.001]\n",
      "Epoch 372/500 - Train Loss: 0.20437448\n",
      "Epoch 372/500 - Test Loss: 0.22152895\n",
      "lr of epoch 372 => [0.001]\n",
      "Epoch 373/500 - Train Loss: 0.20402130\n",
      "Epoch 373/500 - Test Loss: 0.22086393\n",
      "lr of epoch 373 => [0.001]\n",
      "Epoch 374/500 - Train Loss: 0.20428145\n",
      "Epoch 374/500 - Test Loss: 0.22163570\n",
      "lr of epoch 374 => [0.001]\n",
      "Epoch 375/500 - Train Loss: 0.20433533\n",
      "Epoch 375/500 - Test Loss: 0.22099099\n",
      "lr of epoch 375 => [0.001]\n",
      "Epoch 376/500 - Train Loss: 0.20417363\n",
      "Epoch 376/500 - Test Loss: 0.22166603\n",
      "lr of epoch 376 => [0.001]\n",
      "Epoch 377/500 - Train Loss: 0.20494463\n",
      "Epoch 377/500 - Test Loss: 0.22146680\n",
      "lr of epoch 377 => [0.001]\n",
      "Epoch 378/500 - Train Loss: 0.20475960\n",
      "Epoch 378/500 - Test Loss: 0.22168264\n",
      "lr of epoch 378 => [0.001]\n",
      "Epoch 379/500 - Train Loss: 0.20434265\n",
      "Epoch 379/500 - Test Loss: 0.22103862\n",
      "lr of epoch 379 => [0.001]\n",
      "Epoch 380/500 - Train Loss: 0.20423034\n",
      "Epoch 380/500 - Test Loss: 0.22115249\n",
      "lr of epoch 380 => [0.001]\n",
      "Epoch 381/500 - Train Loss: 0.20384749\n",
      "Epoch 381/500 - Test Loss: 0.22094253\n",
      "lr of epoch 381 => [0.001]\n",
      "Epoch 382/500 - Train Loss: 0.20414807\n",
      "Epoch 382/500 - Test Loss: 0.22109491\n",
      "lr of epoch 382 => [0.001]\n",
      "Epoch 383/500 - Train Loss: 0.20393243\n",
      "Epoch 383/500 - Test Loss: 0.22120208\n",
      "lr of epoch 383 => [0.001]\n",
      "Epoch 384/500 - Train Loss: 0.20387554\n",
      "Epoch 384/500 - Test Loss: 0.22089660\n",
      "lr of epoch 384 => [0.001]\n",
      "Epoch 385/500 - Train Loss: 0.20381048\n",
      "Epoch 385/500 - Test Loss: 0.22094546\n",
      "lr of epoch 385 => [0.001]\n",
      "Epoch 386/500 - Train Loss: 0.20439266\n",
      "Epoch 386/500 - Test Loss: 0.22200228\n",
      "lr of epoch 386 => [0.001]\n",
      "Epoch 387/500 - Train Loss: 0.20459794\n",
      "Epoch 387/500 - Test Loss: 0.22092237\n",
      "lr of epoch 387 => [0.001]\n",
      "Epoch 388/500 - Train Loss: 0.20361011\n",
      "Epoch 388/500 - Test Loss: 0.22082108\n",
      "lr of epoch 388 => [0.001]\n",
      "Epoch 389/500 - Train Loss: 0.20362866\n",
      "Epoch 389/500 - Test Loss: 0.22065143\n",
      "lr of epoch 389 => [0.001]\n",
      "Epoch 390/500 - Train Loss: 0.20386747\n",
      "Epoch 390/500 - Test Loss: 0.22157537\n",
      "lr of epoch 390 => [0.001]\n",
      "Epoch 391/500 - Train Loss: 0.20400272\n",
      "Epoch 391/500 - Test Loss: 0.22095305\n",
      "lr of epoch 391 => [0.001]\n",
      "Epoch 392/500 - Train Loss: 0.20367806\n",
      "Epoch 392/500 - Test Loss: 0.22089340\n",
      "lr of epoch 392 => [0.001]\n",
      "Epoch 393/500 - Train Loss: 0.20371989\n",
      "Epoch 393/500 - Test Loss: 0.22120837\n",
      "lr of epoch 393 => [0.001]\n",
      "Epoch 394/500 - Train Loss: 0.20392652\n",
      "Epoch 394/500 - Test Loss: 0.22071007\n",
      "lr of epoch 394 => [0.001]\n",
      "Epoch 395/500 - Train Loss: 0.20375176\n",
      "Epoch 395/500 - Test Loss: 0.22096362\n",
      "lr of epoch 395 => [0.001]\n",
      "Epoch 396/500 - Train Loss: 0.20359170\n",
      "Epoch 396/500 - Test Loss: 0.22060489\n",
      "lr of epoch 396 => [0.001]\n",
      "Epoch 397/500 - Train Loss: 0.20364882\n",
      "Epoch 397/500 - Test Loss: 0.22151181\n",
      "lr of epoch 397 => [0.001]\n",
      "Epoch 398/500 - Train Loss: 0.20423607\n",
      "Epoch 398/500 - Test Loss: 0.22106954\n",
      "lr of epoch 398 => [0.001]\n",
      "Epoch 399/500 - Train Loss: 0.20338747\n",
      "Epoch 399/500 - Test Loss: 0.22064450\n",
      "lr of epoch 399 => [0.001]\n",
      "Epoch 400/500 - Train Loss: 0.20355178\n",
      "Epoch 400/500 - Test Loss: 0.22113370\n",
      "lr of epoch 400 => [0.001]\n",
      "Epoch 401/500 - Train Loss: 0.20353039\n",
      "Epoch 401/500 - Test Loss: 0.22119068\n",
      "lr of epoch 401 => [0.001]\n",
      "Epoch 402/500 - Train Loss: 0.20338931\n",
      "Epoch 402/500 - Test Loss: 0.22144910\n",
      "lr of epoch 402 => [0.001]\n",
      "Epoch 403/500 - Train Loss: 0.20390241\n",
      "Epoch 403/500 - Test Loss: 0.22139110\n",
      "lr of epoch 403 => [0.001]\n",
      "Epoch 404/500 - Train Loss: 0.20357298\n",
      "Epoch 404/500 - Test Loss: 0.22069973\n",
      "lr of epoch 404 => [0.001]\n",
      "Epoch 405/500 - Train Loss: 0.20336267\n",
      "Epoch 405/500 - Test Loss: 0.22048488\n",
      "lr of epoch 405 => [0.001]\n",
      "Epoch 406/500 - Train Loss: 0.20346219\n",
      "Epoch 406/500 - Test Loss: 0.22115755\n",
      "lr of epoch 406 => [0.001]\n",
      "Epoch 407/500 - Train Loss: 0.20333651\n",
      "Epoch 407/500 - Test Loss: 0.22174486\n",
      "lr of epoch 407 => [0.001]\n",
      "Epoch 408/500 - Train Loss: 0.20363933\n",
      "Epoch 408/500 - Test Loss: 0.22084896\n",
      "lr of epoch 408 => [0.001]\n",
      "Epoch 409/500 - Train Loss: 0.20326230\n",
      "Epoch 409/500 - Test Loss: 0.22092829\n",
      "lr of epoch 409 => [0.001]\n",
      "Epoch 410/500 - Train Loss: 0.20331240\n",
      "Epoch 410/500 - Test Loss: 0.22079439\n",
      "lr of epoch 410 => [0.001]\n",
      "Epoch 411/500 - Train Loss: 0.20158605\n",
      "Epoch 411/500 - Test Loss: 0.21918923\n",
      "lr of epoch 411 => [0.0007]\n",
      "Epoch 412/500 - Train Loss: 0.20037647\n",
      "Epoch 412/500 - Test Loss: 0.21881107\n",
      "lr of epoch 412 => [0.00049]\n",
      "Epoch 413/500 - Train Loss: 0.19975278\n",
      "Epoch 413/500 - Test Loss: 0.21851610\n",
      "lr of epoch 413 => [0.00034299999999999993]\n",
      "Epoch 414/500 - Train Loss: 0.19929529\n",
      "Epoch 414/500 - Test Loss: 0.21836112\n",
      "lr of epoch 414 => [0.00024009999999999995]\n",
      "Epoch 415/500 - Train Loss: 0.19898119\n",
      "Epoch 415/500 - Test Loss: 0.21830462\n",
      "lr of epoch 415 => [0.00016806999999999995]\n",
      "Epoch 416/500 - Train Loss: 0.19889900\n",
      "Epoch 416/500 - Test Loss: 0.21823165\n",
      "lr of epoch 416 => [0.00011764899999999997]\n",
      "Epoch 417/500 - Train Loss: 0.19883196\n",
      "Epoch 417/500 - Test Loss: 0.21822338\n",
      "lr of epoch 417 => [8.235429999999997e-05]\n",
      "Epoch 418/500 - Train Loss: 0.19872417\n",
      "Epoch 418/500 - Test Loss: 0.21819594\n",
      "lr of epoch 418 => [5.7648009999999975e-05]\n",
      "Epoch 419/500 - Train Loss: 0.19855585\n",
      "Epoch 419/500 - Test Loss: 0.21818354\n",
      "lr of epoch 419 => [4.035360699999998e-05]\n",
      "Epoch 420/500 - Train Loss: 0.19847753\n",
      "Epoch 420/500 - Test Loss: 0.21818725\n",
      "lr of epoch 420 => [2.8247524899999983e-05]\n",
      "Epoch 421/500 - Train Loss: 0.19849172\n",
      "Epoch 421/500 - Test Loss: 0.21816931\n",
      "lr of epoch 421 => [1.9773267429999988e-05]\n",
      "Epoch 422/500 - Train Loss: 0.19848514\n",
      "Epoch 422/500 - Test Loss: 0.21817564\n",
      "lr of epoch 422 => [1.384128720099999e-05]\n",
      "Epoch 423/500 - Train Loss: 0.19849761\n",
      "Epoch 423/500 - Test Loss: 0.21817282\n",
      "lr of epoch 423 => [9.688901040699991e-06]\n",
      "Epoch 424/500 - Train Loss: 0.19851366\n",
      "Epoch 424/500 - Test Loss: 0.21817981\n",
      "lr of epoch 424 => [6.782230728489995e-06]\n",
      "Epoch 425/500 - Train Loss: 0.19842284\n",
      "Epoch 425/500 - Test Loss: 0.21817920\n",
      "lr of epoch 425 => [4.747561509942996e-06]\n",
      "Epoch 426/500 - Train Loss: 0.19841949\n",
      "Epoch 426/500 - Test Loss: 0.21817641\n",
      "lr of epoch 426 => [3.3232930569600964e-06]\n",
      "Epoch 427/500 - Train Loss: 0.19847289\n",
      "Epoch 427/500 - Test Loss: 0.21817700\n",
      "lr of epoch 427 => [2.326305139872067e-06]\n",
      "Epoch 428/500 - Train Loss: 0.19835161\n",
      "Epoch 428/500 - Test Loss: 0.21817557\n",
      "lr of epoch 428 => [1.6284135979104472e-06]\n",
      "Epoch 429/500 - Train Loss: 0.19841269\n",
      "Epoch 429/500 - Test Loss: 0.21817572\n",
      "lr of epoch 429 => [1.139889518537313e-06]\n",
      "Epoch 430/500 - Train Loss: 0.19847573\n",
      "Epoch 430/500 - Test Loss: 0.21817570\n",
      "lr of epoch 430 => [7.979226629761189e-07]\n",
      "Epoch 431/500 - Train Loss: 0.19846428\n",
      "Epoch 431/500 - Test Loss: 0.21817595\n",
      "lr of epoch 431 => [5.585458640832833e-07]\n",
      "Epoch 432/500 - Train Loss: 0.19838806\n",
      "Epoch 432/500 - Test Loss: 0.21817618\n",
      "lr of epoch 432 => [3.9098210485829825e-07]\n",
      "Epoch 433/500 - Train Loss: 0.19839449\n",
      "Epoch 433/500 - Test Loss: 0.21817611\n",
      "lr of epoch 433 => [2.736874734008088e-07]\n",
      "Epoch 434/500 - Train Loss: 0.19836566\n",
      "Epoch 434/500 - Test Loss: 0.21817616\n",
      "lr of epoch 434 => [1.9158123138056612e-07]\n",
      "Epoch 435/500 - Train Loss: 0.19844145\n",
      "Epoch 435/500 - Test Loss: 0.21817623\n",
      "lr of epoch 435 => [1.341068619663963e-07]\n",
      "Epoch 436/500 - Train Loss: 0.19846353\n",
      "Epoch 436/500 - Test Loss: 0.21817619\n",
      "lr of epoch 436 => [9.38748033764774e-08]\n",
      "Epoch 437/500 - Train Loss: 0.19849832\n",
      "Epoch 437/500 - Test Loss: 0.21817627\n",
      "lr of epoch 437 => [6.571236236353417e-08]\n",
      "Epoch 438/500 - Train Loss: 0.19842637\n",
      "Epoch 438/500 - Test Loss: 0.21817628\n",
      "lr of epoch 438 => [4.5998653654473916e-08]\n",
      "Epoch 439/500 - Train Loss: 0.19846827\n",
      "Epoch 439/500 - Test Loss: 0.21817628\n",
      "lr of epoch 439 => [3.219905755813174e-08]\n",
      "Epoch 440/500 - Train Loss: 0.19842374\n",
      "Epoch 440/500 - Test Loss: 0.21817631\n",
      "lr of epoch 440 => [2.2539340290692218e-08]\n",
      "Epoch 441/500 - Train Loss: 0.19843527\n",
      "Epoch 441/500 - Test Loss: 0.21817628\n",
      "lr of epoch 441 => [1.577753820348455e-08]\n",
      "Epoch 442/500 - Train Loss: 0.19843249\n",
      "Epoch 442/500 - Test Loss: 0.21817630\n",
      "lr of epoch 442 => [1.1044276742439185e-08]\n",
      "Epoch 443/500 - Train Loss: 0.19848717\n",
      "Epoch 443/500 - Test Loss: 0.21817630\n",
      "lr of epoch 443 => [7.730993719707429e-09]\n",
      "Epoch 444/500 - Train Loss: 0.19839352\n",
      "Epoch 444/500 - Test Loss: 0.21817629\n",
      "lr of epoch 444 => [5.4116956037951994e-09]\n",
      "Epoch 445/500 - Train Loss: 0.19848576\n",
      "Epoch 445/500 - Test Loss: 0.21817629\n",
      "lr of epoch 445 => [3.78818692265664e-09]\n",
      "Epoch 446/500 - Train Loss: 0.19845781\n",
      "Epoch 446/500 - Test Loss: 0.21817629\n",
      "lr of epoch 446 => [2.6517308458596473e-09]\n",
      "Early stopping after 25 epochs of no improvement.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':#被別人呼叫的時候不會執行main以下的程式，只會執行\n",
    "\n",
    "    input_dim = data_mut_tcga.shape[1]# (8238sample[0], 2649gene[1])\n",
    "    mut_encode_dim =[128,32]\n",
    "    batch_size = 64\n",
    "    epoch_size = 500 #100\n",
    "    activation_function = nn.ReLU()\n",
    "    model_save_name = \"tcga_exp_%d_%d\" % (mut_encode_dim[0], mut_encode_dim[1])\n",
    "    learning_rate=0.001\n",
    "    warmup_iters = 410\n",
    "    seed=42\n",
    "    patience = 25\n",
    "    Decrease_percent = 0.7\n",
    "    continuous=True\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    train_size = int(0.8 * len(data_mut_tcga))\n",
    "    test_size = len(data_mut_tcga) - train_size\n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = random_split(data_mut_tcga.values, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders for training and testing\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    t = time.time()\n",
    "    torch.manual_seed(seed)\n",
    "    model = AE_dense_layers(input_dim=input_dim, mut_encode_dim=mut_encode_dim, activation_func=activation_function).to(device=device)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if warmup_iters is not None:\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters,Decrease_percent,continuous)\n",
    "        # scheduler = warmup_lr_scheduler(optimizer, warmup_iters1=5, Decrease_percent1=10, warmup_iters2=290, Decrease_percent2=0.8, continuous=False)\n",
    "\n",
    "    # Training with early stopping (assuming you've defined the EarlyStopping logic)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_weight=None\n",
    "    counter = 0\n",
    "    train_epoch_loss_list = []#  for train every epoch loss plot\n",
    "    test_epoch_loss_list=[]#  for validation every epoch loss plot\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    for epoch in range(epoch_size):\n",
    "        model.train()\n",
    "        model.requires_grad = True\n",
    "        total_train_loss = 0.0\n",
    "        for batch_idx,inputs in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs =inputs.float().to(device=device) # change torch.Tensor int64 to float32\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() # sum every batch's loss to compute average loss of every batch for one epoch\n",
    "            #print(f'Epoch {epoch + 1}/{epoch_size} - Batch {batch_idx+1}/{len(dataloader_mut_tcga)} - Loss: {loss.item():.4f}')#batch loss()\n",
    "        # Calculate and print the average loss of batch for the epoch\n",
    "        average_loss = total_train_loss / len(train_loader) # 一個 epoch 的 loss 是 batch 的 average loss\n",
    "        train_epoch_loss_list.append(average_loss)# for loss plot'\n",
    "        print(f'Epoch {epoch + 1}/{epoch_size} - Train Loss: {average_loss:.8f}')\n",
    "        \n",
    "        model.eval()\n",
    "        model.requires_grad = False\n",
    "        total_test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx,inputs in enumerate(test_loader):\n",
    "                inputs =inputs.float().to(device=device) # change torch.Tensor int64 to float32\n",
    "                outputs = model(inputs)\n",
    "                test_loss = criterion(outputs, inputs)\n",
    "                total_test_loss += test_loss.item() \n",
    "        test_average_loss = total_test_loss / len(test_loader)\n",
    "        test_epoch_loss_list.append(test_average_loss)# for loss plot'\n",
    "        print(f'Epoch {epoch + 1}/{epoch_size} - Test Loss: {test_average_loss:.8f}')\n",
    "\n",
    "        if warmup_iters:\n",
    "            print(\"lr of epoch\", epoch + 1, \"=>\", lr_scheduler.get_lr()) \n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        if test_average_loss < best_val_loss:\n",
    "            best_val_loss = test_average_loss\n",
    "            best_weight = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch+1\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping after {patience} epochs of no improvement.')\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a2367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64   activation_function: ReLU()   learning_rate: 0.001   warmup_iters: 410\n",
      "best Epoch :  429  , best_val_loss :  0.21822737638027437\n",
      "\n",
      "Autoencoder training completed in 19.1 mins.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('batch_size:',batch_size,\" \",'activation_function:',activation_function,\" \",\n",
    "      'learning_rate:',learning_rate,\" \",'warmup_iters:',warmup_iters)\n",
    "print(\"best Epoch : \",best_epoch,\" ,\" ,\"best_val_loss : \",best_val_loss)\n",
    "print('\\nAutoencoder training completed in %.1f mins.\\n' % ((time.time() - t) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95c56621",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_best_weight = {key: value for key, value in best_weight.items() if key.startswith('encoder')} # only store the encoder part without decoder part\n",
    "torch.save(encoder_best_weight, f'./results/Encoder_{model_save_name}_best_loss_{best_val_loss:.8}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b8d226b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create a plot of the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_epoch_loss_list) + 1), train_epoch_loss_list, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "# Plot validation loss\n",
    "plt.plot(range(1, len(test_epoch_loss_list) + 1), test_epoch_loss_list, marker='o', linestyle='-', color='r', label='Validation Loss')\n",
    "# Mark the best epoch\n",
    "plt.plot(best_epoch, best_val_loss , marker='o', markersize=8, linestyle='', color='c', label=f'Best Epoch:{best_epoch}')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Pretain AE Loss Curve', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(top=0.25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac20b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67883d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
