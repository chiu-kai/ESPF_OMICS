{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ec8bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "            else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f0ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information\n",
    "filename = \"DAPL_PretrainAE_exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f211e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "\n",
    "def create_mlpEncoder(dimList, activation_func):\n",
    "    layers = []\n",
    "    for i in range(len(dimList) - 1):  \n",
    "        layers.append(nn.Linear(dimList[i], dimList[i + 1]))\n",
    "        if i < len(dimList) - 2:  \n",
    "            layers.append(activation_func)\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class AE_dense_layers(nn.Module):\n",
    "    def __init__(self, input_dim, mut_encode_dim, activation_func):\n",
    "        super(AE_dense_layers, self).__init__()##__init__初始化父類別的屬性和方法\n",
    "        print('input_dim = ', input_dim)\n",
    "        print('first_layer_dim = ', mut_encode_dim[0])\n",
    "        print('second_layer_dim = ', mut_encode_dim[1])\n",
    "\n",
    "        self.encoder = create_mlpEncoder([input_dim] + mut_encode_dim, activation_func\n",
    "            )\n",
    "        self._init_weights(self.encoder)\n",
    "\n",
    "        self.decoder = create_mlpEncoder(([input_dim] + mut_encode_dim)[::-1], activation_func\n",
    "            )\n",
    "        self._init_weights(self.decoder)\n",
    "\n",
    "    def _init_weights(self, model):\n",
    "        if isinstance(model, nn.Linear):  # 直接初始化 nn.Linear 層\n",
    "            init.kaiming_uniform_(model.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "            if model.bias is not None:\n",
    "                init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.LayerNorm):\n",
    "            init.ones_(model.weight)\n",
    "            init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.ModuleList) or isinstance(model, nn.Sequential):  # 遍歷子層\n",
    "            for layer in model:\n",
    "                self._init_weights(layer)\n",
    "\n",
    "    def forward(self, x):#加self才能呼叫self裡其他的屬性和method\n",
    "        x_encoded = self.encoder(x)\n",
    "        x_decoded = self.decoder(x_encoded)\n",
    "        return x_decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "950ec938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, LR: 0.0001\n",
      "Epoch 1, LR: 0.0001\n",
      "Epoch 2, LR: 0.0001\n",
      "Epoch 3, LR: 0.0001\n",
      "Epoch 4, LR: 0.001\n",
      "Epoch 5, LR: 0.001\n",
      "Epoch 6, LR: 0.001\n",
      "Epoch 7, LR: 0.001\n",
      "Epoch 8, LR: 0.001\n",
      "Epoch 9, LR: 8e-05\n",
      "Epoch 10, LR: 8e-05\n",
      "Epoch 11, LR: 8e-05\n",
      "Epoch 12, LR: 8e-05\n",
      "Epoch 13, LR: 8e-05\n",
      "Epoch 14, LR: 8e-05\n",
      "Epoch 15, LR: 8e-05\n",
      "Epoch 16, LR: 8e-05\n",
      "Epoch 17, LR: 8e-05\n",
      "Epoch 18, LR: 8e-05\n",
      "Epoch 19, LR: 8e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "def warmup_lr_scheduler(optimizer, warmup_iters1, Decrease_percent1, warmup_iters2, Decrease_percent2, continuous=True):\n",
    "    def f(epoch):  \n",
    "        if epoch < warmup_iters1:\n",
    "            return 1  # No decrease during first warmup phase\n",
    "        elif epoch < warmup_iters2:\n",
    "            if continuous:\n",
    "                return Decrease_percent1 ** (epoch - warmup_iters1 + 1)\n",
    "            else:\n",
    "                return Decrease_percent1\n",
    "        else:\n",
    "            if continuous:\n",
    "                return (Decrease_percent1 ** (warmup_iters2 - warmup_iters1)) * (Decrease_percent2 ** (epoch - warmup_iters2 + 1))\n",
    "            else:\n",
    "                return Decrease_percent2\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = warmup_lr_scheduler(optimizer, warmup_iters1=5, Decrease_percent1=10, warmup_iters2=10, Decrease_percent2=0.8, continuous=False)\n",
    "\n",
    "for epoch in range(20):\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch}, LR: {scheduler.get_last_lr()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c7b35",
   "metadata": {},
   "source": [
    "## def warmup_lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3c57499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_lr_scheduler(optimizer, warmup_iters, Decrease_percent,continuous=True):\n",
    "    def f(epoch):  \n",
    "        if epoch >= warmup_iters:\n",
    "            if continuous is True:\n",
    "                return Decrease_percent ** (epoch-warmup_iters+1)\n",
    "            elif continuous is not True:\n",
    "                return Decrease_percent\n",
    "        return 1\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "\n",
    "# def warmup_lr_scheduler(optimizer, warmup_iters1, Decrease_percent1, warmup_iters2, Decrease_percent2, continuous=True):\n",
    "#     def f(epoch):  \n",
    "#         if epoch < warmup_iters1:\n",
    "#             return 1  # No decrease during first warmup phase\n",
    "#         elif epoch < warmup_iters2:\n",
    "#             if continuous:\n",
    "#                 return Decrease_percent1 ** (epoch - warmup_iters1 + 1)\n",
    "#             else:\n",
    "#                 return Decrease_percent1\n",
    "#         else:\n",
    "#             if continuous:\n",
    "#                 return (Decrease_percent1 ** (warmup_iters2 - warmup_iters1)) * (Decrease_percent2 ** (epoch - warmup_iters2 + 1))\n",
    "#             else:\n",
    "#                 return Decrease_percent2\n",
    "#     return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "\n",
    "import random\n",
    "# Function to set the seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# Set the seed\n",
    "seed = 42\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5f77cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Datasets successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# load TCGA mutation data, substitute here with other genomics\n",
    "data_mut_tcga= pd.read_csv(\"../data/DAPL/share/pretrain_tcga.csv\", sep=',', index_col=0)\n",
    "\n",
    "print(\"\\n\\nDatasets successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70928f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9808, 1426)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mut_tcga.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e0448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcga_EXP : \n",
      "Range: 22.04906963\n",
      "Minimum: -13.71211562\n",
      "Maximum: 8.33695401\n",
      "Mean: 0.00000000\n",
      "Median: 0.01146598\n",
      "Standard Deviation: 1.00000000\n",
      "Skewness: -0.02756315\n",
      "binary data:False\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_data_value_range\n\u001b[1;32m      2\u001b[0m get_data_value_range(data_mut_tcga\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtcga_EXP\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mkdeplot(np\u001b[38;5;241m.\u001b[39mconcatenate(np\u001b[38;5;241m.\u001b[39marray(omics_data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist())), fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "from utils.tools import get_data_value_range\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "get_data_value_range(data_mut_tcga.values.tolist(),\"tcga_EXP\", file=None)\n",
    "sns.kdeplot(np.concatenate(np.array(data_mut_tcga.values.tolist())), fill=True, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba8afd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #exchange the row and column to match the format of doctor chiu's data (row is gene ;column is cancer)\n",
    "# data_mut_tcga=data_mut_tcga.transpose()\n",
    "# data_mut_tcga.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75196629",
   "metadata": {},
   "source": [
    "## 也許應該要分train validation set，這樣才能更學到真正重要的特徵，而不是背答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e452a702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim =  1426\n",
      "first_layer_dim =  128\n",
      "second_layer_dim =  32\n",
      "Epoch 1/500 - Train Loss: 1.27612219\n",
      "Epoch 1/500 - Test Loss: 0.90643192\n",
      "lr of epoch 1 => [0.001]\n",
      "Epoch 2/500 - Train Loss: 0.76061652\n",
      "Epoch 2/500 - Test Loss: 0.64880209\n",
      "lr of epoch 2 => [0.001]\n",
      "Epoch 3/500 - Train Loss: 0.56135899\n",
      "Epoch 3/500 - Test Loss: 0.50677222\n",
      "lr of epoch 3 => [0.001]\n",
      "Epoch 4/500 - Train Loss: 0.46102953\n",
      "Epoch 4/500 - Test Loss: 0.44013021\n",
      "lr of epoch 4 => [0.001]\n",
      "Epoch 5/500 - Train Loss: 0.40969900\n",
      "Epoch 5/500 - Test Loss: 0.40168567\n",
      "lr of epoch 5 => [0.001]\n",
      "Epoch 6/500 - Train Loss: 0.37722669\n",
      "Epoch 6/500 - Test Loss: 0.37320868\n",
      "lr of epoch 6 => [0.001]\n",
      "Epoch 7/500 - Train Loss: 0.35379454\n",
      "Epoch 7/500 - Test Loss: 0.35446100\n",
      "lr of epoch 7 => [0.001]\n",
      "Epoch 8/500 - Train Loss: 0.33675984\n",
      "Epoch 8/500 - Test Loss: 0.33948755\n",
      "lr of epoch 8 => [0.001]\n",
      "Epoch 9/500 - Train Loss: 0.32380581\n",
      "Epoch 9/500 - Test Loss: 0.32781486\n",
      "lr of epoch 9 => [0.001]\n",
      "Epoch 10/500 - Train Loss: 0.31348281\n",
      "Epoch 10/500 - Test Loss: 0.31904828\n",
      "lr of epoch 10 => [0.001]\n",
      "Epoch 11/500 - Train Loss: 0.30530168\n",
      "Epoch 11/500 - Test Loss: 0.31170885\n",
      "lr of epoch 11 => [0.001]\n",
      "Epoch 12/500 - Train Loss: 0.29875222\n",
      "Epoch 12/500 - Test Loss: 0.30526889\n",
      "lr of epoch 12 => [0.001]\n",
      "Epoch 13/500 - Train Loss: 0.29290556\n",
      "Epoch 13/500 - Test Loss: 0.30030330\n",
      "lr of epoch 13 => [0.001]\n",
      "Epoch 14/500 - Train Loss: 0.28805253\n",
      "Epoch 14/500 - Test Loss: 0.29588151\n",
      "lr of epoch 14 => [0.001]\n",
      "Epoch 15/500 - Train Loss: 0.28392667\n",
      "Epoch 15/500 - Test Loss: 0.29114116\n",
      "lr of epoch 15 => [0.001]\n",
      "Epoch 16/500 - Train Loss: 0.28029043\n",
      "Epoch 16/500 - Test Loss: 0.28771036\n",
      "lr of epoch 16 => [0.001]\n",
      "Epoch 17/500 - Train Loss: 0.27648811\n",
      "Epoch 17/500 - Test Loss: 0.28464473\n",
      "lr of epoch 17 => [0.001]\n",
      "Epoch 18/500 - Train Loss: 0.27308812\n",
      "Epoch 18/500 - Test Loss: 0.28109503\n",
      "lr of epoch 18 => [0.001]\n",
      "Epoch 19/500 - Train Loss: 0.27065082\n",
      "Epoch 19/500 - Test Loss: 0.27825437\n",
      "lr of epoch 19 => [0.001]\n",
      "Epoch 20/500 - Train Loss: 0.26810824\n",
      "Epoch 20/500 - Test Loss: 0.27587948\n",
      "lr of epoch 20 => [0.001]\n",
      "Epoch 21/500 - Train Loss: 0.26558409\n",
      "Epoch 21/500 - Test Loss: 0.27370538\n",
      "lr of epoch 21 => [0.001]\n",
      "Epoch 22/500 - Train Loss: 0.26340302\n",
      "Epoch 22/500 - Test Loss: 0.27140254\n",
      "lr of epoch 22 => [0.001]\n",
      "Epoch 23/500 - Train Loss: 0.26159651\n",
      "Epoch 23/500 - Test Loss: 0.26934731\n",
      "lr of epoch 23 => [0.001]\n",
      "Epoch 24/500 - Train Loss: 0.25978908\n",
      "Epoch 24/500 - Test Loss: 0.26830520\n",
      "lr of epoch 24 => [0.001]\n",
      "Epoch 25/500 - Train Loss: 0.25831235\n",
      "Epoch 25/500 - Test Loss: 0.26607388\n",
      "lr of epoch 25 => [0.001]\n",
      "Epoch 26/500 - Train Loss: 0.25684012\n",
      "Epoch 26/500 - Test Loss: 0.26557803\n",
      "lr of epoch 26 => [0.001]\n",
      "Epoch 27/500 - Train Loss: 0.25560518\n",
      "Epoch 27/500 - Test Loss: 0.26333762\n",
      "lr of epoch 27 => [0.001]\n",
      "Epoch 28/500 - Train Loss: 0.25425140\n",
      "Epoch 28/500 - Test Loss: 0.26227959\n",
      "lr of epoch 28 => [0.001]\n",
      "Epoch 29/500 - Train Loss: 0.25319270\n",
      "Epoch 29/500 - Test Loss: 0.26196390\n",
      "lr of epoch 29 => [0.001]\n",
      "Epoch 30/500 - Train Loss: 0.25225926\n",
      "Epoch 30/500 - Test Loss: 0.26017330\n",
      "lr of epoch 30 => [0.001]\n",
      "Epoch 31/500 - Train Loss: 0.25105708\n",
      "Epoch 31/500 - Test Loss: 0.25894170\n",
      "lr of epoch 31 => [0.001]\n",
      "Epoch 32/500 - Train Loss: 0.25035825\n",
      "Epoch 32/500 - Test Loss: 0.25900533\n",
      "lr of epoch 32 => [0.001]\n",
      "Epoch 33/500 - Train Loss: 0.24975363\n",
      "Epoch 33/500 - Test Loss: 0.25776835\n",
      "lr of epoch 33 => [0.001]\n",
      "Epoch 34/500 - Train Loss: 0.24877509\n",
      "Epoch 34/500 - Test Loss: 0.25640293\n",
      "lr of epoch 34 => [0.001]\n",
      "Epoch 35/500 - Train Loss: 0.24780332\n",
      "Epoch 35/500 - Test Loss: 0.25741677\n",
      "lr of epoch 35 => [0.001]\n",
      "Epoch 36/500 - Train Loss: 0.24753537\n",
      "Epoch 36/500 - Test Loss: 0.25554545\n",
      "lr of epoch 36 => [0.001]\n",
      "Epoch 37/500 - Train Loss: 0.24648231\n",
      "Epoch 37/500 - Test Loss: 0.25392191\n",
      "lr of epoch 37 => [0.001]\n",
      "Epoch 38/500 - Train Loss: 0.24591904\n",
      "Epoch 38/500 - Test Loss: 0.25380840\n",
      "lr of epoch 38 => [0.001]\n",
      "Epoch 39/500 - Train Loss: 0.24545400\n",
      "Epoch 39/500 - Test Loss: 0.25350130\n",
      "lr of epoch 39 => [0.001]\n",
      "Epoch 40/500 - Train Loss: 0.24521057\n",
      "Epoch 40/500 - Test Loss: 0.25319374\n",
      "lr of epoch 40 => [0.001]\n",
      "Epoch 41/500 - Train Loss: 0.24484491\n",
      "Epoch 41/500 - Test Loss: 0.25341716\n",
      "lr of epoch 41 => [0.001]\n",
      "Epoch 42/500 - Train Loss: 0.24642745\n",
      "Epoch 42/500 - Test Loss: 0.25194947\n",
      "lr of epoch 42 => [0.001]\n",
      "Epoch 43/500 - Train Loss: 0.24347448\n",
      "Epoch 43/500 - Test Loss: 0.25108258\n",
      "lr of epoch 43 => [0.001]\n",
      "Epoch 44/500 - Train Loss: 0.24276199\n",
      "Epoch 44/500 - Test Loss: 0.25077016\n",
      "lr of epoch 44 => [0.001]\n",
      "Epoch 45/500 - Train Loss: 0.24231233\n",
      "Epoch 45/500 - Test Loss: 0.25057914\n",
      "lr of epoch 45 => [0.001]\n",
      "Epoch 46/500 - Train Loss: 0.24199469\n",
      "Epoch 46/500 - Test Loss: 0.25000267\n",
      "lr of epoch 46 => [0.001]\n",
      "Epoch 47/500 - Train Loss: 0.24222378\n",
      "Epoch 47/500 - Test Loss: 0.24973584\n",
      "lr of epoch 47 => [0.001]\n",
      "Epoch 48/500 - Train Loss: 0.24174406\n",
      "Epoch 48/500 - Test Loss: 0.25027273\n",
      "lr of epoch 48 => [0.001]\n",
      "Epoch 49/500 - Train Loss: 0.24141499\n",
      "Epoch 49/500 - Test Loss: 0.24893264\n",
      "lr of epoch 49 => [0.001]\n",
      "Epoch 50/500 - Train Loss: 0.24083933\n",
      "Epoch 50/500 - Test Loss: 0.24870135\n",
      "lr of epoch 50 => [0.001]\n",
      "Epoch 51/500 - Train Loss: 0.24028666\n",
      "Epoch 51/500 - Test Loss: 0.24977543\n",
      "lr of epoch 51 => [0.001]\n",
      "Epoch 52/500 - Train Loss: 0.24043270\n",
      "Epoch 52/500 - Test Loss: 0.24767717\n",
      "lr of epoch 52 => [0.001]\n",
      "Epoch 53/500 - Train Loss: 0.23970139\n",
      "Epoch 53/500 - Test Loss: 0.24880100\n",
      "lr of epoch 53 => [0.001]\n",
      "Epoch 54/500 - Train Loss: 0.24010648\n",
      "Epoch 54/500 - Test Loss: 0.24785479\n",
      "lr of epoch 54 => [0.001]\n",
      "Epoch 55/500 - Train Loss: 0.23938429\n",
      "Epoch 55/500 - Test Loss: 0.24668461\n",
      "lr of epoch 55 => [0.001]\n",
      "Epoch 56/500 - Train Loss: 0.23892817\n",
      "Epoch 56/500 - Test Loss: 0.24655975\n",
      "lr of epoch 56 => [0.001]\n",
      "Epoch 57/500 - Train Loss: 0.23839388\n",
      "Epoch 57/500 - Test Loss: 0.24629032\n",
      "lr of epoch 57 => [0.001]\n",
      "Epoch 58/500 - Train Loss: 0.23812671\n",
      "Epoch 58/500 - Test Loss: 0.24580289\n",
      "lr of epoch 58 => [0.001]\n",
      "Epoch 59/500 - Train Loss: 0.23770857\n",
      "Epoch 59/500 - Test Loss: 0.24579954\n",
      "lr of epoch 59 => [0.001]\n",
      "Epoch 60/500 - Train Loss: 0.23756372\n",
      "Epoch 60/500 - Test Loss: 0.24556270\n",
      "lr of epoch 60 => [0.001]\n",
      "Epoch 61/500 - Train Loss: 0.23719216\n",
      "Epoch 61/500 - Test Loss: 0.24551595\n",
      "lr of epoch 61 => [0.001]\n",
      "Epoch 62/500 - Train Loss: 0.23699367\n",
      "Epoch 62/500 - Test Loss: 0.24542995\n",
      "lr of epoch 62 => [0.001]\n",
      "Epoch 63/500 - Train Loss: 0.23673562\n",
      "Epoch 63/500 - Test Loss: 0.24517562\n",
      "lr of epoch 63 => [0.001]\n",
      "Epoch 64/500 - Train Loss: 0.23629926\n",
      "Epoch 64/500 - Test Loss: 0.24503024\n",
      "lr of epoch 64 => [0.001]\n",
      "Epoch 65/500 - Train Loss: 0.23609079\n",
      "Epoch 65/500 - Test Loss: 0.24470125\n",
      "lr of epoch 65 => [0.001]\n",
      "Epoch 66/500 - Train Loss: 0.23605842\n",
      "Epoch 66/500 - Test Loss: 0.24431589\n",
      "lr of epoch 66 => [0.001]\n",
      "Epoch 67/500 - Train Loss: 0.23562043\n",
      "Epoch 67/500 - Test Loss: 0.24408021\n",
      "lr of epoch 67 => [0.001]\n",
      "Epoch 68/500 - Train Loss: 0.23534450\n",
      "Epoch 68/500 - Test Loss: 0.24403676\n",
      "lr of epoch 68 => [0.001]\n",
      "Epoch 69/500 - Train Loss: 0.23520608\n",
      "Epoch 69/500 - Test Loss: 0.24320779\n",
      "lr of epoch 69 => [0.001]\n",
      "Epoch 70/500 - Train Loss: 0.23563977\n",
      "Epoch 70/500 - Test Loss: 0.24358752\n",
      "lr of epoch 70 => [0.001]\n",
      "Epoch 71/500 - Train Loss: 0.23508809\n",
      "Epoch 71/500 - Test Loss: 0.24403475\n",
      "lr of epoch 71 => [0.001]\n",
      "Epoch 72/500 - Train Loss: 0.23457695\n",
      "Epoch 72/500 - Test Loss: 0.24264685\n",
      "lr of epoch 72 => [0.001]\n",
      "Epoch 73/500 - Train Loss: 0.23427333\n",
      "Epoch 73/500 - Test Loss: 0.24265712\n",
      "lr of epoch 73 => [0.001]\n",
      "Epoch 74/500 - Train Loss: 0.23373798\n",
      "Epoch 74/500 - Test Loss: 0.24188123\n",
      "lr of epoch 74 => [0.001]\n",
      "Epoch 75/500 - Train Loss: 0.23378653\n",
      "Epoch 75/500 - Test Loss: 0.24262063\n",
      "lr of epoch 75 => [0.001]\n",
      "Epoch 76/500 - Train Loss: 0.23352412\n",
      "Epoch 76/500 - Test Loss: 0.24144727\n",
      "lr of epoch 76 => [0.001]\n",
      "Epoch 77/500 - Train Loss: 0.23314338\n",
      "Epoch 77/500 - Test Loss: 0.24135624\n",
      "lr of epoch 77 => [0.001]\n",
      "Epoch 78/500 - Train Loss: 0.23235053\n",
      "Epoch 78/500 - Test Loss: 0.24051688\n",
      "lr of epoch 78 => [0.001]\n",
      "Epoch 79/500 - Train Loss: 0.23184218\n",
      "Epoch 79/500 - Test Loss: 0.24035116\n",
      "lr of epoch 79 => [0.001]\n",
      "Epoch 80/500 - Train Loss: 0.23182937\n",
      "Epoch 80/500 - Test Loss: 0.24103028\n",
      "lr of epoch 80 => [0.001]\n",
      "Epoch 81/500 - Train Loss: 0.23246050\n",
      "Epoch 81/500 - Test Loss: 0.24162741\n",
      "lr of epoch 81 => [0.001]\n",
      "Epoch 82/500 - Train Loss: 0.23178905\n",
      "Epoch 82/500 - Test Loss: 0.24016380\n",
      "lr of epoch 82 => [0.001]\n",
      "Epoch 83/500 - Train Loss: 0.23166310\n",
      "Epoch 83/500 - Test Loss: 0.24109819\n",
      "lr of epoch 83 => [0.001]\n",
      "Epoch 84/500 - Train Loss: 0.23158631\n",
      "Epoch 84/500 - Test Loss: 0.23974734\n",
      "lr of epoch 84 => [0.001]\n",
      "Epoch 85/500 - Train Loss: 0.23067680\n",
      "Epoch 85/500 - Test Loss: 0.23950734\n",
      "lr of epoch 85 => [0.001]\n",
      "Epoch 86/500 - Train Loss: 0.23064165\n",
      "Epoch 86/500 - Test Loss: 0.23953721\n",
      "lr of epoch 86 => [0.001]\n",
      "Epoch 87/500 - Train Loss: 0.23006196\n",
      "Epoch 87/500 - Test Loss: 0.23929065\n",
      "lr of epoch 87 => [0.001]\n",
      "Epoch 88/500 - Train Loss: 0.22982994\n",
      "Epoch 88/500 - Test Loss: 0.23913973\n",
      "lr of epoch 88 => [0.001]\n",
      "Epoch 89/500 - Train Loss: 0.22915656\n",
      "Epoch 89/500 - Test Loss: 0.23865184\n",
      "lr of epoch 89 => [0.001]\n",
      "Epoch 90/500 - Train Loss: 0.22928530\n",
      "Epoch 90/500 - Test Loss: 0.23887030\n",
      "lr of epoch 90 => [0.001]\n",
      "Epoch 91/500 - Train Loss: 0.22906257\n",
      "Epoch 91/500 - Test Loss: 0.23761396\n",
      "lr of epoch 91 => [0.001]\n",
      "Epoch 92/500 - Train Loss: 0.22847473\n",
      "Epoch 92/500 - Test Loss: 0.23762509\n",
      "lr of epoch 92 => [0.001]\n",
      "Epoch 93/500 - Train Loss: 0.22800651\n",
      "Epoch 93/500 - Test Loss: 0.23739421\n",
      "lr of epoch 93 => [0.001]\n",
      "Epoch 94/500 - Train Loss: 0.22779669\n",
      "Epoch 94/500 - Test Loss: 0.23741438\n",
      "lr of epoch 94 => [0.001]\n",
      "Epoch 95/500 - Train Loss: 0.22802526\n",
      "Epoch 95/500 - Test Loss: 0.23713943\n",
      "lr of epoch 95 => [0.001]\n",
      "Epoch 96/500 - Train Loss: 0.22796344\n",
      "Epoch 96/500 - Test Loss: 0.23749495\n",
      "lr of epoch 96 => [0.001]\n",
      "Epoch 97/500 - Train Loss: 0.22723691\n",
      "Epoch 97/500 - Test Loss: 0.23688183\n",
      "lr of epoch 97 => [0.001]\n",
      "Epoch 98/500 - Train Loss: 0.22700027\n",
      "Epoch 98/500 - Test Loss: 0.23843260\n",
      "lr of epoch 98 => [0.001]\n",
      "Epoch 99/500 - Train Loss: 0.22777756\n",
      "Epoch 99/500 - Test Loss: 0.23651891\n",
      "lr of epoch 99 => [0.001]\n",
      "Epoch 100/500 - Train Loss: 0.22695352\n",
      "Epoch 100/500 - Test Loss: 0.23664456\n",
      "lr of epoch 100 => [0.001]\n",
      "Epoch 101/500 - Train Loss: 0.22659248\n",
      "Epoch 101/500 - Test Loss: 0.23673095\n",
      "lr of epoch 101 => [0.001]\n",
      "Epoch 102/500 - Train Loss: 0.22633307\n",
      "Epoch 102/500 - Test Loss: 0.23587280\n",
      "lr of epoch 102 => [0.001]\n",
      "Epoch 103/500 - Train Loss: 0.22675570\n",
      "Epoch 103/500 - Test Loss: 0.23755217\n",
      "lr of epoch 103 => [0.001]\n",
      "Epoch 104/500 - Train Loss: 0.22654526\n",
      "Epoch 104/500 - Test Loss: 0.23615837\n",
      "lr of epoch 104 => [0.001]\n",
      "Epoch 105/500 - Train Loss: 0.22578720\n",
      "Epoch 105/500 - Test Loss: 0.23555993\n",
      "lr of epoch 105 => [0.001]\n",
      "Epoch 106/500 - Train Loss: 0.22534049\n",
      "Epoch 106/500 - Test Loss: 0.23594575\n",
      "lr of epoch 106 => [0.001]\n",
      "Epoch 107/500 - Train Loss: 0.22575892\n",
      "Epoch 107/500 - Test Loss: 0.23573535\n",
      "lr of epoch 107 => [0.001]\n",
      "Epoch 108/500 - Train Loss: 0.22572321\n",
      "Epoch 108/500 - Test Loss: 0.23593430\n",
      "lr of epoch 108 => [0.001]\n",
      "Epoch 109/500 - Train Loss: 0.22546851\n",
      "Epoch 109/500 - Test Loss: 0.23517629\n",
      "lr of epoch 109 => [0.001]\n",
      "Epoch 110/500 - Train Loss: 0.22532050\n",
      "Epoch 110/500 - Test Loss: 0.23563578\n",
      "lr of epoch 110 => [0.001]\n",
      "Epoch 111/500 - Train Loss: 0.22532680\n",
      "Epoch 111/500 - Test Loss: 0.23588803\n",
      "lr of epoch 111 => [0.001]\n",
      "Epoch 112/500 - Train Loss: 0.22512978\n",
      "Epoch 112/500 - Test Loss: 0.23535935\n",
      "lr of epoch 112 => [0.001]\n",
      "Epoch 113/500 - Train Loss: 0.22543137\n",
      "Epoch 113/500 - Test Loss: 0.23603753\n",
      "lr of epoch 113 => [0.001]\n",
      "Epoch 114/500 - Train Loss: 0.22531952\n",
      "Epoch 114/500 - Test Loss: 0.23562374\n",
      "lr of epoch 114 => [0.001]\n",
      "Epoch 115/500 - Train Loss: 0.22572217\n",
      "Epoch 115/500 - Test Loss: 0.23560579\n",
      "lr of epoch 115 => [0.001]\n",
      "Epoch 116/500 - Train Loss: 0.22478613\n",
      "Epoch 116/500 - Test Loss: 0.23469942\n",
      "lr of epoch 116 => [0.001]\n",
      "Epoch 117/500 - Train Loss: 0.22466536\n",
      "Epoch 117/500 - Test Loss: 0.23474455\n",
      "lr of epoch 117 => [0.001]\n",
      "Epoch 118/500 - Train Loss: 0.22480257\n",
      "Epoch 118/500 - Test Loss: 0.23679890\n",
      "lr of epoch 118 => [0.001]\n",
      "Epoch 119/500 - Train Loss: 0.22487157\n",
      "Epoch 119/500 - Test Loss: 0.23474347\n",
      "lr of epoch 119 => [0.001]\n",
      "Epoch 120/500 - Train Loss: 0.22445111\n",
      "Epoch 120/500 - Test Loss: 0.23447268\n",
      "lr of epoch 120 => [0.001]\n",
      "Epoch 121/500 - Train Loss: 0.22431067\n",
      "Epoch 121/500 - Test Loss: 0.23455749\n",
      "lr of epoch 121 => [0.001]\n",
      "Epoch 122/500 - Train Loss: 0.22378507\n",
      "Epoch 122/500 - Test Loss: 0.23384324\n",
      "lr of epoch 122 => [0.001]\n",
      "Epoch 123/500 - Train Loss: 0.22367943\n",
      "Epoch 123/500 - Test Loss: 0.23388554\n",
      "lr of epoch 123 => [0.001]\n",
      "Epoch 124/500 - Train Loss: 0.22323656\n",
      "Epoch 124/500 - Test Loss: 0.23415500\n",
      "lr of epoch 124 => [0.001]\n",
      "Epoch 125/500 - Train Loss: 0.22315436\n",
      "Epoch 125/500 - Test Loss: 0.23408224\n",
      "lr of epoch 125 => [0.001]\n",
      "Epoch 126/500 - Train Loss: 0.22310414\n",
      "Epoch 126/500 - Test Loss: 0.23330191\n",
      "lr of epoch 126 => [0.001]\n",
      "Epoch 127/500 - Train Loss: 0.22299432\n",
      "Epoch 127/500 - Test Loss: 0.23354790\n",
      "lr of epoch 127 => [0.001]\n",
      "Epoch 128/500 - Train Loss: 0.22291201\n",
      "Epoch 128/500 - Test Loss: 0.23404971\n",
      "lr of epoch 128 => [0.001]\n",
      "Epoch 129/500 - Train Loss: 0.22270136\n",
      "Epoch 129/500 - Test Loss: 0.23343197\n",
      "lr of epoch 129 => [0.001]\n",
      "Epoch 130/500 - Train Loss: 0.22300694\n",
      "Epoch 130/500 - Test Loss: 0.23386788\n",
      "lr of epoch 130 => [0.001]\n",
      "Epoch 131/500 - Train Loss: 0.22347008\n",
      "Epoch 131/500 - Test Loss: 0.23342708\n",
      "lr of epoch 131 => [0.001]\n",
      "Epoch 132/500 - Train Loss: 0.22317079\n",
      "Epoch 132/500 - Test Loss: 0.23474109\n",
      "lr of epoch 132 => [0.001]\n",
      "Epoch 133/500 - Train Loss: 0.22286528\n",
      "Epoch 133/500 - Test Loss: 0.23347183\n",
      "lr of epoch 133 => [0.001]\n",
      "Epoch 134/500 - Train Loss: 0.22249743\n",
      "Epoch 134/500 - Test Loss: 0.23281223\n",
      "lr of epoch 134 => [0.001]\n",
      "Epoch 135/500 - Train Loss: 0.22177910\n",
      "Epoch 135/500 - Test Loss: 0.23242564\n",
      "lr of epoch 135 => [0.001]\n",
      "Epoch 136/500 - Train Loss: 0.22200369\n",
      "Epoch 136/500 - Test Loss: 0.23264770\n",
      "lr of epoch 136 => [0.001]\n",
      "Epoch 137/500 - Train Loss: 0.22190878\n",
      "Epoch 137/500 - Test Loss: 0.23305455\n",
      "lr of epoch 137 => [0.001]\n",
      "Epoch 139/500 - Train Loss: 0.22172054\n",
      "Epoch 139/500 - Test Loss: 0.23274496\n",
      "lr of epoch 139 => [0.001]\n",
      "Epoch 140/500 - Train Loss: 0.22145958\n",
      "Epoch 140/500 - Test Loss: 0.23202018\n",
      "lr of epoch 140 => [0.001]\n",
      "Epoch 141/500 - Train Loss: 0.22130575\n",
      "Epoch 141/500 - Test Loss: 0.23293548\n",
      "lr of epoch 141 => [0.001]\n",
      "Epoch 142/500 - Train Loss: 0.22087845\n",
      "Epoch 142/500 - Test Loss: 0.23226444\n",
      "lr of epoch 142 => [0.001]\n",
      "Epoch 143/500 - Train Loss: 0.22074183\n",
      "Epoch 143/500 - Test Loss: 0.23219526\n",
      "lr of epoch 143 => [0.001]\n",
      "Epoch 144/500 - Train Loss: 0.22099753\n",
      "Epoch 144/500 - Test Loss: 0.23231572\n",
      "lr of epoch 144 => [0.001]\n",
      "Epoch 145/500 - Train Loss: 0.22078162\n",
      "Epoch 145/500 - Test Loss: 0.23204858\n",
      "lr of epoch 145 => [0.001]\n",
      "Epoch 146/500 - Train Loss: 0.22048730\n",
      "Epoch 146/500 - Test Loss: 0.23193677\n",
      "lr of epoch 146 => [0.001]\n",
      "Epoch 147/500 - Train Loss: 0.22056159\n",
      "Epoch 147/500 - Test Loss: 0.23179595\n",
      "lr of epoch 147 => [0.001]\n",
      "Epoch 148/500 - Train Loss: 0.22022434\n",
      "Epoch 148/500 - Test Loss: 0.23192819\n",
      "lr of epoch 148 => [0.001]\n",
      "Epoch 149/500 - Train Loss: 0.22009777\n",
      "Epoch 149/500 - Test Loss: 0.23170509\n",
      "lr of epoch 149 => [0.001]\n",
      "Epoch 150/500 - Train Loss: 0.22003451\n",
      "Epoch 150/500 - Test Loss: 0.23199555\n",
      "lr of epoch 150 => [0.001]\n",
      "Epoch 151/500 - Train Loss: 0.22026962\n",
      "Epoch 151/500 - Test Loss: 0.23172908\n",
      "lr of epoch 151 => [0.001]\n",
      "Epoch 152/500 - Train Loss: 0.21996713\n",
      "Epoch 152/500 - Test Loss: 0.23192314\n",
      "lr of epoch 152 => [0.001]\n",
      "Epoch 153/500 - Train Loss: 0.22031187\n",
      "Epoch 153/500 - Test Loss: 0.23168173\n",
      "lr of epoch 153 => [0.001]\n",
      "Epoch 154/500 - Train Loss: 0.21961060\n",
      "Epoch 154/500 - Test Loss: 0.23184373\n",
      "lr of epoch 154 => [0.001]\n",
      "Epoch 155/500 - Train Loss: 0.21943814\n",
      "Epoch 155/500 - Test Loss: 0.23070979\n",
      "lr of epoch 155 => [0.001]\n",
      "Epoch 156/500 - Train Loss: 0.21893941\n",
      "Epoch 156/500 - Test Loss: 0.23113079\n",
      "lr of epoch 156 => [0.001]\n",
      "Epoch 157/500 - Train Loss: 0.21897926\n",
      "Epoch 157/500 - Test Loss: 0.23107907\n",
      "lr of epoch 157 => [0.001]\n",
      "Epoch 158/500 - Train Loss: 0.21917940\n",
      "Epoch 158/500 - Test Loss: 0.23119896\n",
      "lr of epoch 158 => [0.001]\n",
      "Epoch 159/500 - Train Loss: 0.21945834\n",
      "Epoch 159/500 - Test Loss: 0.23117489\n",
      "lr of epoch 159 => [0.001]\n",
      "Epoch 160/500 - Train Loss: 0.21914083\n",
      "Epoch 160/500 - Test Loss: 0.23056269\n",
      "lr of epoch 160 => [0.001]\n",
      "Epoch 161/500 - Train Loss: 0.21876878\n",
      "Epoch 161/500 - Test Loss: 0.23094423\n",
      "lr of epoch 161 => [0.001]\n",
      "Epoch 162/500 - Train Loss: 0.21862592\n",
      "Epoch 162/500 - Test Loss: 0.23048969\n",
      "lr of epoch 162 => [0.001]\n",
      "Epoch 163/500 - Train Loss: 0.21884454\n",
      "Epoch 163/500 - Test Loss: 0.23063533\n",
      "lr of epoch 163 => [0.001]\n",
      "Epoch 164/500 - Train Loss: 0.21882178\n",
      "Epoch 164/500 - Test Loss: 0.23174938\n",
      "lr of epoch 164 => [0.001]\n",
      "Epoch 165/500 - Train Loss: 0.21873607\n",
      "Epoch 165/500 - Test Loss: 0.23022230\n",
      "lr of epoch 165 => [0.001]\n",
      "Epoch 166/500 - Train Loss: 0.21865169\n",
      "Epoch 166/500 - Test Loss: 0.23080825\n",
      "lr of epoch 166 => [0.001]\n",
      "Epoch 167/500 - Train Loss: 0.21799852\n",
      "Epoch 167/500 - Test Loss: 0.22993554\n",
      "lr of epoch 167 => [0.001]\n",
      "Epoch 168/500 - Train Loss: 0.21841774\n",
      "Epoch 168/500 - Test Loss: 0.22996890\n",
      "lr of epoch 168 => [0.001]\n",
      "Epoch 169/500 - Train Loss: 0.21807949\n",
      "Epoch 169/500 - Test Loss: 0.23023831\n",
      "lr of epoch 169 => [0.001]\n",
      "Epoch 170/500 - Train Loss: 0.21839309\n",
      "Epoch 170/500 - Test Loss: 0.22996137\n",
      "lr of epoch 170 => [0.001]\n",
      "Epoch 171/500 - Train Loss: 0.21833951\n",
      "Epoch 171/500 - Test Loss: 0.23011799\n",
      "lr of epoch 171 => [0.001]\n",
      "Epoch 172/500 - Train Loss: 0.21792609\n",
      "Epoch 172/500 - Test Loss: 0.23061152\n",
      "lr of epoch 172 => [0.001]\n",
      "Epoch 174/500 - Train Loss: 0.21769043\n",
      "Epoch 174/500 - Test Loss: 0.23014551\n",
      "lr of epoch 174 => [0.001]\n",
      "Epoch 175/500 - Train Loss: 0.21787028\n",
      "Epoch 175/500 - Test Loss: 0.23008134\n",
      "lr of epoch 175 => [0.001]\n",
      "Epoch 177/500 - Train Loss: 0.21731908\n",
      "Epoch 177/500 - Test Loss: 0.22954369\n",
      "lr of epoch 177 => [0.001]\n",
      "Epoch 178/500 - Train Loss: 0.21716272\n",
      "Epoch 178/500 - Test Loss: 0.22959084\n",
      "lr of epoch 178 => [0.001]\n",
      "Epoch 179/500 - Train Loss: 0.21758104\n",
      "Epoch 179/500 - Test Loss: 0.22956193\n",
      "lr of epoch 179 => [0.001]\n",
      "Epoch 180/500 - Train Loss: 0.21768672\n",
      "Epoch 180/500 - Test Loss: 0.22967957\n",
      "lr of epoch 180 => [0.001]\n",
      "Epoch 181/500 - Train Loss: 0.21732696\n",
      "Epoch 181/500 - Test Loss: 0.23018145\n",
      "lr of epoch 181 => [0.001]\n",
      "Epoch 182/500 - Train Loss: 0.21714411\n",
      "Epoch 182/500 - Test Loss: 0.22938737\n",
      "lr of epoch 182 => [0.001]\n",
      "Epoch 183/500 - Train Loss: 0.21760475\n",
      "Epoch 183/500 - Test Loss: 0.22980932\n",
      "lr of epoch 183 => [0.001]\n",
      "Epoch 184/500 - Train Loss: 0.21719565\n",
      "Epoch 184/500 - Test Loss: 0.23074322\n",
      "lr of epoch 184 => [0.001]\n",
      "Epoch 185/500 - Train Loss: 0.21703235\n",
      "Epoch 185/500 - Test Loss: 0.22909043\n",
      "lr of epoch 185 => [0.001]\n",
      "Epoch 186/500 - Train Loss: 0.21647862\n",
      "Epoch 186/500 - Test Loss: 0.22897261\n",
      "lr of epoch 186 => [0.001]\n",
      "Epoch 187/500 - Train Loss: 0.21622203\n",
      "Epoch 187/500 - Test Loss: 0.22899802\n",
      "lr of epoch 187 => [0.001]\n",
      "Epoch 188/500 - Train Loss: 0.21645130\n",
      "Epoch 188/500 - Test Loss: 0.22920984\n",
      "lr of epoch 188 => [0.001]\n",
      "Epoch 190/500 - Train Loss: 0.21549034\n",
      "Epoch 190/500 - Test Loss: 0.22909193\n",
      "lr of epoch 190 => [0.001]\n",
      "Epoch 191/500 - Train Loss: 0.21561219\n",
      "Epoch 191/500 - Test Loss: 0.22805475\n",
      "lr of epoch 191 => [0.001]\n",
      "Epoch 192/500 - Train Loss: 0.21527722\n",
      "Epoch 192/500 - Test Loss: 0.22815471\n",
      "lr of epoch 192 => [0.001]\n",
      "Epoch 193/500 - Train Loss: 0.21534134\n",
      "Epoch 193/500 - Test Loss: 0.22845353\n",
      "lr of epoch 193 => [0.001]\n",
      "Epoch 194/500 - Train Loss: 0.21559796\n",
      "Epoch 194/500 - Test Loss: 0.22779381\n",
      "lr of epoch 194 => [0.001]\n",
      "Epoch 195/500 - Train Loss: 0.21480240\n",
      "Epoch 195/500 - Test Loss: 0.22729842\n",
      "lr of epoch 195 => [0.001]\n",
      "Epoch 196/500 - Train Loss: 0.21455615\n",
      "Epoch 196/500 - Test Loss: 0.22723518\n",
      "lr of epoch 196 => [0.001]\n",
      "Epoch 197/500 - Train Loss: 0.21457828\n",
      "Epoch 197/500 - Test Loss: 0.22849688\n",
      "lr of epoch 197 => [0.001]\n",
      "Epoch 198/500 - Train Loss: 0.21451563\n",
      "Epoch 198/500 - Test Loss: 0.22767692\n",
      "lr of epoch 198 => [0.001]\n",
      "Epoch 199/500 - Train Loss: 0.21427638\n",
      "Epoch 199/500 - Test Loss: 0.22784832\n",
      "lr of epoch 199 => [0.001]\n",
      "Epoch 200/500 - Train Loss: 0.21484266\n",
      "Epoch 200/500 - Test Loss: 0.22753484\n",
      "lr of epoch 200 => [0.001]\n",
      "Epoch 201/500 - Train Loss: 0.21452516\n",
      "Epoch 201/500 - Test Loss: 0.22695665\n",
      "lr of epoch 201 => [0.001]\n",
      "Epoch 202/500 - Train Loss: 0.21357217\n",
      "Epoch 202/500 - Test Loss: 0.22662554\n",
      "lr of epoch 202 => [0.001]\n",
      "Epoch 203/500 - Train Loss: 0.21342759\n",
      "Epoch 203/500 - Test Loss: 0.22656371\n",
      "lr of epoch 203 => [0.001]\n",
      "Epoch 204/500 - Train Loss: 0.21356904\n",
      "Epoch 204/500 - Test Loss: 0.22660169\n",
      "lr of epoch 204 => [0.001]\n",
      "Epoch 205/500 - Train Loss: 0.21360500\n",
      "Epoch 205/500 - Test Loss: 0.22742207\n",
      "lr of epoch 205 => [0.001]\n",
      "Epoch 206/500 - Train Loss: 0.21365072\n",
      "Epoch 206/500 - Test Loss: 0.22728824\n",
      "lr of epoch 206 => [0.001]\n",
      "Epoch 207/500 - Train Loss: 0.21347816\n",
      "Epoch 207/500 - Test Loss: 0.22728864\n",
      "lr of epoch 207 => [0.001]\n",
      "Epoch 208/500 - Train Loss: 0.21435767\n",
      "Epoch 208/500 - Test Loss: 0.22764449\n",
      "lr of epoch 208 => [0.001]\n",
      "Epoch 209/500 - Train Loss: 0.21337343\n",
      "Epoch 209/500 - Test Loss: 0.22643032\n",
      "lr of epoch 209 => [0.001]\n",
      "Epoch 211/500 - Train Loss: 0.21233554\n",
      "Epoch 211/500 - Test Loss: 0.22578779\n",
      "lr of epoch 211 => [0.001]\n",
      "Epoch 212/500 - Train Loss: 0.21200591\n",
      "Epoch 212/500 - Test Loss: 0.22551285\n",
      "lr of epoch 212 => [0.001]\n",
      "Epoch 213/500 - Train Loss: 0.21210169\n",
      "Epoch 213/500 - Test Loss: 0.22567244\n",
      "lr of epoch 213 => [0.001]\n",
      "Epoch 214/500 - Train Loss: 0.21186213\n",
      "Epoch 214/500 - Test Loss: 0.22588535\n",
      "lr of epoch 214 => [0.001]\n",
      "Epoch 215/500 - Train Loss: 0.21214832\n",
      "Epoch 215/500 - Test Loss: 0.22615686\n",
      "lr of epoch 215 => [0.001]\n",
      "Epoch 216/500 - Train Loss: 0.21217563\n",
      "Epoch 216/500 - Test Loss: 0.22578816\n",
      "lr of epoch 216 => [0.001]\n",
      "Epoch 218/500 - Train Loss: 0.21194672\n",
      "Epoch 218/500 - Test Loss: 0.22638556\n",
      "lr of epoch 218 => [0.001]\n",
      "Epoch 219/500 - Train Loss: 0.21199158\n",
      "Epoch 219/500 - Test Loss: 0.22533059\n",
      "lr of epoch 219 => [0.001]\n",
      "Epoch 220/500 - Train Loss: 0.21181787\n",
      "Epoch 220/500 - Test Loss: 0.22488385\n",
      "lr of epoch 220 => [0.001]\n",
      "Epoch 221/500 - Train Loss: 0.21168287\n",
      "Epoch 221/500 - Test Loss: 0.22566976\n",
      "lr of epoch 221 => [0.001]\n",
      "Epoch 222/500 - Train Loss: 0.21124916\n",
      "Epoch 222/500 - Test Loss: 0.22478474\n",
      "lr of epoch 222 => [0.001]\n",
      "Epoch 223/500 - Train Loss: 0.21114918\n",
      "Epoch 223/500 - Test Loss: 0.22527221\n",
      "lr of epoch 223 => [0.001]\n",
      "Epoch 224/500 - Train Loss: 0.21139186\n",
      "Epoch 224/500 - Test Loss: 0.22487751\n",
      "lr of epoch 224 => [0.001]\n",
      "Epoch 225/500 - Train Loss: 0.21105692\n",
      "Epoch 225/500 - Test Loss: 0.22464698\n",
      "lr of epoch 225 => [0.001]\n",
      "Epoch 226/500 - Train Loss: 0.21139735\n",
      "Epoch 226/500 - Test Loss: 0.22522009\n",
      "lr of epoch 226 => [0.001]\n",
      "Epoch 227/500 - Train Loss: 0.21120191\n",
      "Epoch 227/500 - Test Loss: 0.22490802\n",
      "lr of epoch 227 => [0.001]\n",
      "Epoch 228/500 - Train Loss: 0.21123943\n",
      "Epoch 228/500 - Test Loss: 0.22538607\n",
      "lr of epoch 228 => [0.001]\n",
      "Epoch 229/500 - Train Loss: 0.21134647\n",
      "Epoch 229/500 - Test Loss: 0.22470431\n",
      "lr of epoch 229 => [0.001]\n",
      "Epoch 230/500 - Train Loss: 0.21081829\n",
      "Epoch 230/500 - Test Loss: 0.22462318\n",
      "lr of epoch 230 => [0.001]\n",
      "Epoch 231/500 - Train Loss: 0.21122930\n",
      "Epoch 231/500 - Test Loss: 0.22523231\n",
      "lr of epoch 231 => [0.001]\n",
      "Epoch 232/500 - Train Loss: 0.21068526\n",
      "Epoch 232/500 - Test Loss: 0.22514701\n",
      "lr of epoch 232 => [0.001]\n",
      "Epoch 233/500 - Train Loss: 0.21098271\n",
      "Epoch 233/500 - Test Loss: 0.22466752\n",
      "lr of epoch 233 => [0.001]\n",
      "Epoch 234/500 - Train Loss: 0.21045012\n",
      "Epoch 234/500 - Test Loss: 0.22402304\n",
      "lr of epoch 234 => [0.001]\n",
      "Epoch 235/500 - Train Loss: 0.21023026\n",
      "Epoch 235/500 - Test Loss: 0.22710675\n",
      "lr of epoch 235 => [0.001]\n",
      "Epoch 236/500 - Train Loss: 0.21136322\n",
      "Epoch 236/500 - Test Loss: 0.22426184\n",
      "lr of epoch 236 => [0.001]\n",
      "Epoch 237/500 - Train Loss: 0.21023056\n",
      "Epoch 237/500 - Test Loss: 0.22418443\n",
      "lr of epoch 237 => [0.001]\n",
      "Epoch 238/500 - Train Loss: 0.21002796\n",
      "Epoch 238/500 - Test Loss: 0.22421109\n",
      "lr of epoch 238 => [0.001]\n",
      "Epoch 239/500 - Train Loss: 0.21036353\n",
      "Epoch 239/500 - Test Loss: 0.22452425\n",
      "lr of epoch 239 => [0.001]\n",
      "Epoch 240/500 - Train Loss: 0.21020597\n",
      "Epoch 240/500 - Test Loss: 0.22409839\n",
      "lr of epoch 240 => [0.001]\n",
      "Epoch 241/500 - Train Loss: 0.20986134\n",
      "Epoch 241/500 - Test Loss: 0.22445877\n",
      "lr of epoch 241 => [0.001]\n",
      "Epoch 242/500 - Train Loss: 0.20961083\n",
      "Epoch 242/500 - Test Loss: 0.22409428\n",
      "lr of epoch 242 => [0.001]\n",
      "Epoch 243/500 - Train Loss: 0.20979528\n",
      "Epoch 243/500 - Test Loss: 0.22432518\n",
      "lr of epoch 243 => [0.001]\n",
      "Epoch 244/500 - Train Loss: 0.20984115\n",
      "Epoch 244/500 - Test Loss: 0.22442281\n",
      "lr of epoch 244 => [0.001]\n",
      "Epoch 245/500 - Train Loss: 0.20953921\n",
      "Epoch 245/500 - Test Loss: 0.22417268\n",
      "lr of epoch 245 => [0.001]\n",
      "Epoch 246/500 - Train Loss: 0.21002923\n",
      "Epoch 246/500 - Test Loss: 0.22496756\n",
      "lr of epoch 246 => [0.001]\n",
      "Epoch 247/500 - Train Loss: 0.20990538\n",
      "Epoch 247/500 - Test Loss: 0.22436996\n",
      "lr of epoch 247 => [0.001]\n",
      "Epoch 248/500 - Train Loss: 0.20987451\n",
      "Epoch 248/500 - Test Loss: 0.22447036\n",
      "lr of epoch 248 => [0.001]\n",
      "Epoch 249/500 - Train Loss: 0.20958493\n",
      "Epoch 249/500 - Test Loss: 0.22441569\n",
      "lr of epoch 249 => [0.001]\n",
      "Epoch 250/500 - Train Loss: 0.20939644\n",
      "Epoch 250/500 - Test Loss: 0.22417700\n",
      "lr of epoch 250 => [0.001]\n",
      "Epoch 251/500 - Train Loss: 0.20973665\n",
      "Epoch 251/500 - Test Loss: 0.22409186\n",
      "lr of epoch 251 => [0.001]\n",
      "Epoch 253/500 - Train Loss: 0.20934139\n",
      "Epoch 253/500 - Test Loss: 0.22496613\n",
      "lr of epoch 253 => [0.001]\n",
      "Epoch 254/500 - Train Loss: 0.20954131\n",
      "Epoch 254/500 - Test Loss: 0.22404414\n",
      "lr of epoch 254 => [0.001]\n",
      "Epoch 255/500 - Train Loss: 0.20928112\n",
      "Epoch 255/500 - Test Loss: 0.22441729\n",
      "lr of epoch 255 => [0.001]\n",
      "Epoch 256/500 - Train Loss: 0.20976341\n",
      "Epoch 256/500 - Test Loss: 0.22410745\n",
      "lr of epoch 256 => [0.001]\n",
      "Epoch 257/500 - Train Loss: 0.20933612\n",
      "Epoch 257/500 - Test Loss: 0.22354144\n",
      "lr of epoch 257 => [0.001]\n",
      "Epoch 258/500 - Train Loss: 0.20917170\n",
      "Epoch 258/500 - Test Loss: 0.22383668\n",
      "lr of epoch 258 => [0.001]\n",
      "Epoch 259/500 - Train Loss: 0.20911562\n",
      "Epoch 259/500 - Test Loss: 0.22453554\n",
      "lr of epoch 259 => [0.001]\n",
      "Epoch 260/500 - Train Loss: 0.20937028\n",
      "Epoch 260/500 - Test Loss: 0.22393052\n",
      "lr of epoch 260 => [0.001]\n",
      "Epoch 261/500 - Train Loss: 0.20897861\n",
      "Epoch 261/500 - Test Loss: 0.22380667\n",
      "lr of epoch 261 => [0.001]\n",
      "Epoch 262/500 - Train Loss: 0.20871532\n",
      "Epoch 262/500 - Test Loss: 0.22400913\n",
      "lr of epoch 262 => [0.001]\n",
      "Epoch 263/500 - Train Loss: 0.20889120\n",
      "Epoch 263/500 - Test Loss: 0.22420748\n",
      "lr of epoch 263 => [0.001]\n",
      "Epoch 264/500 - Train Loss: 0.20873983\n",
      "Epoch 264/500 - Test Loss: 0.22363509\n",
      "lr of epoch 264 => [0.001]\n",
      "Epoch 265/500 - Train Loss: 0.20857393\n",
      "Epoch 265/500 - Test Loss: 0.22333922\n",
      "lr of epoch 265 => [0.001]\n",
      "Epoch 266/500 - Train Loss: 0.20854844\n",
      "Epoch 266/500 - Test Loss: 0.22363145\n",
      "lr of epoch 266 => [0.001]\n",
      "Epoch 267/500 - Train Loss: 0.20842908\n",
      "Epoch 267/500 - Test Loss: 0.22350461\n",
      "lr of epoch 267 => [0.001]\n",
      "Epoch 268/500 - Train Loss: 0.20977719\n",
      "Epoch 268/500 - Test Loss: 0.22346927\n",
      "lr of epoch 268 => [0.001]\n",
      "Epoch 269/500 - Train Loss: 0.20868808\n",
      "Epoch 269/500 - Test Loss: 0.22384626\n",
      "lr of epoch 269 => [0.001]\n",
      "Epoch 270/500 - Train Loss: 0.20847593\n",
      "Epoch 270/500 - Test Loss: 0.22382365\n",
      "lr of epoch 270 => [0.001]\n",
      "Epoch 271/500 - Train Loss: 0.20880509\n",
      "Epoch 271/500 - Test Loss: 0.22417241\n",
      "lr of epoch 271 => [0.001]\n",
      "Epoch 272/500 - Train Loss: 0.20810793\n",
      "Epoch 272/500 - Test Loss: 0.22285627\n",
      "lr of epoch 272 => [0.001]\n",
      "Epoch 273/500 - Train Loss: 0.20796679\n",
      "Epoch 273/500 - Test Loss: 0.22310046\n",
      "lr of epoch 273 => [0.001]\n",
      "Epoch 274/500 - Train Loss: 0.20832877\n",
      "Epoch 274/500 - Test Loss: 0.22341360\n",
      "lr of epoch 274 => [0.001]\n",
      "Epoch 275/500 - Train Loss: 0.20834235\n",
      "Epoch 275/500 - Test Loss: 0.22403488\n",
      "lr of epoch 275 => [0.001]\n",
      "Epoch 276/500 - Train Loss: 0.20888560\n",
      "Epoch 276/500 - Test Loss: 0.22365613\n",
      "lr of epoch 276 => [0.001]\n",
      "Epoch 277/500 - Train Loss: 0.20840630\n",
      "Epoch 277/500 - Test Loss: 0.22336973\n",
      "lr of epoch 277 => [0.001]\n",
      "Epoch 278/500 - Train Loss: 0.20804231\n",
      "Epoch 278/500 - Test Loss: 0.22313493\n",
      "lr of epoch 278 => [0.001]\n",
      "Epoch 279/500 - Train Loss: 0.20772122\n",
      "Epoch 279/500 - Test Loss: 0.22303792\n",
      "lr of epoch 279 => [0.001]\n",
      "Epoch 280/500 - Train Loss: 0.20779291\n",
      "Epoch 280/500 - Test Loss: 0.22291524\n",
      "lr of epoch 280 => [0.001]\n",
      "Epoch 281/500 - Train Loss: 0.20772191\n",
      "Epoch 281/500 - Test Loss: 0.22345904\n",
      "lr of epoch 281 => [0.001]\n",
      "Epoch 282/500 - Train Loss: 0.20785249\n",
      "Epoch 282/500 - Test Loss: 0.22350669\n",
      "lr of epoch 282 => [0.001]\n",
      "Epoch 283/500 - Train Loss: 0.20841721\n",
      "Epoch 283/500 - Test Loss: 0.22329392\n",
      "lr of epoch 283 => [0.001]\n",
      "Epoch 284/500 - Train Loss: 0.20788301\n",
      "Epoch 284/500 - Test Loss: 0.22318075\n",
      "lr of epoch 284 => [0.001]\n",
      "Epoch 285/500 - Train Loss: 0.20889293\n",
      "Epoch 285/500 - Test Loss: 0.22515280\n",
      "lr of epoch 285 => [0.001]\n",
      "Epoch 286/500 - Train Loss: 0.20793874\n",
      "Epoch 286/500 - Test Loss: 0.22311794\n",
      "lr of epoch 286 => [0.001]\n",
      "Epoch 287/500 - Train Loss: 0.20858763\n",
      "Epoch 287/500 - Test Loss: 0.22380553\n",
      "lr of epoch 287 => [0.001]\n",
      "Epoch 288/500 - Train Loss: 0.20776005\n",
      "Epoch 288/500 - Test Loss: 0.22285640\n",
      "lr of epoch 288 => [0.001]\n",
      "Epoch 289/500 - Train Loss: 0.20765933\n",
      "Epoch 289/500 - Test Loss: 0.22295202\n",
      "lr of epoch 289 => [0.001]\n",
      "Epoch 290/500 - Train Loss: 0.20696403\n",
      "Epoch 290/500 - Test Loss: 0.22236742\n",
      "lr of epoch 290 => [0.001]\n",
      "Epoch 291/500 - Train Loss: 0.20695171\n",
      "Epoch 291/500 - Test Loss: 0.22263176\n",
      "lr of epoch 291 => [0.001]\n",
      "Epoch 292/500 - Train Loss: 0.20757865\n",
      "Epoch 292/500 - Test Loss: 0.22435186\n",
      "lr of epoch 292 => [0.001]\n",
      "Epoch 293/500 - Train Loss: 0.20757308\n",
      "Epoch 293/500 - Test Loss: 0.22292664\n",
      "lr of epoch 293 => [0.001]\n",
      "Epoch 294/500 - Train Loss: 0.20737827\n",
      "Epoch 294/500 - Test Loss: 0.22307449\n",
      "lr of epoch 294 => [0.001]\n",
      "Epoch 295/500 - Train Loss: 0.20695496\n",
      "Epoch 295/500 - Test Loss: 0.22279147\n",
      "lr of epoch 295 => [0.001]\n",
      "Epoch 296/500 - Train Loss: 0.20729572\n",
      "Epoch 296/500 - Test Loss: 0.22260072\n",
      "lr of epoch 296 => [0.001]\n",
      "Epoch 297/500 - Train Loss: 0.20728805\n",
      "Epoch 297/500 - Test Loss: 0.22256262\n",
      "lr of epoch 297 => [0.001]\n",
      "Epoch 298/500 - Train Loss: 0.20721826\n",
      "Epoch 298/500 - Test Loss: 0.22306832\n",
      "lr of epoch 298 => [0.001]\n",
      "Epoch 299/500 - Train Loss: 0.20755922\n",
      "Epoch 299/500 - Test Loss: 0.22359581\n",
      "lr of epoch 299 => [0.001]\n",
      "Epoch 300/500 - Train Loss: 0.20748141\n",
      "Epoch 300/500 - Test Loss: 0.22286013\n",
      "lr of epoch 300 => [0.001]\n",
      "Epoch 301/500 - Train Loss: 0.20703000\n",
      "Epoch 301/500 - Test Loss: 0.22306014\n",
      "lr of epoch 301 => [0.001]\n",
      "Epoch 302/500 - Train Loss: 0.20708679\n",
      "Epoch 302/500 - Test Loss: 0.22328628\n",
      "lr of epoch 302 => [0.001]\n",
      "Epoch 303/500 - Train Loss: 0.20693555\n",
      "Epoch 303/500 - Test Loss: 0.22257601\n",
      "lr of epoch 303 => [0.001]\n",
      "Epoch 304/500 - Train Loss: 0.20688198\n",
      "Epoch 304/500 - Test Loss: 0.22248735\n",
      "lr of epoch 304 => [0.001]\n",
      "Epoch 305/500 - Train Loss: 0.20762319\n",
      "Epoch 305/500 - Test Loss: 0.22337505\n",
      "lr of epoch 305 => [0.001]\n",
      "Epoch 306/500 - Train Loss: 0.20705783\n",
      "Epoch 306/500 - Test Loss: 0.22250863\n",
      "lr of epoch 306 => [0.001]\n",
      "Epoch 307/500 - Train Loss: 0.20710833\n",
      "Epoch 307/500 - Test Loss: 0.22381850\n",
      "lr of epoch 307 => [0.001]\n",
      "Epoch 308/500 - Train Loss: 0.20722411\n",
      "Epoch 308/500 - Test Loss: 0.22322707\n",
      "lr of epoch 308 => [0.001]\n",
      "Epoch 309/500 - Train Loss: 0.20665643\n",
      "Epoch 309/500 - Test Loss: 0.22230788\n",
      "lr of epoch 309 => [0.001]\n",
      "Epoch 310/500 - Train Loss: 0.20639778\n",
      "Epoch 310/500 - Test Loss: 0.22331441\n",
      "lr of epoch 310 => [0.001]\n",
      "Epoch 311/500 - Train Loss: 0.20668609\n",
      "Epoch 311/500 - Test Loss: 0.22270910\n",
      "lr of epoch 311 => [0.001]\n",
      "Epoch 312/500 - Train Loss: 0.20669191\n",
      "Epoch 312/500 - Test Loss: 0.22251189\n",
      "lr of epoch 312 => [0.001]\n",
      "Epoch 313/500 - Train Loss: 0.20667854\n",
      "Epoch 313/500 - Test Loss: 0.22244662\n",
      "lr of epoch 313 => [0.001]\n",
      "Epoch 314/500 - Train Loss: 0.20732783\n",
      "Epoch 314/500 - Test Loss: 0.22343715\n",
      "lr of epoch 314 => [0.001]\n",
      "Epoch 315/500 - Train Loss: 0.20656902\n",
      "Epoch 315/500 - Test Loss: 0.22212315\n",
      "lr of epoch 315 => [0.001]\n",
      "Epoch 316/500 - Train Loss: 0.20629060\n",
      "Epoch 316/500 - Test Loss: 0.22199386\n",
      "lr of epoch 316 => [0.001]\n",
      "Epoch 317/500 - Train Loss: 0.20644016\n",
      "Epoch 317/500 - Test Loss: 0.22238982\n",
      "lr of epoch 317 => [0.001]\n",
      "Epoch 318/500 - Train Loss: 0.20623167\n",
      "Epoch 318/500 - Test Loss: 0.22225955\n",
      "lr of epoch 318 => [0.001]\n",
      "Epoch 319/500 - Train Loss: 0.20597022\n",
      "Epoch 319/500 - Test Loss: 0.22174886\n",
      "lr of epoch 319 => [0.001]\n",
      "Epoch 320/500 - Train Loss: 0.20634420\n",
      "Epoch 320/500 - Test Loss: 0.22248758\n",
      "lr of epoch 320 => [0.001]\n",
      "Epoch 321/500 - Train Loss: 0.20619968\n",
      "Epoch 321/500 - Test Loss: 0.22297833\n",
      "lr of epoch 321 => [0.001]\n",
      "Epoch 322/500 - Train Loss: 0.20616817\n",
      "Epoch 322/500 - Test Loss: 0.22198349\n",
      "lr of epoch 322 => [0.001]\n",
      "Epoch 323/500 - Train Loss: 0.20567697\n",
      "Epoch 323/500 - Test Loss: 0.22195761\n",
      "lr of epoch 323 => [0.001]\n",
      "Epoch 324/500 - Train Loss: 0.20581861\n",
      "Epoch 324/500 - Test Loss: 0.22189944\n",
      "lr of epoch 324 => [0.001]\n",
      "Epoch 325/500 - Train Loss: 0.20622903\n",
      "Epoch 325/500 - Test Loss: 0.22172642\n",
      "lr of epoch 325 => [0.001]\n",
      "Epoch 326/500 - Train Loss: 0.20579994\n",
      "Epoch 326/500 - Test Loss: 0.22243047\n",
      "lr of epoch 326 => [0.001]\n",
      "Epoch 327/500 - Train Loss: 0.20556903\n",
      "Epoch 327/500 - Test Loss: 0.22204731\n",
      "lr of epoch 327 => [0.001]\n",
      "Epoch 328/500 - Train Loss: 0.20563218\n",
      "Epoch 328/500 - Test Loss: 0.22198412\n",
      "lr of epoch 328 => [0.001]\n",
      "Epoch 329/500 - Train Loss: 0.20553650\n",
      "Epoch 329/500 - Test Loss: 0.22237258\n",
      "lr of epoch 329 => [0.001]\n",
      "Epoch 330/500 - Train Loss: 0.20621242\n",
      "Epoch 330/500 - Test Loss: 0.22243768\n",
      "lr of epoch 330 => [0.001]\n",
      "Epoch 331/500 - Train Loss: 0.20640353\n",
      "Epoch 331/500 - Test Loss: 0.22319587\n",
      "lr of epoch 331 => [0.001]\n",
      "Epoch 332/500 - Train Loss: 0.20802349\n",
      "Epoch 332/500 - Test Loss: 0.22249898\n",
      "lr of epoch 332 => [0.001]\n",
      "Epoch 333/500 - Train Loss: 0.20614745\n",
      "Epoch 333/500 - Test Loss: 0.22167686\n",
      "lr of epoch 333 => [0.001]\n",
      "Epoch 334/500 - Train Loss: 0.20552143\n",
      "Epoch 334/500 - Test Loss: 0.22252422\n",
      "lr of epoch 334 => [0.001]\n",
      "Epoch 335/500 - Train Loss: 0.20572956\n",
      "Epoch 335/500 - Test Loss: 0.22180959\n",
      "lr of epoch 335 => [0.001]\n",
      "Epoch 336/500 - Train Loss: 0.20524480\n",
      "Epoch 336/500 - Test Loss: 0.22192297\n",
      "lr of epoch 336 => [0.001]\n",
      "Epoch 337/500 - Train Loss: 0.20554653\n",
      "Epoch 337/500 - Test Loss: 0.22222265\n",
      "lr of epoch 337 => [0.001]\n",
      "Epoch 338/500 - Train Loss: 0.20540256\n",
      "Epoch 338/500 - Test Loss: 0.22250272\n",
      "lr of epoch 338 => [0.001]\n",
      "Epoch 339/500 - Train Loss: 0.20548111\n",
      "Epoch 339/500 - Test Loss: 0.22145514\n",
      "lr of epoch 339 => [0.001]\n",
      "Epoch 340/500 - Train Loss: 0.20526815\n",
      "Epoch 340/500 - Test Loss: 0.22158143\n",
      "lr of epoch 340 => [0.001]\n",
      "Epoch 341/500 - Train Loss: 0.20529820\n",
      "Epoch 341/500 - Test Loss: 0.22189447\n",
      "lr of epoch 341 => [0.001]\n",
      "Epoch 342/500 - Train Loss: 0.20532569\n",
      "Epoch 342/500 - Test Loss: 0.22205500\n",
      "lr of epoch 342 => [0.001]\n",
      "Epoch 343/500 - Train Loss: 0.20625175\n",
      "Epoch 343/500 - Test Loss: 0.22251268\n",
      "lr of epoch 343 => [0.001]\n",
      "Epoch 344/500 - Train Loss: 0.20591000\n",
      "Epoch 344/500 - Test Loss: 0.22223595\n",
      "lr of epoch 344 => [0.001]\n",
      "Epoch 345/500 - Train Loss: 0.20568720\n",
      "Epoch 345/500 - Test Loss: 0.22197917\n",
      "lr of epoch 345 => [0.001]\n",
      "Epoch 346/500 - Train Loss: 0.20535818\n",
      "Epoch 346/500 - Test Loss: 0.22185998\n",
      "lr of epoch 346 => [0.001]\n",
      "Epoch 347/500 - Train Loss: 0.20519302\n",
      "Epoch 347/500 - Test Loss: 0.22156746\n",
      "lr of epoch 347 => [0.001]\n",
      "Epoch 348/500 - Train Loss: 0.20508539\n",
      "Epoch 348/500 - Test Loss: 0.22154417\n",
      "lr of epoch 348 => [0.001]\n",
      "Epoch 349/500 - Train Loss: 0.20498401\n",
      "Epoch 349/500 - Test Loss: 0.22184581\n",
      "lr of epoch 349 => [0.001]\n",
      "Epoch 350/500 - Train Loss: 0.20583595\n",
      "Epoch 350/500 - Test Loss: 0.22202274\n",
      "lr of epoch 350 => [0.001]\n",
      "Epoch 351/500 - Train Loss: 0.20614126\n",
      "Epoch 351/500 - Test Loss: 0.22145907\n",
      "lr of epoch 351 => [0.001]\n",
      "Epoch 352/500 - Train Loss: 0.20523287\n",
      "Epoch 352/500 - Test Loss: 0.22198136\n",
      "lr of epoch 352 => [0.001]\n",
      "Epoch 353/500 - Train Loss: 0.20509560\n",
      "Epoch 353/500 - Test Loss: 0.22155091\n",
      "lr of epoch 353 => [0.001]\n",
      "Epoch 354/500 - Train Loss: 0.20504832\n",
      "Epoch 354/500 - Test Loss: 0.22157345\n",
      "lr of epoch 354 => [0.001]\n",
      "Epoch 355/500 - Train Loss: 0.20560577\n",
      "Epoch 355/500 - Test Loss: 0.22312261\n",
      "lr of epoch 355 => [0.001]\n",
      "Epoch 356/500 - Train Loss: 0.20558994\n",
      "Epoch 356/500 - Test Loss: 0.22144116\n",
      "lr of epoch 356 => [0.001]\n",
      "Epoch 357/500 - Train Loss: 0.20534102\n",
      "Epoch 357/500 - Test Loss: 0.22142484\n",
      "lr of epoch 357 => [0.001]\n",
      "Epoch 358/500 - Train Loss: 0.20534253\n",
      "Epoch 358/500 - Test Loss: 0.22120280\n",
      "lr of epoch 358 => [0.001]\n",
      "Epoch 359/500 - Train Loss: 0.20498100\n",
      "Epoch 359/500 - Test Loss: 0.22222012\n",
      "lr of epoch 359 => [0.001]\n",
      "Epoch 360/500 - Train Loss: 0.20507916\n",
      "Epoch 360/500 - Test Loss: 0.22142415\n",
      "lr of epoch 360 => [0.001]\n",
      "Epoch 361/500 - Train Loss: 0.20484164\n",
      "Epoch 361/500 - Test Loss: 0.22157856\n",
      "lr of epoch 361 => [0.001]\n",
      "Epoch 362/500 - Train Loss: 0.20435208\n",
      "Epoch 362/500 - Test Loss: 0.22161385\n",
      "lr of epoch 362 => [0.001]\n",
      "Epoch 363/500 - Train Loss: 0.20451096\n",
      "Epoch 363/500 - Test Loss: 0.22097980\n",
      "lr of epoch 363 => [0.001]\n",
      "Epoch 364/500 - Train Loss: 0.20582786\n",
      "Epoch 364/500 - Test Loss: 0.22811501\n",
      "lr of epoch 364 => [0.001]\n",
      "Epoch 365/500 - Train Loss: 0.20662326\n",
      "Epoch 365/500 - Test Loss: 0.22131476\n",
      "lr of epoch 365 => [0.001]\n",
      "Epoch 366/500 - Train Loss: 0.20440900\n",
      "Epoch 366/500 - Test Loss: 0.22140456\n",
      "lr of epoch 366 => [0.001]\n",
      "Epoch 367/500 - Train Loss: 0.20427056\n",
      "Epoch 367/500 - Test Loss: 0.22106428\n",
      "lr of epoch 367 => [0.001]\n",
      "Epoch 368/500 - Train Loss: 0.20427703\n",
      "Epoch 368/500 - Test Loss: 0.22153081\n",
      "lr of epoch 368 => [0.001]\n",
      "Epoch 369/500 - Train Loss: 0.20429377\n",
      "Epoch 369/500 - Test Loss: 0.22158549\n",
      "lr of epoch 369 => [0.001]\n",
      "Epoch 370/500 - Train Loss: 0.20424352\n",
      "Epoch 370/500 - Test Loss: 0.22162485\n",
      "lr of epoch 370 => [0.001]\n",
      "Epoch 371/500 - Train Loss: 0.20427417\n",
      "Epoch 371/500 - Test Loss: 0.22125701\n",
      "lr of epoch 371 => [0.001]\n",
      "Epoch 372/500 - Train Loss: 0.20437448\n",
      "Epoch 372/500 - Test Loss: 0.22152895\n",
      "lr of epoch 372 => [0.001]\n",
      "Epoch 373/500 - Train Loss: 0.20402130\n",
      "Epoch 373/500 - Test Loss: 0.22086393\n",
      "lr of epoch 373 => [0.001]\n",
      "Epoch 374/500 - Train Loss: 0.20428145\n",
      "Epoch 374/500 - Test Loss: 0.22163570\n",
      "lr of epoch 374 => [0.001]\n",
      "Epoch 375/500 - Train Loss: 0.20433533\n",
      "Epoch 375/500 - Test Loss: 0.22099099\n",
      "lr of epoch 375 => [0.001]\n",
      "Epoch 376/500 - Train Loss: 0.20417363\n",
      "Epoch 376/500 - Test Loss: 0.22166603\n",
      "lr of epoch 376 => [0.001]\n",
      "Epoch 377/500 - Train Loss: 0.20494463\n",
      "Epoch 377/500 - Test Loss: 0.22146680\n",
      "lr of epoch 377 => [0.001]\n",
      "Epoch 378/500 - Train Loss: 0.20475960\n",
      "Epoch 378/500 - Test Loss: 0.22168264\n",
      "lr of epoch 378 => [0.001]\n",
      "Epoch 379/500 - Train Loss: 0.20434265\n",
      "Epoch 379/500 - Test Loss: 0.22103862\n",
      "lr of epoch 379 => [0.001]\n",
      "Epoch 380/500 - Train Loss: 0.20423034\n",
      "Epoch 380/500 - Test Loss: 0.22115249\n",
      "lr of epoch 380 => [0.001]\n",
      "Epoch 381/500 - Train Loss: 0.20384749\n",
      "Epoch 381/500 - Test Loss: 0.22094253\n",
      "lr of epoch 381 => [0.001]\n",
      "Epoch 382/500 - Train Loss: 0.20414807\n",
      "Epoch 382/500 - Test Loss: 0.22109491\n",
      "lr of epoch 382 => [0.001]\n",
      "Epoch 383/500 - Train Loss: 0.20393243\n",
      "Epoch 383/500 - Test Loss: 0.22120208\n",
      "lr of epoch 383 => [0.001]\n",
      "Epoch 384/500 - Train Loss: 0.20387554\n",
      "Epoch 384/500 - Test Loss: 0.22089660\n",
      "lr of epoch 384 => [0.001]\n",
      "Epoch 385/500 - Train Loss: 0.20381048\n",
      "Epoch 385/500 - Test Loss: 0.22094546\n",
      "lr of epoch 385 => [0.001]\n",
      "Epoch 386/500 - Train Loss: 0.20439266\n",
      "Epoch 386/500 - Test Loss: 0.22200228\n",
      "lr of epoch 386 => [0.001]\n",
      "Epoch 387/500 - Train Loss: 0.20459794\n",
      "Epoch 387/500 - Test Loss: 0.22092237\n",
      "lr of epoch 387 => [0.001]\n",
      "Epoch 388/500 - Train Loss: 0.20361011\n",
      "Epoch 388/500 - Test Loss: 0.22082108\n",
      "lr of epoch 388 => [0.001]\n",
      "Epoch 389/500 - Train Loss: 0.20362866\n",
      "Epoch 389/500 - Test Loss: 0.22065143\n",
      "lr of epoch 389 => [0.001]\n",
      "Epoch 390/500 - Train Loss: 0.20386747\n",
      "Epoch 390/500 - Test Loss: 0.22157537\n",
      "lr of epoch 390 => [0.001]\n",
      "Epoch 391/500 - Train Loss: 0.20400272\n",
      "Epoch 391/500 - Test Loss: 0.22095305\n",
      "lr of epoch 391 => [0.001]\n",
      "Epoch 392/500 - Train Loss: 0.20367806\n",
      "Epoch 392/500 - Test Loss: 0.22089340\n",
      "lr of epoch 392 => [0.001]\n",
      "Epoch 393/500 - Train Loss: 0.20371989\n",
      "Epoch 393/500 - Test Loss: 0.22120837\n",
      "lr of epoch 393 => [0.001]\n",
      "Epoch 394/500 - Train Loss: 0.20392652\n",
      "Epoch 394/500 - Test Loss: 0.22071007\n",
      "lr of epoch 394 => [0.001]\n",
      "Epoch 395/500 - Train Loss: 0.20375176\n",
      "Epoch 395/500 - Test Loss: 0.22096362\n",
      "lr of epoch 395 => [0.001]\n",
      "Epoch 396/500 - Train Loss: 0.20359170\n",
      "Epoch 396/500 - Test Loss: 0.22060489\n",
      "lr of epoch 396 => [0.001]\n",
      "Epoch 397/500 - Train Loss: 0.20364882\n",
      "Epoch 397/500 - Test Loss: 0.22151181\n",
      "lr of epoch 397 => [0.001]\n",
      "Epoch 398/500 - Train Loss: 0.20423607\n",
      "Epoch 398/500 - Test Loss: 0.22106954\n",
      "lr of epoch 398 => [0.001]\n",
      "Epoch 399/500 - Train Loss: 0.20338747\n",
      "Epoch 399/500 - Test Loss: 0.22064450\n",
      "lr of epoch 399 => [0.001]\n",
      "Epoch 400/500 - Train Loss: 0.20355178\n",
      "Epoch 400/500 - Test Loss: 0.22113370\n",
      "lr of epoch 400 => [0.001]\n",
      "Epoch 401/500 - Train Loss: 0.20353039\n",
      "Epoch 401/500 - Test Loss: 0.22119068\n",
      "lr of epoch 401 => [0.001]\n",
      "Epoch 402/500 - Train Loss: 0.20338931\n",
      "Epoch 402/500 - Test Loss: 0.22144910\n",
      "lr of epoch 402 => [0.001]\n",
      "Epoch 403/500 - Train Loss: 0.20390241\n",
      "Epoch 403/500 - Test Loss: 0.22139110\n",
      "lr of epoch 403 => [0.001]\n",
      "Epoch 404/500 - Train Loss: 0.20357298\n",
      "Epoch 404/500 - Test Loss: 0.22069973\n",
      "lr of epoch 404 => [0.001]\n",
      "Epoch 405/500 - Train Loss: 0.20336267\n",
      "Epoch 405/500 - Test Loss: 0.22048488\n",
      "lr of epoch 405 => [0.001]\n",
      "Epoch 406/500 - Train Loss: 0.20346219\n",
      "Epoch 406/500 - Test Loss: 0.22115755\n",
      "lr of epoch 406 => [0.001]\n",
      "Epoch 407/500 - Train Loss: 0.20333651\n",
      "Epoch 407/500 - Test Loss: 0.22174486\n",
      "lr of epoch 407 => [0.001]\n",
      "Epoch 408/500 - Train Loss: 0.20363933\n",
      "Epoch 408/500 - Test Loss: 0.22084896\n",
      "lr of epoch 408 => [0.001]\n",
      "Epoch 409/500 - Train Loss: 0.20326230\n",
      "Epoch 409/500 - Test Loss: 0.22092829\n",
      "lr of epoch 409 => [0.001]\n",
      "Epoch 410/500 - Train Loss: 0.20331240\n",
      "Epoch 410/500 - Test Loss: 0.22079439\n",
      "lr of epoch 410 => [0.001]\n",
      "Epoch 411/500 - Train Loss: 0.20207149\n",
      "Epoch 411/500 - Test Loss: 0.21942972\n",
      "lr of epoch 411 => [0.0008]\n",
      "Epoch 412/500 - Train Loss: 0.20100985\n",
      "Epoch 412/500 - Test Loss: 0.21912835\n",
      "lr of epoch 412 => [0.0006400000000000002]\n",
      "Epoch 413/500 - Train Loss: 0.20034108\n",
      "Epoch 413/500 - Test Loss: 0.21889263\n",
      "lr of epoch 413 => [0.0005120000000000001]\n",
      "Epoch 414/500 - Train Loss: 0.19984888\n",
      "Epoch 414/500 - Test Loss: 0.21870185\n",
      "lr of epoch 414 => [0.0004096000000000001]\n",
      "Epoch 415/500 - Train Loss: 0.19948806\n",
      "Epoch 415/500 - Test Loss: 0.21862315\n",
      "lr of epoch 415 => [0.0003276800000000001]\n",
      "Epoch 416/500 - Train Loss: 0.19934953\n",
      "Epoch 416/500 - Test Loss: 0.21848826\n",
      "lr of epoch 416 => [0.0002621440000000001]\n",
      "Epoch 417/500 - Train Loss: 0.19921741\n",
      "Epoch 417/500 - Test Loss: 0.21847717\n",
      "lr of epoch 417 => [0.0002097152000000001]\n",
      "Epoch 418/500 - Train Loss: 0.19905467\n",
      "Epoch 418/500 - Test Loss: 0.21840774\n",
      "lr of epoch 418 => [0.0001677721600000001]\n",
      "Epoch 419/500 - Train Loss: 0.19882841\n",
      "Epoch 419/500 - Test Loss: 0.21838510\n",
      "lr of epoch 419 => [0.00013421772800000008]\n",
      "Epoch 420/500 - Train Loss: 0.19870399\n",
      "Epoch 420/500 - Test Loss: 0.21838742\n",
      "lr of epoch 420 => [0.00010737418240000006]\n",
      "Epoch 421/500 - Train Loss: 0.19866956\n",
      "Epoch 421/500 - Test Loss: 0.21836874\n",
      "lr of epoch 421 => [8.589934592000005e-05]\n",
      "Epoch 422/500 - Train Loss: 0.19862747\n",
      "Epoch 422/500 - Test Loss: 0.21831935\n",
      "lr of epoch 422 => [6.871947673600005e-05]\n",
      "Epoch 423/500 - Train Loss: 0.19860335\n",
      "Epoch 423/500 - Test Loss: 0.21832766\n",
      "lr of epoch 423 => [5.497558138880004e-05]\n",
      "Epoch 424/500 - Train Loss: 0.19858920\n",
      "Epoch 424/500 - Test Loss: 0.21830884\n",
      "lr of epoch 424 => [4.398046511104004e-05]\n",
      "Epoch 425/500 - Train Loss: 0.19847143\n",
      "Epoch 425/500 - Test Loss: 0.21831348\n",
      "lr of epoch 425 => [3.518437208883203e-05]\n",
      "Epoch 426/500 - Train Loss: 0.19844533\n",
      "Epoch 426/500 - Test Loss: 0.21830860\n",
      "lr of epoch 426 => [2.8147497671065623e-05]\n",
      "Epoch 427/500 - Train Loss: 0.19848038\n",
      "Epoch 427/500 - Test Loss: 0.21830453\n",
      "lr of epoch 427 => [2.2517998136852502e-05]\n",
      "Epoch 428/500 - Train Loss: 0.19834139\n",
      "Epoch 428/500 - Test Loss: 0.21831212\n",
      "lr of epoch 428 => [1.8014398509482003e-05]\n",
      "Epoch 429/500 - Train Loss: 0.19838840\n",
      "Epoch 429/500 - Test Loss: 0.21829880\n",
      "lr of epoch 429 => [1.4411518807585603e-05]\n",
      "Epoch 430/500 - Train Loss: 0.19844004\n",
      "Epoch 430/500 - Test Loss: 0.21830341\n",
      "lr of epoch 430 => [1.1529215046068483e-05]\n",
      "Epoch 431/500 - Train Loss: 0.19841871\n",
      "Epoch 431/500 - Test Loss: 0.21830272\n",
      "lr of epoch 431 => [9.223372036854787e-06]\n",
      "Epoch 432/500 - Train Loss: 0.19833483\n",
      "Epoch 432/500 - Test Loss: 0.21830026\n",
      "lr of epoch 432 => [7.37869762948383e-06]\n",
      "Epoch 433/500 - Train Loss: 0.19833495\n",
      "Epoch 433/500 - Test Loss: 0.21830568\n",
      "lr of epoch 433 => [5.902958103587064e-06]\n",
      "Epoch 434/500 - Train Loss: 0.19830051\n",
      "Epoch 434/500 - Test Loss: 0.21830338\n",
      "lr of epoch 434 => [4.722366482869652e-06]\n",
      "Epoch 435/500 - Train Loss: 0.19837183\n",
      "Epoch 435/500 - Test Loss: 0.21830088\n",
      "lr of epoch 435 => [3.7778931862957216e-06]\n",
      "Epoch 436/500 - Train Loss: 0.19839014\n",
      "Epoch 436/500 - Test Loss: 0.21830588\n",
      "lr of epoch 436 => [3.0223145490365774e-06]\n",
      "Epoch 437/500 - Train Loss: 0.19842178\n",
      "Epoch 437/500 - Test Loss: 0.21830515\n",
      "lr of epoch 437 => [2.417851639229262e-06]\n",
      "Epoch 438/500 - Train Loss: 0.19834768\n",
      "Epoch 438/500 - Test Loss: 0.21830363\n",
      "lr of epoch 438 => [1.93428131138341e-06]\n",
      "Epoch 439/500 - Train Loss: 0.19838761\n",
      "Epoch 439/500 - Test Loss: 0.21830761\n",
      "lr of epoch 439 => [1.547425049106728e-06]\n",
      "Epoch 440/500 - Train Loss: 0.19834140\n",
      "Epoch 440/500 - Test Loss: 0.21830615\n",
      "lr of epoch 440 => [1.2379400392853823e-06]\n",
      "Epoch 441/500 - Train Loss: 0.19835188\n",
      "Epoch 441/500 - Test Loss: 0.21830558\n",
      "lr of epoch 441 => [9.903520314283058e-07]\n",
      "Epoch 442/500 - Train Loss: 0.19834768\n",
      "Epoch 442/500 - Test Loss: 0.21830708\n",
      "lr of epoch 442 => [7.922816251426449e-07]\n",
      "Epoch 443/500 - Train Loss: 0.19840178\n",
      "Epoch 443/500 - Test Loss: 0.21830738\n",
      "lr of epoch 443 => [6.338253001141158e-07]\n",
      "Epoch 444/500 - Train Loss: 0.19830735\n",
      "Epoch 444/500 - Test Loss: 0.21830753\n",
      "lr of epoch 444 => [5.070602400912927e-07]\n",
      "Epoch 445/500 - Train Loss: 0.19839889\n",
      "Epoch 445/500 - Test Loss: 0.21830706\n",
      "lr of epoch 445 => [4.056481920730342e-07]\n",
      "Epoch 446/500 - Train Loss: 0.19837070\n",
      "Epoch 446/500 - Test Loss: 0.21830724\n",
      "lr of epoch 446 => [3.2451855365842734e-07]\n",
      "Epoch 447/500 - Train Loss: 0.19838662\n",
      "Epoch 447/500 - Test Loss: 0.21830746\n",
      "lr of epoch 447 => [2.5961484292674195e-07]\n",
      "Epoch 448/500 - Train Loss: 0.19830964\n",
      "Epoch 448/500 - Test Loss: 0.21830733\n",
      "lr of epoch 448 => [2.0769187434139353e-07]\n",
      "Epoch 449/500 - Train Loss: 0.19827896\n",
      "Epoch 449/500 - Test Loss: 0.21830713\n",
      "lr of epoch 449 => [1.6615349947311486e-07]\n",
      "Epoch 450/500 - Train Loss: 0.19830354\n",
      "Epoch 450/500 - Test Loss: 0.21830732\n",
      "lr of epoch 450 => [1.329227995784919e-07]\n",
      "Epoch 451/500 - Train Loss: 0.19833666\n",
      "Epoch 451/500 - Test Loss: 0.21830734\n",
      "lr of epoch 451 => [1.0633823966279352e-07]\n",
      "Epoch 452/500 - Train Loss: 0.19838860\n",
      "Epoch 452/500 - Test Loss: 0.21830746\n",
      "lr of epoch 452 => [8.507059173023481e-08]\n",
      "Epoch 453/500 - Train Loss: 0.19843896\n",
      "Epoch 453/500 - Test Loss: 0.21830737\n",
      "lr of epoch 453 => [6.805647338418787e-08]\n",
      "Epoch 454/500 - Train Loss: 0.19830228\n",
      "Epoch 454/500 - Test Loss: 0.21830753\n",
      "lr of epoch 454 => [5.4445178707350285e-08]\n",
      "Early stopping after 25 epochs of no improvement.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':#被別人呼叫的時候不會執行main以下的程式，只會執行\n",
    "\n",
    "    input_dim = data_mut_tcga.shape[1]# (8238sample[0], 2649gene[1])\n",
    "    mut_encode_dim =[128,32]\n",
    "    batch_size = 64\n",
    "    epoch_size = 500 #100\n",
    "    activation_function = nn.ReLU()\n",
    "    model_save_name = \"tcga_exp_%d_%d\" % (mut_encode_dim[0], mut_encode_dim[1])\n",
    "    learning_rate=0.001\n",
    "    warmup_iters = 410\n",
    "    seed=42\n",
    "    patience = 25\n",
    "    Decrease_percent = 0.8\n",
    "    continuous=True\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    train_size = int(0.8 * len(data_mut_tcga))\n",
    "    test_size = len(data_mut_tcga) - train_size\n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = random_split(data_mut_tcga.values, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders for training and testing\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    t = time.time()\n",
    "    torch.manual_seed(seed)\n",
    "    model = AE_dense_layers(input_dim=input_dim, mut_encode_dim=mut_encode_dim, activation_func=activation_function).to(device=device)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if warmup_iters is not None:\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters,Decrease_percent,continuous)\n",
    "        # scheduler = warmup_lr_scheduler(optimizer, warmup_iters1=5, Decrease_percent1=10, warmup_iters2=290, Decrease_percent2=0.8, continuous=False)\n",
    "\n",
    "    # Training with early stopping (assuming you've defined the EarlyStopping logic)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_weight=None\n",
    "    counter = 0\n",
    "    train_epoch_loss_list = []#  for train every epoch loss plot\n",
    "    test_epoch_loss_list=[]#  for validation every epoch loss plot\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    for epoch in range(epoch_size):\n",
    "        model.train()\n",
    "        model.requires_grad = True\n",
    "        total_train_loss = 0.0\n",
    "        for batch_idx,inputs in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs =inputs.float().to(device=device) # change torch.Tensor int64 to float32\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() # sum every batch's loss to compute average loss of every batch for one epoch\n",
    "            #print(f'Epoch {epoch + 1}/{epoch_size} - Batch {batch_idx+1}/{len(dataloader_mut_tcga)} - Loss: {loss.item():.4f}')#batch loss()\n",
    "        # Calculate and print the average loss of batch for the epoch\n",
    "        average_loss = total_train_loss / len(train_loader) # 一個 epoch 的 loss 是 batch 的 average loss\n",
    "        train_epoch_loss_list.append(average_loss)# for loss plot'\n",
    "        print(f'Epoch {epoch + 1}/{epoch_size} - Train Loss: {average_loss:.8f}')\n",
    "        \n",
    "        model.eval()\n",
    "        model.requires_grad = False\n",
    "        total_test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx,inputs in enumerate(test_loader):\n",
    "                inputs =inputs.float().to(device=device) # change torch.Tensor int64 to float32\n",
    "                outputs = model(inputs)\n",
    "                test_loss = criterion(outputs, inputs)\n",
    "                total_test_loss += test_loss.item() \n",
    "        test_average_loss = total_test_loss / len(test_loader)\n",
    "        test_epoch_loss_list.append(test_average_loss)# for loss plot'\n",
    "        print(f'Epoch {epoch + 1}/{epoch_size} - Test Loss: {test_average_loss:.8f}')\n",
    "\n",
    "        if warmup_iters:\n",
    "            print(\"lr of epoch\", epoch + 1, \"=>\", lr_scheduler.get_lr()) \n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        if test_average_loss < best_val_loss:\n",
    "            best_val_loss = test_average_loss\n",
    "            best_weight = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch+1\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping after {patience} epochs of no improvement.')\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b9a2367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64   activation_function: ReLU()   learning_rate: 0.001   warmup_iters: 410\n",
      "best Epoch :  432  , best_val_loss :  0.21840485834306286\n",
      "\n",
      "Autoencoder training completed in 14.3 mins.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('batch_size:',batch_size,\" \",'activation_function:',activation_function,\" \",\n",
    "      'learning_rate:',learning_rate,\" \",'warmup_iters:',warmup_iters)\n",
    "print(\"best Epoch : \",best_epoch,\" ,\" ,\"best_val_loss : \",best_val_loss)\n",
    "print('\\nAutoencoder training completed in %.1f mins.\\n' % ((time.time() - t) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95c56621",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_best_weight = {key: value for key, value in best_weight.items() if key.startswith('encoder')} # only store the encoder part without decoder part\n",
    "torch.save(encoder_best_weight, f'./results/Encoder_{model_save_name}_best_loss_{best_val_loss:.8}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8d226b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create a plot of the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_epoch_loss_list) + 1), train_epoch_loss_list, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "# Plot validation loss\n",
    "plt.plot(range(1, len(test_epoch_loss_list) + 1), test_epoch_loss_list, marker='o', linestyle='-', color='r', label='Validation Loss')\n",
    "# Mark the best epoch\n",
    "plt.plot(best_epoch, best_val_loss , marker='o', markersize=8, linestyle='', color='c', label=f'Best Epoch:{best_epoch}')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Pretain AE Loss Curve', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(top=0.25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac20b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67883d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
