{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ec8bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "            else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f0ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information\n",
    "filename     = \"ESPF_PretrainAE_exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f211e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "\n",
    "class AE_dense_3layers(nn.Module):\n",
    "    def __init__(self, input_dim, mut_encode_dim, activation_func):\n",
    "        super(AE_dense_3layers, self).__init__()##__init__初始化父類別的屬性和方法\n",
    "        print('input_dim = ', input_dim)\n",
    "        print('first_layer_dim = ', mut_encode_dim[0])\n",
    "        print('second_layer_dim = ', mut_encode_dim[1])\n",
    "        print('third_layer_dim = ', mut_encode_dim[2])\n",
    "        # nn.Module 自己將batch 裡的samples 拆開一個一個送入self.encoder nn.Sequential\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, mut_encode_dim[0]),\n",
    "            activation_func,\n",
    "            nn.Linear(mut_encode_dim[0], mut_encode_dim[1]),\n",
    "            activation_func,\n",
    "            nn.Linear(mut_encode_dim[1], mut_encode_dim[2]),\n",
    "            activation_func\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(mut_encode_dim[2], mut_encode_dim[1]),\n",
    "            activation_func,\n",
    "            nn.Linear(mut_encode_dim[1], mut_encode_dim[0]),\n",
    "            activation_func,\n",
    "            nn.Linear(mut_encode_dim[0], input_dim),\n",
    "            activation_func\n",
    "        )\n",
    "        # Initialize weights using He uniform initialization\n",
    "        for layer in [self.encoder[0], self.encoder[2], self.encoder[4], self.decoder[0], self.decoder[2], self.decoder[4]]:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):#加self才能呼叫self裡其他的屬性和method\n",
    "        x_encoded = self.encoder(x)\n",
    "        x_decoded = self.decoder(x_encoded)\n",
    "        return x_decoded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c7b35",
   "metadata": {},
   "source": [
    "## def warmup_lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c57499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_scheduler\n",
    "def warmup_lr_scheduler(optimizer, warmup_iters,Decrease_percent):\n",
    "    def f(epoch):\n",
    "        if epoch >= warmup_iters:\n",
    "          return Decrease_percent ** (epoch-warmup_iters+1)\n",
    "        return 1\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9704b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function to set the seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# Set the seed\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f77cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Datasets successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# load TCGA mutation data, substitute here with other genomics\n",
    "data_mut_tcga= pd.read_csv(\"../data/TCGA/TCGA_exp_matchCCLEgenes_8238samples_4692genes.txt\", sep='\\t', index_col=0)\n",
    "\n",
    "print(\"\\n\\nDatasets successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70928f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8238, 4692)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mut_tcga.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8afd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #exchange the row and column to match the format of doctor chiu's data (row is gene ;column is cancer)\n",
    "# data_mut_tcga=data_mut_tcga.transpose()\n",
    "# data_mut_tcga.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75196629",
   "metadata": {},
   "source": [
    "## 也許應該要分train validation set，這樣才能更學到真正重要的特徵，而不是背答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e452a702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim =  4692\n",
      "first_layer_dim =  1250\n",
      "second_layer_dim =  250\n",
      "third_layer_dim =  50\n",
      "Epoch 1/300 - Train Loss: 7.22055740\n",
      "Epoch 1/300 - Test Loss: 6.07800706\n",
      "lr of epoch 1 => [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300 - Train Loss: 6.10093148\n",
      "Epoch 2/300 - Test Loss: 6.07180021\n",
      "lr of epoch 2 => [0.001]\n",
      "Epoch 3/300 - Train Loss: 6.08846956\n",
      "Epoch 3/300 - Test Loss: 6.06979579\n",
      "lr of epoch 3 => [0.001]\n",
      "Epoch 4/300 - Train Loss: 6.04143737\n",
      "Epoch 4/300 - Test Loss: 6.20666852\n",
      "lr of epoch 4 => [0.001]\n",
      "Epoch 5/300 - Train Loss: 5.99578038\n",
      "Epoch 5/300 - Test Loss: 5.95218737\n",
      "lr of epoch 5 => [0.001]\n",
      "Epoch 6/300 - Train Loss: 5.99038140\n",
      "Epoch 6/300 - Test Loss: 5.97679443\n",
      "lr of epoch 6 => [0.001]\n",
      "Epoch 7/300 - Train Loss: 5.95332549\n",
      "Epoch 7/300 - Test Loss: 5.92964803\n",
      "lr of epoch 7 => [0.001]\n",
      "Epoch 8/300 - Train Loss: 5.93885731\n",
      "Epoch 8/300 - Test Loss: 5.89847942\n",
      "lr of epoch 8 => [0.001]\n",
      "Epoch 9/300 - Train Loss: 5.89978550\n",
      "Epoch 9/300 - Test Loss: 5.82800225\n",
      "lr of epoch 9 => [0.001]\n",
      "Epoch 10/300 - Train Loss: 5.83130629\n",
      "Epoch 10/300 - Test Loss: 5.78674421\n",
      "lr of epoch 10 => [0.001]\n",
      "Epoch 11/300 - Train Loss: 5.77675589\n",
      "Epoch 11/300 - Test Loss: 5.71021265\n",
      "lr of epoch 11 => [0.001]\n",
      "Epoch 12/300 - Train Loss: 5.72163342\n",
      "Epoch 12/300 - Test Loss: 5.67467691\n",
      "lr of epoch 12 => [0.001]\n",
      "Epoch 13/300 - Train Loss: 5.68800394\n",
      "Epoch 13/300 - Test Loss: 5.77197713\n",
      "lr of epoch 13 => [0.001]\n",
      "Epoch 14/300 - Train Loss: 5.55170357\n",
      "Epoch 14/300 - Test Loss: 5.50596771\n",
      "lr of epoch 14 => [0.001]\n",
      "Epoch 15/300 - Train Loss: 5.50723805\n",
      "Epoch 15/300 - Test Loss: 5.45988048\n",
      "lr of epoch 15 => [0.001]\n",
      "Epoch 16/300 - Train Loss: 5.44472351\n",
      "Epoch 16/300 - Test Loss: 5.29536704\n",
      "lr of epoch 16 => [0.001]\n",
      "Epoch 17/300 - Train Loss: 5.27070108\n",
      "Epoch 17/300 - Test Loss: 5.21861104\n",
      "lr of epoch 17 => [0.001]\n",
      "Epoch 18/300 - Train Loss: 5.22386369\n",
      "Epoch 18/300 - Test Loss: 5.21322093\n",
      "lr of epoch 18 => [0.001]\n",
      "Epoch 19/300 - Train Loss: 5.16283255\n",
      "Epoch 19/300 - Test Loss: 5.12059457\n",
      "lr of epoch 19 => [0.001]\n",
      "Epoch 20/300 - Train Loss: 5.01768038\n",
      "Epoch 20/300 - Test Loss: 4.84546630\n",
      "lr of epoch 20 => [0.001]\n",
      "Epoch 21/300 - Train Loss: 4.67087105\n",
      "Epoch 21/300 - Test Loss: 4.53230751\n",
      "lr of epoch 21 => [0.001]\n",
      "Epoch 22/300 - Train Loss: 4.45128656\n",
      "Epoch 22/300 - Test Loss: 4.32101563\n",
      "lr of epoch 22 => [0.001]\n",
      "Epoch 23/300 - Train Loss: 4.21360076\n",
      "Epoch 23/300 - Test Loss: 3.82877674\n",
      "lr of epoch 23 => [0.001]\n",
      "Epoch 24/300 - Train Loss: 3.88740554\n",
      "Epoch 24/300 - Test Loss: 3.90213640\n",
      "lr of epoch 24 => [0.001]\n",
      "Epoch 25/300 - Train Loss: 13.98536460\n",
      "Epoch 25/300 - Test Loss: 4.19164626\n",
      "lr of epoch 25 => [0.001]\n",
      "Epoch 26/300 - Train Loss: 3.33833385\n",
      "Epoch 26/300 - Test Loss: 3.29729715\n",
      "lr of epoch 26 => [0.001]\n",
      "Epoch 27/300 - Train Loss: 3.28690045\n",
      "Epoch 27/300 - Test Loss: 3.29038003\n",
      "lr of epoch 27 => [0.001]\n",
      "Epoch 28/300 - Train Loss: 3.28407292\n",
      "Epoch 28/300 - Test Loss: 3.29141084\n",
      "lr of epoch 28 => [0.001]\n",
      "Epoch 29/300 - Train Loss: 3.28379899\n",
      "Epoch 29/300 - Test Loss: 3.28848303\n",
      "lr of epoch 29 => [0.001]\n",
      "Epoch 30/300 - Train Loss: 3.28259378\n",
      "Epoch 30/300 - Test Loss: 3.28717321\n",
      "lr of epoch 30 => [0.001]\n",
      "Epoch 31/300 - Train Loss: 3.28196662\n",
      "Epoch 31/300 - Test Loss: 3.28898775\n",
      "lr of epoch 31 => [0.001]\n",
      "Epoch 32/300 - Train Loss: 3.28095721\n",
      "Epoch 32/300 - Test Loss: 3.28465718\n",
      "lr of epoch 32 => [0.001]\n",
      "Epoch 33/300 - Train Loss: 3.28123162\n",
      "Epoch 33/300 - Test Loss: 3.28841785\n",
      "lr of epoch 33 => [0.001]\n",
      "Epoch 34/300 - Train Loss: 3.28234481\n",
      "Epoch 34/300 - Test Loss: 3.28432300\n",
      "lr of epoch 34 => [0.001]\n",
      "Epoch 35/300 - Train Loss: 3.27870364\n",
      "Epoch 35/300 - Test Loss: 3.28387156\n",
      "lr of epoch 35 => [0.001]\n",
      "Epoch 36/300 - Train Loss: 3.27806220\n",
      "Epoch 36/300 - Test Loss: 3.28426090\n",
      "lr of epoch 36 => [0.001]\n",
      "Epoch 37/300 - Train Loss: 3.27491910\n",
      "Epoch 37/300 - Test Loss: 3.28052601\n",
      "lr of epoch 37 => [0.001]\n",
      "Epoch 38/300 - Train Loss: 3.27309042\n",
      "Epoch 38/300 - Test Loss: 3.27938482\n",
      "lr of epoch 38 => [0.001]\n",
      "Epoch 39/300 - Train Loss: 3.27180418\n",
      "Epoch 39/300 - Test Loss: 3.27728893\n",
      "lr of epoch 39 => [0.001]\n",
      "Epoch 40/300 - Train Loss: 3.26955225\n",
      "Epoch 40/300 - Test Loss: 3.27731014\n",
      "lr of epoch 40 => [0.001]\n",
      "Epoch 41/300 - Train Loss: 3.26923416\n",
      "Epoch 41/300 - Test Loss: 3.27610332\n",
      "lr of epoch 41 => [0.001]\n",
      "Epoch 42/300 - Train Loss: 3.26810160\n",
      "Epoch 42/300 - Test Loss: 3.27551720\n",
      "lr of epoch 42 => [0.001]\n",
      "Epoch 43/300 - Train Loss: 3.26670121\n",
      "Epoch 43/300 - Test Loss: 3.27982009\n",
      "lr of epoch 43 => [0.001]\n",
      "Epoch 44/300 - Train Loss: 3.26537935\n",
      "Epoch 44/300 - Test Loss: 3.27550581\n",
      "lr of epoch 44 => [0.001]\n",
      "Epoch 45/300 - Train Loss: 3.26322869\n",
      "Epoch 45/300 - Test Loss: 3.26760161\n",
      "lr of epoch 45 => [0.001]\n",
      "Epoch 46/300 - Train Loss: 3.26123063\n",
      "Epoch 46/300 - Test Loss: 3.26476781\n",
      "lr of epoch 46 => [0.001]\n",
      "Epoch 47/300 - Train Loss: 3.25813846\n",
      "Epoch 47/300 - Test Loss: 3.26620789\n",
      "lr of epoch 47 => [0.001]\n",
      "Epoch 48/300 - Train Loss: 3.25859549\n",
      "Epoch 48/300 - Test Loss: 3.26557860\n",
      "lr of epoch 48 => [0.001]\n",
      "Epoch 49/300 - Train Loss: 3.25779646\n",
      "Epoch 49/300 - Test Loss: 3.26425871\n",
      "lr of epoch 49 => [0.001]\n",
      "Epoch 50/300 - Train Loss: 3.25445031\n",
      "Epoch 50/300 - Test Loss: 3.25958104\n",
      "lr of epoch 50 => [0.001]\n",
      "Epoch 51/300 - Train Loss: 3.25269922\n",
      "Epoch 51/300 - Test Loss: 3.25834813\n",
      "lr of epoch 51 => [0.001]\n",
      "Epoch 52/300 - Train Loss: 3.25298720\n",
      "Epoch 52/300 - Test Loss: 3.25610676\n",
      "lr of epoch 52 => [0.001]\n",
      "Epoch 53/300 - Train Loss: 3.25284063\n",
      "Epoch 53/300 - Test Loss: 3.25638321\n",
      "lr of epoch 53 => [0.001]\n",
      "Epoch 54/300 - Train Loss: 3.24969567\n",
      "Epoch 54/300 - Test Loss: 3.25220369\n",
      "lr of epoch 54 => [0.001]\n",
      "Epoch 55/300 - Train Loss: 3.24565205\n",
      "Epoch 55/300 - Test Loss: 3.25085419\n",
      "lr of epoch 55 => [0.001]\n",
      "Epoch 56/300 - Train Loss: 3.24599176\n",
      "Epoch 56/300 - Test Loss: 3.24996252\n",
      "lr of epoch 56 => [0.001]\n",
      "Epoch 57/300 - Train Loss: 3.24405042\n",
      "Epoch 57/300 - Test Loss: 3.24968071\n",
      "lr of epoch 57 => [0.001]\n",
      "Epoch 58/300 - Train Loss: 3.24313668\n",
      "Epoch 58/300 - Test Loss: 3.25314822\n",
      "lr of epoch 58 => [0.001]\n",
      "Epoch 59/300 - Train Loss: 3.24134397\n",
      "Epoch 59/300 - Test Loss: 3.24923894\n",
      "lr of epoch 59 => [0.001]\n",
      "Epoch 60/300 - Train Loss: 3.23903318\n",
      "Epoch 60/300 - Test Loss: 3.24406509\n",
      "lr of epoch 60 => [0.001]\n",
      "Epoch 61/300 - Train Loss: 3.23478645\n",
      "Epoch 61/300 - Test Loss: 3.24247778\n",
      "lr of epoch 61 => [0.001]\n",
      "Epoch 62/300 - Train Loss: 3.23431011\n",
      "Epoch 62/300 - Test Loss: 3.24154169\n",
      "lr of epoch 62 => [0.001]\n",
      "Epoch 63/300 - Train Loss: 3.23416167\n",
      "Epoch 63/300 - Test Loss: 3.24574694\n",
      "lr of epoch 63 => [0.001]\n",
      "Epoch 64/300 - Train Loss: 3.23153631\n",
      "Epoch 64/300 - Test Loss: 3.23759736\n",
      "lr of epoch 64 => [0.001]\n",
      "Epoch 65/300 - Train Loss: 3.22959272\n",
      "Epoch 65/300 - Test Loss: 3.23390234\n",
      "lr of epoch 65 => [0.001]\n",
      "Epoch 66/300 - Train Loss: 3.22475336\n",
      "Epoch 66/300 - Test Loss: 3.22947388\n",
      "lr of epoch 66 => [0.001]\n",
      "Epoch 67/300 - Train Loss: 3.22150284\n",
      "Epoch 67/300 - Test Loss: 3.22791106\n",
      "lr of epoch 67 => [0.001]\n",
      "Epoch 68/300 - Train Loss: 3.22112071\n",
      "Epoch 68/300 - Test Loss: 3.23231074\n",
      "lr of epoch 68 => [0.001]\n",
      "Epoch 69/300 - Train Loss: 3.21872362\n",
      "Epoch 69/300 - Test Loss: 3.22666292\n",
      "lr of epoch 69 => [0.001]\n",
      "Epoch 70/300 - Train Loss: 3.21908832\n",
      "Epoch 70/300 - Test Loss: 3.22934942\n",
      "lr of epoch 70 => [0.001]\n",
      "Epoch 71/300 - Train Loss: 3.21732374\n",
      "Epoch 71/300 - Test Loss: 3.22738850\n",
      "lr of epoch 71 => [0.001]\n",
      "Epoch 72/300 - Train Loss: 3.21380999\n",
      "Epoch 72/300 - Test Loss: 3.22078633\n",
      "lr of epoch 72 => [0.001]\n",
      "Epoch 73/300 - Train Loss: 3.21079759\n",
      "Epoch 73/300 - Test Loss: 3.21913864\n",
      "lr of epoch 73 => [0.001]\n",
      "Epoch 74/300 - Train Loss: 3.20902221\n",
      "Epoch 74/300 - Test Loss: 3.21905076\n",
      "lr of epoch 74 => [0.001]\n",
      "Epoch 75/300 - Train Loss: 3.20498704\n",
      "Epoch 75/300 - Test Loss: 3.21132987\n",
      "lr of epoch 75 => [0.001]\n",
      "Epoch 76/300 - Train Loss: 3.20351669\n",
      "Epoch 76/300 - Test Loss: 3.21183769\n",
      "lr of epoch 76 => [0.001]\n",
      "Epoch 77/300 - Train Loss: 3.20099666\n",
      "Epoch 77/300 - Test Loss: 3.20959854\n",
      "lr of epoch 77 => [0.001]\n",
      "Epoch 78/300 - Train Loss: 3.19843639\n",
      "Epoch 78/300 - Test Loss: 3.20663384\n",
      "lr of epoch 78 => [0.001]\n",
      "Epoch 79/300 - Train Loss: 3.19574355\n",
      "Epoch 79/300 - Test Loss: 3.20259980\n",
      "lr of epoch 79 => [0.001]\n",
      "Epoch 80/300 - Train Loss: 3.19506619\n",
      "Epoch 80/300 - Test Loss: 3.19755503\n",
      "lr of epoch 80 => [0.001]\n",
      "Epoch 81/300 - Train Loss: 3.18783376\n",
      "Epoch 81/300 - Test Loss: 3.19784412\n",
      "lr of epoch 81 => [0.001]\n",
      "Epoch 82/300 - Train Loss: 3.18773658\n",
      "Epoch 82/300 - Test Loss: 3.19412966\n",
      "lr of epoch 82 => [0.001]\n",
      "Epoch 83/300 - Train Loss: 3.18472647\n",
      "Epoch 83/300 - Test Loss: 3.19004748\n",
      "lr of epoch 83 => [0.001]\n",
      "Epoch 84/300 - Train Loss: 3.17948781\n",
      "Epoch 84/300 - Test Loss: 3.18423885\n",
      "lr of epoch 84 => [0.001]\n",
      "Epoch 85/300 - Train Loss: 3.17603148\n",
      "Epoch 85/300 - Test Loss: 3.18349372\n",
      "lr of epoch 85 => [0.001]\n",
      "Epoch 86/300 - Train Loss: 3.17490406\n",
      "Epoch 86/300 - Test Loss: 3.18082025\n",
      "lr of epoch 86 => [0.001]\n",
      "Epoch 87/300 - Train Loss: 3.17280708\n",
      "Epoch 87/300 - Test Loss: 3.17941223\n",
      "lr of epoch 87 => [0.001]\n",
      "Epoch 88/300 - Train Loss: 3.17050587\n",
      "Epoch 88/300 - Test Loss: 3.17801277\n",
      "lr of epoch 88 => [0.001]\n",
      "Epoch 89/300 - Train Loss: 3.16315810\n",
      "Epoch 89/300 - Test Loss: 3.17076444\n",
      "lr of epoch 89 => [0.001]\n",
      "Epoch 90/300 - Train Loss: 3.16096741\n",
      "Epoch 90/300 - Test Loss: 3.16780236\n",
      "lr of epoch 90 => [0.001]\n",
      "Epoch 91/300 - Train Loss: 3.16178656\n",
      "Epoch 91/300 - Test Loss: 3.16216097\n",
      "lr of epoch 91 => [0.001]\n",
      "Epoch 92/300 - Train Loss: 3.15396988\n",
      "Epoch 92/300 - Test Loss: 3.15906922\n",
      "lr of epoch 92 => [0.001]\n",
      "Epoch 93/300 - Train Loss: 3.15096064\n",
      "Epoch 93/300 - Test Loss: 3.16099601\n",
      "lr of epoch 93 => [0.001]\n",
      "Epoch 94/300 - Train Loss: 3.14680307\n",
      "Epoch 94/300 - Test Loss: 3.15568282\n",
      "lr of epoch 94 => [0.001]\n",
      "Epoch 95/300 - Train Loss: 3.14359813\n",
      "Epoch 95/300 - Test Loss: 3.15135727\n",
      "lr of epoch 95 => [0.001]\n",
      "Epoch 96/300 - Train Loss: 3.14280063\n",
      "Epoch 96/300 - Test Loss: 3.14799079\n",
      "lr of epoch 96 => [0.001]\n",
      "Epoch 97/300 - Train Loss: 3.13681037\n",
      "Epoch 97/300 - Test Loss: 3.14737330\n",
      "lr of epoch 97 => [0.001]\n",
      "Epoch 98/300 - Train Loss: 3.13423238\n",
      "Epoch 98/300 - Test Loss: 3.14714136\n",
      "lr of epoch 98 => [0.001]\n",
      "Epoch 99/300 - Train Loss: 3.12853905\n",
      "Epoch 99/300 - Test Loss: 3.13843110\n",
      "lr of epoch 99 => [0.001]\n",
      "Epoch 100/300 - Train Loss: 3.12692531\n",
      "Epoch 100/300 - Test Loss: 3.13658487\n",
      "lr of epoch 100 => [0.001]\n",
      "Epoch 101/300 - Train Loss: 3.12702794\n",
      "Epoch 101/300 - Test Loss: 3.13813444\n",
      "lr of epoch 101 => [0.001]\n",
      "Epoch 102/300 - Train Loss: 3.12549226\n",
      "Epoch 102/300 - Test Loss: 3.13213489\n",
      "lr of epoch 102 => [0.001]\n",
      "Epoch 103/300 - Train Loss: 3.12281316\n",
      "Epoch 103/300 - Test Loss: 3.12966253\n",
      "lr of epoch 103 => [0.001]\n",
      "Epoch 104/300 - Train Loss: 3.12084795\n",
      "Epoch 104/300 - Test Loss: 3.12914558\n",
      "lr of epoch 104 => [0.001]\n",
      "Epoch 105/300 - Train Loss: 3.12097986\n",
      "Epoch 105/300 - Test Loss: 3.12902544\n",
      "lr of epoch 105 => [0.001]\n",
      "Epoch 106/300 - Train Loss: 3.12060120\n",
      "Epoch 106/300 - Test Loss: 3.13209386\n",
      "lr of epoch 106 => [0.001]\n",
      "Epoch 107/300 - Train Loss: 3.11844288\n",
      "Epoch 107/300 - Test Loss: 3.12631764\n",
      "lr of epoch 107 => [0.001]\n",
      "Epoch 108/300 - Train Loss: 3.11697345\n",
      "Epoch 108/300 - Test Loss: 3.12615200\n",
      "lr of epoch 108 => [0.001]\n",
      "Epoch 109/300 - Train Loss: 3.11581630\n",
      "Epoch 109/300 - Test Loss: 3.12864380\n",
      "lr of epoch 109 => [0.001]\n",
      "Epoch 110/300 - Train Loss: 3.11651995\n",
      "Epoch 110/300 - Test Loss: 3.12381693\n",
      "lr of epoch 110 => [0.001]\n",
      "Epoch 111/300 - Train Loss: 3.11426872\n",
      "Epoch 111/300 - Test Loss: 3.12129570\n",
      "lr of epoch 111 => [0.001]\n",
      "Epoch 112/300 - Train Loss: 3.11206999\n",
      "Epoch 112/300 - Test Loss: 3.12040742\n",
      "lr of epoch 112 => [0.001]\n",
      "Epoch 113/300 - Train Loss: 3.11143614\n",
      "Epoch 113/300 - Test Loss: 3.12156882\n",
      "lr of epoch 113 => [0.001]\n",
      "Epoch 114/300 - Train Loss: 3.11208063\n",
      "Epoch 114/300 - Test Loss: 3.12127681\n",
      "lr of epoch 114 => [0.001]\n",
      "Epoch 115/300 - Train Loss: 3.11039030\n",
      "Epoch 115/300 - Test Loss: 3.12170857\n",
      "lr of epoch 115 => [0.001]\n",
      "Epoch 116/300 - Train Loss: 3.10879483\n",
      "Epoch 116/300 - Test Loss: 3.11774677\n",
      "lr of epoch 116 => [0.001]\n",
      "Epoch 117/300 - Train Loss: 3.10661295\n",
      "Epoch 117/300 - Test Loss: 3.11497419\n",
      "lr of epoch 117 => [0.001]\n",
      "Epoch 118/300 - Train Loss: 3.10631198\n",
      "Epoch 118/300 - Test Loss: 3.11379579\n",
      "lr of epoch 118 => [0.001]\n",
      "Epoch 119/300 - Train Loss: 3.10585931\n",
      "Epoch 119/300 - Test Loss: 3.11329536\n",
      "lr of epoch 119 => [0.001]\n",
      "Epoch 120/300 - Train Loss: 3.10323545\n",
      "Epoch 120/300 - Test Loss: 3.11247687\n",
      "lr of epoch 120 => [0.001]\n",
      "Epoch 121/300 - Train Loss: 3.10346687\n",
      "Epoch 121/300 - Test Loss: 3.11232709\n",
      "lr of epoch 121 => [0.001]\n",
      "Epoch 122/300 - Train Loss: 3.10326636\n",
      "Epoch 122/300 - Test Loss: 3.11300352\n",
      "lr of epoch 122 => [0.001]\n",
      "Epoch 123/300 - Train Loss: 3.10222402\n",
      "Epoch 123/300 - Test Loss: 3.11075702\n",
      "lr of epoch 123 => [0.001]\n",
      "Epoch 124/300 - Train Loss: 3.10200459\n",
      "Epoch 124/300 - Test Loss: 3.11084867\n",
      "lr of epoch 124 => [0.001]\n",
      "Epoch 125/300 - Train Loss: 3.10032142\n",
      "Epoch 125/300 - Test Loss: 3.11065582\n",
      "lr of epoch 125 => [0.001]\n",
      "Epoch 126/300 - Train Loss: 3.10016133\n",
      "Epoch 126/300 - Test Loss: 3.11166756\n",
      "lr of epoch 126 => [0.001]\n",
      "Epoch 127/300 - Train Loss: 3.09793218\n",
      "Epoch 127/300 - Test Loss: 3.10349673\n",
      "lr of epoch 127 => [0.001]\n",
      "Epoch 128/300 - Train Loss: 3.09073479\n",
      "Epoch 128/300 - Test Loss: 3.10043689\n",
      "lr of epoch 128 => [0.001]\n",
      "Epoch 129/300 - Train Loss: 3.08952633\n",
      "Epoch 129/300 - Test Loss: 3.09831930\n",
      "lr of epoch 129 => [0.001]\n",
      "Epoch 130/300 - Train Loss: 3.08896257\n",
      "Epoch 130/300 - Test Loss: 3.09913040\n",
      "lr of epoch 130 => [0.001]\n",
      "Epoch 131/300 - Train Loss: 3.08945448\n",
      "Epoch 131/300 - Test Loss: 3.09694255\n",
      "lr of epoch 131 => [0.001]\n",
      "Epoch 132/300 - Train Loss: 3.08867742\n",
      "Epoch 132/300 - Test Loss: 3.09705907\n",
      "lr of epoch 132 => [0.001]\n",
      "Epoch 133/300 - Train Loss: 3.08965298\n",
      "Epoch 133/300 - Test Loss: 3.09875554\n",
      "lr of epoch 133 => [0.001]\n",
      "Epoch 134/300 - Train Loss: 3.08826733\n",
      "Epoch 134/300 - Test Loss: 3.09778752\n",
      "lr of epoch 134 => [0.001]\n",
      "Epoch 135/300 - Train Loss: 3.08726677\n",
      "Epoch 135/300 - Test Loss: 3.09508458\n",
      "lr of epoch 135 => [0.001]\n",
      "Epoch 136/300 - Train Loss: 3.08752625\n",
      "Epoch 136/300 - Test Loss: 3.09352403\n",
      "lr of epoch 136 => [0.001]\n",
      "Epoch 137/300 - Train Loss: 3.08362316\n",
      "Epoch 137/300 - Test Loss: 3.09453535\n",
      "lr of epoch 137 => [0.001]\n",
      "Epoch 138/300 - Train Loss: 3.08263678\n",
      "Epoch 138/300 - Test Loss: 3.09263836\n",
      "lr of epoch 138 => [0.001]\n",
      "Epoch 139/300 - Train Loss: 3.08222450\n",
      "Epoch 139/300 - Test Loss: 3.09162283\n",
      "lr of epoch 139 => [0.001]\n",
      "Epoch 140/300 - Train Loss: 3.08199971\n",
      "Epoch 140/300 - Test Loss: 3.09112042\n",
      "lr of epoch 140 => [0.001]\n",
      "Epoch 141/300 - Train Loss: 3.08264595\n",
      "Epoch 141/300 - Test Loss: 3.09044508\n",
      "lr of epoch 141 => [0.001]\n",
      "Epoch 142/300 - Train Loss: 3.08139169\n",
      "Epoch 142/300 - Test Loss: 3.08902714\n",
      "lr of epoch 142 => [0.001]\n",
      "Epoch 143/300 - Train Loss: 3.08097021\n",
      "Epoch 143/300 - Test Loss: 3.09222696\n",
      "lr of epoch 143 => [0.001]\n",
      "Epoch 144/300 - Train Loss: 3.08046657\n",
      "Epoch 144/300 - Test Loss: 3.08939863\n",
      "lr of epoch 144 => [0.001]\n",
      "Epoch 145/300 - Train Loss: 3.08030739\n",
      "Epoch 145/300 - Test Loss: 3.09235535\n",
      "lr of epoch 145 => [0.001]\n",
      "Epoch 146/300 - Train Loss: 3.07991740\n",
      "Epoch 146/300 - Test Loss: 3.08860853\n",
      "lr of epoch 146 => [0.001]\n",
      "Epoch 147/300 - Train Loss: 3.07981774\n",
      "Epoch 147/300 - Test Loss: 3.08762669\n",
      "lr of epoch 147 => [0.001]\n",
      "Epoch 148/300 - Train Loss: 3.07739908\n",
      "Epoch 148/300 - Test Loss: 3.08712619\n",
      "lr of epoch 148 => [0.001]\n",
      "Epoch 149/300 - Train Loss: 3.07697778\n",
      "Epoch 149/300 - Test Loss: 3.08909163\n",
      "lr of epoch 149 => [0.001]\n",
      "Epoch 150/300 - Train Loss: 3.07604511\n",
      "Epoch 150/300 - Test Loss: 3.08954844\n",
      "lr of epoch 150 => [0.001]\n",
      "Epoch 151/300 - Train Loss: 3.07711034\n",
      "Epoch 151/300 - Test Loss: 3.08732155\n",
      "lr of epoch 151 => [0.001]\n",
      "Epoch 152/300 - Train Loss: 3.07665378\n",
      "Epoch 152/300 - Test Loss: 3.08879849\n",
      "lr of epoch 152 => [0.001]\n",
      "Epoch 153/300 - Train Loss: 3.07668765\n",
      "Epoch 153/300 - Test Loss: 3.08620139\n",
      "lr of epoch 153 => [0.001]\n",
      "Epoch 154/300 - Train Loss: 3.07583344\n",
      "Epoch 154/300 - Test Loss: 3.08703568\n",
      "lr of epoch 154 => [0.001]\n",
      "Epoch 155/300 - Train Loss: 3.07569623\n",
      "Epoch 155/300 - Test Loss: 3.08452549\n",
      "lr of epoch 155 => [0.001]\n",
      "Epoch 156/300 - Train Loss: 3.07543512\n",
      "Epoch 156/300 - Test Loss: 3.08460217\n",
      "lr of epoch 156 => [0.001]\n",
      "Epoch 157/300 - Train Loss: 3.07520516\n",
      "Epoch 157/300 - Test Loss: 3.08417700\n",
      "lr of epoch 157 => [0.001]\n",
      "Epoch 158/300 - Train Loss: 3.07554018\n",
      "Epoch 158/300 - Test Loss: 3.08527907\n",
      "lr of epoch 158 => [0.001]\n",
      "Epoch 159/300 - Train Loss: 3.07478331\n",
      "Epoch 159/300 - Test Loss: 3.08369979\n",
      "lr of epoch 159 => [0.001]\n",
      "Epoch 160/300 - Train Loss: 3.07475569\n",
      "Epoch 160/300 - Test Loss: 3.08396362\n",
      "lr of epoch 160 => [0.001]\n",
      "Epoch 161/300 - Train Loss: 3.07502380\n",
      "Epoch 161/300 - Test Loss: 3.08364168\n",
      "lr of epoch 161 => [0.001]\n",
      "Epoch 162/300 - Train Loss: 3.07477397\n",
      "Epoch 162/300 - Test Loss: 3.09022493\n",
      "lr of epoch 162 => [0.001]\n",
      "Epoch 163/300 - Train Loss: 3.07535673\n",
      "Epoch 163/300 - Test Loss: 3.08403293\n",
      "lr of epoch 163 => [0.001]\n",
      "Epoch 164/300 - Train Loss: 3.07479702\n",
      "Epoch 164/300 - Test Loss: 3.08391516\n",
      "lr of epoch 164 => [0.001]\n",
      "Epoch 165/300 - Train Loss: 3.07516752\n",
      "Epoch 165/300 - Test Loss: 3.08744877\n",
      "lr of epoch 165 => [0.001]\n",
      "Epoch 166/300 - Train Loss: 3.07407668\n",
      "Epoch 166/300 - Test Loss: 3.08550660\n",
      "lr of epoch 166 => [0.001]\n",
      "Epoch 167/300 - Train Loss: 3.07476633\n",
      "Epoch 167/300 - Test Loss: 3.08408169\n",
      "lr of epoch 167 => [0.001]\n",
      "Epoch 168/300 - Train Loss: 3.07502966\n",
      "Epoch 168/300 - Test Loss: 3.08484181\n",
      "lr of epoch 168 => [0.001]\n",
      "Epoch 169/300 - Train Loss: 3.07449067\n",
      "Epoch 169/300 - Test Loss: 3.08311381\n",
      "lr of epoch 169 => [0.001]\n",
      "Epoch 170/300 - Train Loss: 3.07440319\n",
      "Epoch 170/300 - Test Loss: 3.08549839\n",
      "lr of epoch 170 => [0.001]\n",
      "Epoch 171/300 - Train Loss: 3.07499715\n",
      "Epoch 171/300 - Test Loss: 3.08228021\n",
      "lr of epoch 171 => [0.001]\n",
      "Epoch 172/300 - Train Loss: 3.07397448\n",
      "Epoch 172/300 - Test Loss: 3.08183115\n",
      "lr of epoch 172 => [0.001]\n",
      "Epoch 173/300 - Train Loss: 3.07369255\n",
      "Epoch 173/300 - Test Loss: 3.08279021\n",
      "lr of epoch 173 => [0.001]\n",
      "Epoch 174/300 - Train Loss: 3.07415790\n",
      "Epoch 174/300 - Test Loss: 3.08382763\n",
      "lr of epoch 174 => [0.001]\n",
      "Epoch 175/300 - Train Loss: 3.07449360\n",
      "Epoch 175/300 - Test Loss: 3.08247928\n",
      "lr of epoch 175 => [0.001]\n",
      "Epoch 176/300 - Train Loss: 3.07403436\n",
      "Epoch 176/300 - Test Loss: 3.08282881\n",
      "lr of epoch 176 => [0.001]\n",
      "Epoch 177/300 - Train Loss: 3.07367304\n",
      "Epoch 177/300 - Test Loss: 3.08248862\n",
      "lr of epoch 177 => [0.001]\n",
      "Epoch 178/300 - Train Loss: 3.07386990\n",
      "Epoch 178/300 - Test Loss: 3.08117910\n",
      "lr of epoch 178 => [0.001]\n",
      "Epoch 179/300 - Train Loss: 3.07201856\n",
      "Epoch 179/300 - Test Loss: 3.08243937\n",
      "lr of epoch 179 => [0.001]\n",
      "Epoch 180/300 - Train Loss: 3.07291708\n",
      "Epoch 180/300 - Test Loss: 3.08430620\n",
      "lr of epoch 180 => [0.001]\n",
      "Epoch 181/300 - Train Loss: 3.07322819\n",
      "Epoch 181/300 - Test Loss: 3.08465385\n",
      "lr of epoch 181 => [0.001]\n",
      "Epoch 182/300 - Train Loss: 3.07259726\n",
      "Epoch 182/300 - Test Loss: 3.08167585\n",
      "lr of epoch 182 => [0.001]\n",
      "Epoch 183/300 - Train Loss: 3.07252477\n",
      "Epoch 183/300 - Test Loss: 3.08212886\n",
      "lr of epoch 183 => [0.001]\n",
      "Epoch 184/300 - Train Loss: 3.07272939\n",
      "Epoch 184/300 - Test Loss: 3.08255175\n",
      "lr of epoch 184 => [0.001]\n",
      "Epoch 185/300 - Train Loss: 3.07295793\n",
      "Epoch 185/300 - Test Loss: 3.08216608\n",
      "lr of epoch 185 => [0.001]\n",
      "Epoch 186/300 - Train Loss: 3.07289864\n",
      "Epoch 186/300 - Test Loss: 3.08248370\n",
      "lr of epoch 186 => [0.001]\n",
      "Epoch 187/300 - Train Loss: 3.07090677\n",
      "Epoch 187/300 - Test Loss: 3.08525289\n",
      "lr of epoch 187 => [0.001]\n",
      "Epoch 188/300 - Train Loss: 3.06837328\n",
      "Epoch 188/300 - Test Loss: 3.07197087\n",
      "lr of epoch 188 => [0.001]\n",
      "Epoch 189/300 - Train Loss: 3.06011336\n",
      "Epoch 189/300 - Test Loss: 3.07062587\n",
      "lr of epoch 189 => [0.001]\n",
      "Epoch 190/300 - Train Loss: 3.06023449\n",
      "Epoch 190/300 - Test Loss: 3.06967883\n",
      "lr of epoch 190 => [0.001]\n",
      "Epoch 191/300 - Train Loss: 3.06053707\n",
      "Epoch 191/300 - Test Loss: 3.06997246\n",
      "lr of epoch 191 => [0.001]\n",
      "Epoch 192/300 - Train Loss: 3.05997690\n",
      "Epoch 192/300 - Test Loss: 3.07174058\n",
      "lr of epoch 192 => [0.001]\n",
      "Epoch 193/300 - Train Loss: 3.05967763\n",
      "Epoch 193/300 - Test Loss: 3.06831796\n",
      "lr of epoch 193 => [0.001]\n",
      "Epoch 194/300 - Train Loss: 3.05802043\n",
      "Epoch 194/300 - Test Loss: 3.06832965\n",
      "lr of epoch 194 => [0.001]\n",
      "Epoch 195/300 - Train Loss: 3.05882214\n",
      "Epoch 195/300 - Test Loss: 3.06996752\n",
      "lr of epoch 195 => [0.001]\n",
      "Epoch 196/300 - Train Loss: 3.05859125\n",
      "Epoch 196/300 - Test Loss: 3.06756786\n",
      "lr of epoch 196 => [0.001]\n",
      "Epoch 197/300 - Train Loss: 3.05835075\n",
      "Epoch 197/300 - Test Loss: 3.06782984\n",
      "lr of epoch 197 => [0.001]\n",
      "Epoch 198/300 - Train Loss: 3.05832113\n",
      "Epoch 198/300 - Test Loss: 3.06988803\n",
      "lr of epoch 198 => [0.001]\n",
      "Epoch 199/300 - Train Loss: 3.05810035\n",
      "Epoch 199/300 - Test Loss: 3.06772450\n",
      "lr of epoch 199 => [0.001]\n",
      "Epoch 200/300 - Train Loss: 3.05798026\n",
      "Epoch 200/300 - Test Loss: 3.06838971\n",
      "lr of epoch 200 => [0.001]\n",
      "Epoch 201/300 - Train Loss: 3.05785969\n",
      "Epoch 201/300 - Test Loss: 3.07225284\n",
      "lr of epoch 201 => [0.001]\n",
      "Epoch 202/300 - Train Loss: 3.05823350\n",
      "Epoch 202/300 - Test Loss: 3.07133757\n",
      "lr of epoch 202 => [0.001]\n",
      "Epoch 203/300 - Train Loss: 3.05834723\n",
      "Epoch 203/300 - Test Loss: 3.06730572\n",
      "lr of epoch 203 => [0.001]\n",
      "Epoch 204/300 - Train Loss: 3.05893264\n",
      "Epoch 204/300 - Test Loss: 3.07025661\n",
      "lr of epoch 204 => [0.001]\n",
      "Epoch 205/300 - Train Loss: 3.05791482\n",
      "Epoch 205/300 - Test Loss: 3.06857256\n",
      "lr of epoch 205 => [0.001]\n",
      "Epoch 206/300 - Train Loss: 3.05544131\n",
      "Epoch 206/300 - Test Loss: 3.06264671\n",
      "lr of epoch 206 => [0.001]\n",
      "Epoch 207/300 - Train Loss: 3.05225333\n",
      "Epoch 207/300 - Test Loss: 3.06380975\n",
      "lr of epoch 207 => [0.001]\n",
      "Epoch 208/300 - Train Loss: 3.05252728\n",
      "Epoch 208/300 - Test Loss: 3.06354609\n",
      "lr of epoch 208 => [0.001]\n",
      "Epoch 209/300 - Train Loss: 3.05247361\n",
      "Epoch 209/300 - Test Loss: 3.06408707\n",
      "lr of epoch 209 => [0.001]\n",
      "Epoch 210/300 - Train Loss: 3.05088823\n",
      "Epoch 210/300 - Test Loss: 3.05820495\n",
      "lr of epoch 210 => [0.001]\n",
      "Epoch 211/300 - Train Loss: 3.04767268\n",
      "Epoch 211/300 - Test Loss: 3.05766055\n",
      "lr of epoch 211 => [0.001]\n",
      "Epoch 212/300 - Train Loss: 3.04828014\n",
      "Epoch 212/300 - Test Loss: 3.05883846\n",
      "lr of epoch 212 => [0.001]\n",
      "Epoch 213/300 - Train Loss: 3.04729310\n",
      "Epoch 213/300 - Test Loss: 3.06014170\n",
      "lr of epoch 213 => [0.001]\n",
      "Epoch 214/300 - Train Loss: 3.04786704\n",
      "Epoch 214/300 - Test Loss: 3.05737392\n",
      "lr of epoch 214 => [0.001]\n",
      "Epoch 215/300 - Train Loss: 3.04805707\n",
      "Epoch 215/300 - Test Loss: 3.05913221\n",
      "lr of epoch 215 => [0.001]\n",
      "Epoch 216/300 - Train Loss: 3.04823467\n",
      "Epoch 216/300 - Test Loss: 3.05726060\n",
      "lr of epoch 216 => [0.001]\n",
      "Epoch 217/300 - Train Loss: 3.04810522\n",
      "Epoch 217/300 - Test Loss: 3.05913249\n",
      "lr of epoch 217 => [0.001]\n",
      "Epoch 218/300 - Train Loss: 3.04750136\n",
      "Epoch 218/300 - Test Loss: 3.05699624\n",
      "lr of epoch 218 => [0.001]\n",
      "Epoch 219/300 - Train Loss: 3.04774268\n",
      "Epoch 219/300 - Test Loss: 3.05871388\n",
      "lr of epoch 219 => [0.001]\n",
      "Epoch 220/300 - Train Loss: 3.04862663\n",
      "Epoch 220/300 - Test Loss: 3.05722411\n",
      "lr of epoch 220 => [0.001]\n",
      "Epoch 221/300 - Train Loss: 3.04699569\n",
      "Epoch 221/300 - Test Loss: 3.05558013\n",
      "lr of epoch 221 => [0.001]\n",
      "Epoch 222/300 - Train Loss: 3.04637239\n",
      "Epoch 222/300 - Test Loss: 3.05637496\n",
      "lr of epoch 222 => [0.001]\n",
      "Epoch 223/300 - Train Loss: 3.04657890\n",
      "Epoch 223/300 - Test Loss: 3.05704419\n",
      "lr of epoch 223 => [0.001]\n",
      "Epoch 224/300 - Train Loss: 3.04560220\n",
      "Epoch 224/300 - Test Loss: 3.05059640\n",
      "lr of epoch 224 => [0.001]\n",
      "Epoch 225/300 - Train Loss: 3.03997094\n",
      "Epoch 225/300 - Test Loss: 3.05294892\n",
      "lr of epoch 225 => [0.001]\n",
      "Epoch 226/300 - Train Loss: 3.04000031\n",
      "Epoch 226/300 - Test Loss: 3.04983708\n",
      "lr of epoch 226 => [0.001]\n",
      "Epoch 227/300 - Train Loss: 3.04011815\n",
      "Epoch 227/300 - Test Loss: 3.05697535\n",
      "lr of epoch 227 => [0.001]\n",
      "Epoch 228/300 - Train Loss: 3.04106319\n",
      "Epoch 228/300 - Test Loss: 3.05243616\n",
      "lr of epoch 228 => [0.001]\n",
      "Epoch 229/300 - Train Loss: 3.04052121\n",
      "Epoch 229/300 - Test Loss: 3.05477144\n",
      "lr of epoch 229 => [0.001]\n",
      "Epoch 230/300 - Train Loss: 3.04027647\n",
      "Epoch 230/300 - Test Loss: 3.05049463\n",
      "lr of epoch 230 => [0.001]\n",
      "Epoch 231/300 - Train Loss: 3.03994932\n",
      "Epoch 231/300 - Test Loss: 3.05221570\n",
      "lr of epoch 231 => [0.001]\n",
      "Epoch 232/300 - Train Loss: 3.04023922\n",
      "Epoch 232/300 - Test Loss: 3.05196522\n",
      "lr of epoch 232 => [0.001]\n",
      "Epoch 233/300 - Train Loss: 3.04123415\n",
      "Epoch 233/300 - Test Loss: 3.05075790\n",
      "lr of epoch 233 => [0.001]\n",
      "Epoch 234/300 - Train Loss: 3.04002298\n",
      "Epoch 234/300 - Test Loss: 3.04912475\n",
      "lr of epoch 234 => [0.001]\n",
      "Epoch 235/300 - Train Loss: 3.03981730\n",
      "Epoch 235/300 - Test Loss: 3.05203799\n",
      "lr of epoch 235 => [0.001]\n",
      "Epoch 236/300 - Train Loss: 3.04072359\n",
      "Epoch 236/300 - Test Loss: 3.05036180\n",
      "lr of epoch 236 => [0.001]\n",
      "Epoch 237/300 - Train Loss: 3.03996372\n",
      "Epoch 237/300 - Test Loss: 3.05403377\n",
      "lr of epoch 237 => [0.001]\n",
      "Epoch 238/300 - Train Loss: 3.04000032\n",
      "Epoch 238/300 - Test Loss: 3.04920296\n",
      "lr of epoch 238 => [0.001]\n",
      "Epoch 239/300 - Train Loss: 3.03942953\n",
      "Epoch 239/300 - Test Loss: 3.04990983\n",
      "lr of epoch 239 => [0.001]\n",
      "Epoch 240/300 - Train Loss: 3.03943486\n",
      "Epoch 240/300 - Test Loss: 3.05224598\n",
      "lr of epoch 240 => [0.001]\n",
      "Epoch 241/300 - Train Loss: 3.03979754\n",
      "Epoch 241/300 - Test Loss: 3.04966110\n",
      "lr of epoch 241 => [0.001]\n",
      "Epoch 242/300 - Train Loss: 3.03925425\n",
      "Epoch 242/300 - Test Loss: 3.05095326\n",
      "lr of epoch 242 => [0.001]\n",
      "Epoch 243/300 - Train Loss: 3.03928779\n",
      "Epoch 243/300 - Test Loss: 3.04843786\n",
      "lr of epoch 243 => [0.001]\n",
      "Epoch 244/300 - Train Loss: 3.03915580\n",
      "Epoch 244/300 - Test Loss: 3.04821395\n",
      "lr of epoch 244 => [0.001]\n",
      "Epoch 245/300 - Train Loss: 3.03916742\n",
      "Epoch 245/300 - Test Loss: 3.04930271\n",
      "lr of epoch 245 => [0.001]\n",
      "Epoch 246/300 - Train Loss: 3.03836520\n",
      "Epoch 246/300 - Test Loss: 3.04767058\n",
      "lr of epoch 246 => [0.001]\n",
      "Epoch 247/300 - Train Loss: 3.03821824\n",
      "Epoch 247/300 - Test Loss: 3.04988151\n",
      "lr of epoch 247 => [0.001]\n",
      "Epoch 248/300 - Train Loss: 3.03803683\n",
      "Epoch 248/300 - Test Loss: 3.04737236\n",
      "lr of epoch 248 => [0.001]\n",
      "Epoch 249/300 - Train Loss: 3.03592740\n",
      "Epoch 249/300 - Test Loss: 3.04246792\n",
      "lr of epoch 249 => [0.001]\n",
      "Epoch 250/300 - Train Loss: 3.03216634\n",
      "Epoch 250/300 - Test Loss: 3.04237972\n",
      "lr of epoch 250 => [0.001]\n",
      "Epoch 251/300 - Train Loss: 3.03193692\n",
      "Epoch 251/300 - Test Loss: 3.04296042\n",
      "lr of epoch 251 => [0.001]\n",
      "Epoch 252/300 - Train Loss: 3.03204894\n",
      "Epoch 252/300 - Test Loss: 3.04618645\n",
      "lr of epoch 252 => [0.001]\n",
      "Epoch 253/300 - Train Loss: 3.03214046\n",
      "Epoch 253/300 - Test Loss: 3.04324890\n",
      "lr of epoch 253 => [0.001]\n",
      "Epoch 254/300 - Train Loss: 3.03194912\n",
      "Epoch 254/300 - Test Loss: 3.04454113\n",
      "lr of epoch 254 => [0.001]\n",
      "Epoch 255/300 - Train Loss: 3.03217405\n",
      "Epoch 255/300 - Test Loss: 3.04408706\n",
      "lr of epoch 255 => [0.001]\n",
      "Epoch 256/300 - Train Loss: 3.03217967\n",
      "Epoch 256/300 - Test Loss: 3.04568217\n",
      "lr of epoch 256 => [0.001]\n",
      "Epoch 257/300 - Train Loss: 3.03265839\n",
      "Epoch 257/300 - Test Loss: 3.04483394\n",
      "lr of epoch 257 => [0.001]\n",
      "Epoch 258/300 - Train Loss: 3.03213719\n",
      "Epoch 258/300 - Test Loss: 3.04240171\n",
      "lr of epoch 258 => [0.001]\n",
      "Epoch 259/300 - Train Loss: 3.03202143\n",
      "Epoch 259/300 - Test Loss: 3.04344986\n",
      "lr of epoch 259 => [0.001]\n",
      "Epoch 260/300 - Train Loss: 3.03236092\n",
      "Epoch 260/300 - Test Loss: 3.04449074\n",
      "lr of epoch 260 => [0.001]\n",
      "Epoch 261/300 - Train Loss: 3.03167806\n",
      "Epoch 261/300 - Test Loss: 3.04203470\n",
      "lr of epoch 261 => [0.001]\n",
      "Epoch 262/300 - Train Loss: 3.03248442\n",
      "Epoch 262/300 - Test Loss: 3.04244519\n",
      "lr of epoch 262 => [0.001]\n",
      "Epoch 263/300 - Train Loss: 3.03222157\n",
      "Epoch 263/300 - Test Loss: 3.04233245\n",
      "lr of epoch 263 => [0.001]\n",
      "Epoch 264/300 - Train Loss: 3.03206416\n",
      "Epoch 264/300 - Test Loss: 3.04341662\n",
      "lr of epoch 264 => [0.001]\n",
      "Epoch 265/300 - Train Loss: 3.03180032\n",
      "Epoch 265/300 - Test Loss: 3.04264637\n",
      "lr of epoch 265 => [0.001]\n",
      "Epoch 266/300 - Train Loss: 3.03187613\n",
      "Epoch 266/300 - Test Loss: 3.04222341\n",
      "lr of epoch 266 => [0.001]\n",
      "Epoch 267/300 - Train Loss: 3.03183068\n",
      "Epoch 267/300 - Test Loss: 3.04250274\n",
      "lr of epoch 267 => [0.001]\n",
      "Epoch 268/300 - Train Loss: 3.03214464\n",
      "Epoch 268/300 - Test Loss: 3.04269359\n",
      "lr of epoch 268 => [0.001]\n",
      "Epoch 269/300 - Train Loss: 3.03202040\n",
      "Epoch 269/300 - Test Loss: 3.04221649\n",
      "lr of epoch 269 => [0.001]\n",
      "Epoch 270/300 - Train Loss: 3.03124256\n",
      "Epoch 270/300 - Test Loss: 3.04101183\n",
      "lr of epoch 270 => [0.001]\n",
      "Epoch 271/300 - Train Loss: 3.03128081\n",
      "Epoch 271/300 - Test Loss: 3.04192316\n",
      "lr of epoch 271 => [0.001]\n",
      "Epoch 272/300 - Train Loss: 3.03122441\n",
      "Epoch 272/300 - Test Loss: 3.04310139\n",
      "lr of epoch 272 => [0.001]\n",
      "Epoch 273/300 - Train Loss: 3.03150986\n",
      "Epoch 273/300 - Test Loss: 3.04169998\n",
      "lr of epoch 273 => [0.001]\n",
      "Epoch 274/300 - Train Loss: 3.03005701\n",
      "Epoch 274/300 - Test Loss: 3.04240617\n",
      "lr of epoch 274 => [0.001]\n",
      "Epoch 275/300 - Train Loss: 3.03076422\n",
      "Epoch 275/300 - Test Loss: 3.04229998\n",
      "lr of epoch 275 => [0.001]\n",
      "Epoch 276/300 - Train Loss: 3.03111560\n",
      "Epoch 276/300 - Test Loss: 3.04260826\n",
      "lr of epoch 276 => [0.001]\n",
      "Epoch 277/300 - Train Loss: 3.03085272\n",
      "Epoch 277/300 - Test Loss: 3.04135078\n",
      "lr of epoch 277 => [0.001]\n",
      "Epoch 278/300 - Train Loss: 3.03130392\n",
      "Epoch 278/300 - Test Loss: 3.04164312\n",
      "lr of epoch 278 => [0.001]\n",
      "Epoch 279/300 - Train Loss: 3.03148348\n",
      "Epoch 279/300 - Test Loss: 3.03881219\n",
      "lr of epoch 279 => [0.001]\n",
      "Epoch 280/300 - Train Loss: 3.02717450\n",
      "Epoch 280/300 - Test Loss: 3.03707645\n",
      "lr of epoch 280 => [0.001]\n",
      "Epoch 281/300 - Train Loss: 3.02747963\n",
      "Epoch 281/300 - Test Loss: 3.03914570\n",
      "lr of epoch 281 => [0.001]\n",
      "Epoch 282/300 - Train Loss: 3.02659275\n",
      "Epoch 282/300 - Test Loss: 3.03687411\n",
      "lr of epoch 282 => [0.001]\n",
      "Epoch 283/300 - Train Loss: 3.02737921\n",
      "Epoch 283/300 - Test Loss: 3.03671112\n",
      "lr of epoch 283 => [0.001]\n",
      "Epoch 284/300 - Train Loss: 3.02577309\n",
      "Epoch 284/300 - Test Loss: 3.03642245\n",
      "lr of epoch 284 => [0.001]\n",
      "Epoch 285/300 - Train Loss: 3.02597642\n",
      "Epoch 285/300 - Test Loss: 3.03656700\n",
      "lr of epoch 285 => [0.001]\n",
      "Epoch 286/300 - Train Loss: 3.02578091\n",
      "Epoch 286/300 - Test Loss: 3.03900769\n",
      "lr of epoch 286 => [0.001]\n",
      "Epoch 287/300 - Train Loss: 3.02601473\n",
      "Epoch 287/300 - Test Loss: 3.03669464\n",
      "lr of epoch 287 => [0.001]\n",
      "Epoch 288/300 - Train Loss: 3.02613276\n",
      "Epoch 288/300 - Test Loss: 3.03736611\n",
      "lr of epoch 288 => [0.001]\n",
      "Epoch 289/300 - Train Loss: 3.02600582\n",
      "Epoch 289/300 - Test Loss: 3.03639327\n",
      "lr of epoch 289 => [0.001]\n",
      "Epoch 290/300 - Train Loss: 3.02632392\n",
      "Epoch 290/300 - Test Loss: 3.03915576\n",
      "lr of epoch 290 => [0.001]\n",
      "Epoch 291/300 - Train Loss: 3.02631987\n",
      "Epoch 291/300 - Test Loss: 3.03545876\n",
      "lr of epoch 291 => [0.001]\n",
      "Epoch 292/300 - Train Loss: 3.02575939\n",
      "Epoch 292/300 - Test Loss: 3.03709135\n",
      "lr of epoch 292 => [0.001]\n",
      "Epoch 293/300 - Train Loss: 3.02573910\n",
      "Epoch 293/300 - Test Loss: 3.03653161\n",
      "lr of epoch 293 => [0.001]\n",
      "Epoch 294/300 - Train Loss: 3.02586447\n",
      "Epoch 294/300 - Test Loss: 3.03765505\n",
      "lr of epoch 294 => [0.001]\n",
      "Epoch 295/300 - Train Loss: 3.02637092\n",
      "Epoch 295/300 - Test Loss: 3.03664030\n",
      "lr of epoch 295 => [0.001]\n",
      "Epoch 296/300 - Train Loss: 3.02623852\n",
      "Epoch 296/300 - Test Loss: 3.04097513\n",
      "lr of epoch 296 => [0.001]\n",
      "Epoch 297/300 - Train Loss: 3.02590645\n",
      "Epoch 297/300 - Test Loss: 3.03576888\n",
      "lr of epoch 297 => [0.001]\n",
      "Epoch 298/300 - Train Loss: 3.02598216\n",
      "Epoch 298/300 - Test Loss: 3.03776096\n",
      "lr of epoch 298 => [0.001]\n",
      "Epoch 299/300 - Train Loss: 3.02628211\n",
      "Epoch 299/300 - Test Loss: 3.03851799\n",
      "lr of epoch 299 => [0.001]\n",
      "Epoch 300/300 - Train Loss: 3.02623624\n",
      "Epoch 300/300 - Test Loss: 3.03632551\n",
      "lr of epoch 300 => [0.001]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory /root/DeepTTA/results does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     92\u001b[0m encoder_best_weight \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m best_weight\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m)} \u001b[38;5;66;03m# only store the encoder part without decoder part\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_best_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/root/DeepTTA/results/Encoder_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_save_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_best_loss_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbest_val_loss\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.2\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size:\u001b[39m\u001b[38;5;124m'\u001b[39m,batch_size,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation_function:\u001b[39m\u001b[38;5;124m'\u001b[39m,activation_function,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     96\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate:\u001b[39m\u001b[38;5;124m'\u001b[39m,learning_rate,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_iters:\u001b[39m\u001b[38;5;124m'\u001b[39m,warmup_iters)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest Epoch : \u001b[39m\u001b[38;5;124m\"\u001b[39m,best_epoch,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ,\u001b[39m\u001b[38;5;124m\"\u001b[39m ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_val_loss : \u001b[39m\u001b[38;5;124m\"\u001b[39m,best_val_loss)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:620\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    617\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    621\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    622\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:494\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:465\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /root/DeepTTA/results does not exist."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':#被別人呼叫的時候不會執行main以下的程式，只會執行\n",
    "\n",
    "    input_dim = data_mut_tcga.shape[1]# (8238sample[0], 2649gene[1])\n",
    "    mut_encode_dim =[1250,250,50]\n",
    "    batch_size = 64\n",
    "    epoch_size = 300 #100\n",
    "    activation_function = nn.ReLU()\n",
    "    model_save_name = \"tcga_exp_%d_%d_%d\" % (mut_encode_dim[0], mut_encode_dim[1], mut_encode_dim[2])\n",
    "    learning_rate=0.001\n",
    "    warmup_iters = 185\n",
    "    seed=42\n",
    "    patience = 25\n",
    "    Decrease_percent = 1\n",
    "    \n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    train_size = int(0.8 * len(data_mut_tcga))\n",
    "    test_size = len(data_mut_tcga) - train_size\n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = random_split(data_mut_tcga.values, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders for training and testing\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    t = time.time()\n",
    "    torch.manual_seed(seed)\n",
    "    model = AE_dense_3layers(input_dim=input_dim, mut_encode_dim=mut_encode_dim, activation_func=activation_function).to(device=device)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if warmup_iters is not None:\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters,Decrease_percent)\n",
    "    \n",
    "    # Training with early stopping (assuming you've defined the EarlyStopping logic)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_weight=None\n",
    "    counter = 0\n",
    "    train_epoch_loss_list = []#  for train every epoch loss plot\n",
    "    test_epoch_loss_list=[]#  for validation every epoch loss plot\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    for epoch in range(epoch_size):\n",
    "        model.train()\n",
    "        model.requires_grad = True\n",
    "        total_train_loss = 0.0\n",
    "        for batch_idx,inputs in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs =inputs.float().to(device=device) # change torch.Tensor int64 to float32\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() # sum every batch's loss to compute average loss of every batch for one epoch\n",
    "            #print(f'Epoch {epoch + 1}/{epoch_size} - Batch {batch_idx+1}/{len(dataloader_mut_tcga)} - Loss: {loss.item():.4f}')#batch loss()\n",
    "        # Calculate and print the average loss of batch for the epoch\n",
    "        average_loss = total_train_loss / len(train_loader) # 一個 epoch 的 loss 是 batch 的 average loss\n",
    "        train_epoch_loss_list.append(average_loss)# for loss plot'\n",
    "        print(f'Epoch {epoch + 1}/{epoch_size} - Train Loss: {average_loss:.8f}')\n",
    "        \n",
    "        model.eval()\n",
    "        model.requires_grad = False\n",
    "        total_test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx,inputs in enumerate(test_loader):\n",
    "                inputs =inputs.float().to(device=device) # change torch.Tensor int64 to float32\n",
    "                outputs = model(inputs)\n",
    "                test_loss = criterion(outputs, inputs)\n",
    "                total_test_loss += test_loss.item() \n",
    "        test_average_loss = total_test_loss / len(test_loader)\n",
    "        test_epoch_loss_list.append(test_average_loss)# for loss plot'\n",
    "        print(f'Epoch {epoch + 1}/{epoch_size} - Test Loss: {test_average_loss:.8f}')\n",
    "\n",
    "        if warmup_iters:\n",
    "            print(\"lr of epoch\", epoch + 1, \"=>\", lr_scheduler.get_lr()) \n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        if test_average_loss < best_val_loss:\n",
    "            best_val_loss = test_average_loss\n",
    "            best_weight = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch+1\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping after {patience} epochs of no improvement.')\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95c56621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64   activation_function: ReLU()   learning_rate: 0.001   warmup_iters: 185\n",
      "best Epoch :  291  , best_val_loss :  3.035458757327153\n",
      "\n",
      "Autoencoder training completed in 71.2 mins.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_best_weight = {key: value for key, value in best_weight.items() if key.startswith('encoder')} # only store the encoder part without decoder part\n",
    "torch.save(encoder_best_weight, f'./results/Encoder_{model_save_name}_best_loss_{best_val_loss:.8}.pt')\n",
    "\n",
    "print('batch_size:',batch_size,\" \",'activation_function:',activation_function,\" \",\n",
    "      'learning_rate:',learning_rate,\" \",'warmup_iters:',warmup_iters)\n",
    "print(\"best Epoch : \",best_epoch,\" ,\" ,\"best_val_loss : \",best_val_loss)\n",
    "print('\\nAutoencoder training completed in %.1f mins.\\n' % ((time.time() - t) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8d226b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1QAAAIrCAYAAADoYtj9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAACF6ElEQVR4nO3dd3hTZePG8TudULqAtrS0ZW8ZIqAWBKoCIorFIiKggIoTEZyIioCC4Ab1J+/rq4ILBwjiYIhI2SgoKCgOFChg2aNllbQ5vz9CQtOmKx0ZfD/XlUtyzpNznvRJa+8+y2QYhiEAAAAAQKn5ubsCAAAAAOCtCFQAAAAA4CICFQAAAAC4iEAFAAAAAC4iUAEAAACAiwhUAAAAAOAiAhUAAAAAuIhABQAAAAAuIlABAAAAgIsIVADggnr16slkMjk8goODVadOHfXv318rV650dxXLbMeOHTKZTKpXr55b7n///ffbv7ZffvllkWVnzpxZoD2cPUrzXtLS0uyv83UnTpzQq6++qp49e6p27doKDg5WaGiomjZtqptvvlnz58+XxWJxdzUBwCMFuLsCAODNOnXqpEaNGkmSjh49qg0bNujTTz/V7Nmz9eKLL+rBBx+s8DqMHz9eEyZM0Lhx4zR+/PgKv19lyM7O1ocffmh//s4776h3797Fvq5atWq64YYbCj0fFRVVLvXzJd98841uvvlmHThwQAEBAWrXrp06d+6snJwc/f333/rwww/14YcfqkOHDvrhhx/cXV0A8DgEKgAog2HDhmno0KH256dPn9Zdd92l9957T48++qiuvfZaNWnSxH0VLIP4+Hht3bpVgYGBlX7vefPm6fDhw6pdu7YyMjL01Vdfad++fapVq1aRr4uKitLMmTMrp5I+4Ouvv1ZKSopyc3N12223afLkyYqJiXEok56ermeffVaffvqpm2oJAJ6NIX8AUI6qVKmi//u//1O1atWUm5uruXPnurtKLgsMDFSzZs3UsGHDSr/322+/LUkaOXKkunbtqpycHL333nuVXg9fdujQId18883Kzc3V/fffr7fffrtAmJKkOnXq6D//+Y8+//zzyq8kAHgBAhUAlDPb3BPJOg/JJu98nBkzZigpKUkREREymUwO5f799189+OCDat68uUJCQhQWFqYOHTro9ddfV05OjsO9TCaTJkyYIEmaMGGCw3yhvD1nv/32m8aNG6dOnTopPj5eQUFBqlmzprp161Zoz0NRc6jyvpfPPvtMl112mcLDw1WtWjV16tRJCxYsKO2XzeG+S5cuVUBAgAYPHqzbb79dknXYnzdYvHixrr32WsXExCgoKEi1a9dW//79tWHDBqfljx07pieffFKtWrVStWrVFBwcrNq1a6tTp0566qmnZDabHcr/+OOP6t+/vxISEhQUFKTw8HA1aNBAffv21fz580tcz9dff11Hjx5VTEyMnn/++WLLd+nSxeF5cfPLkpOTZTKZlJaWVujxlStXqnfv3oqOjpafn59mzpypAQMGyGQyacqUKYVe+6uvvpLJZFLbtm0LnPvzzz911113qWHDhqpSpYoiIiLUpUsXffDBB8W+RwBwBYEKACpAZmamJCk4OLjAuREjRmjYsGEKCAjQNddco0suucT+i+mKFSvUsmVLvfLKKzp9+rS6d++uTp066e+//9aIESN0zTXXOPyCPWTIELVp00aS1KZNGw0ZMsT+uOyyy+zlXn75ZT399NM6fPiwWrVqpdTUVDVt2lTLli1T//79XZ7rNW7cOPXr10+S1KtXLzVu3Fhr1qzRtddeq3nz5rl0zXfeeUeGYahXr16KjY1V3759FRERod9//11r1qxx6ZqVZezYserZs6cWLFigJk2a6IYbblCtWrX06aef6tJLLy0QCk+ePKnLLrtMkyZN0r59+3TllVfa2+aff/7RM888oxMnTtjLL126VElJSfr0008VFRWllJQUdevWTdHR0fr66681Y8aMEtfVFr769+/v9HNa0WbPnq3k5GT9888/6tatm7p3767g4GDdeuutkqR333230Nfa3udtt91W4Jpt2rTRm2++qaCgIPXq1Uvt27fXTz/9pFtuuaVAeQAoFwYAoNTq1q1rSDJmzJhR4NzPP/9s+Pn5GZKMd955x35ckiHJCA8PN9auXVvgdRkZGUbNmjUNk8lkvPHGG0Zubq793MGDB40rrrjCkGRMmDDB4XXjxo0zJBnjxo0rtL5paWnG33//XeD477//biQkJBiSjO+//97h3Pbt2w1JRt26dQu8zvZeIiMjjXXr1jmtT5MmTQqtT2Fyc3ONxMREQ5Lx+eef24/fddddhiTjtttuc/q6GTNmFFpXVy1btsz+Pkti4cKFhiSjSpUqxjfffONw7q233jIkGYGBgcaWLVvsx999911DknH11VcbZ86ccXhNbm6ukZaWZmRnZ9uPXX755YYk44MPPihw/6NHjzr9XDljNpvtn9H33nuvRK/Jr7ivTdeuXQ1JxrJly5wel2T83//9X4HX5ebmGnXq1DEkOX0/Bw4cMAIDA42goCDj4MGD9uO//PKLERwcbFSpUsX47LPPHF6zY8cOo1WrVoYk49133y3lOwWAotFDBQDl5NixY1qwYIFSU1NlsVhUu3Zt3XjjjQXKPfzww7r00ksLHJ86daoOHTqk4cOH65577pGf37kf0TVr1tR7772nwMBAvf766zIMo1R169q1qxo0aFDgeNOmTTV27FhJ0pw5c0p1TUl6+umndckllzgcGzNmjCIiIvTnn39q165dpbreN998o127dqlWrVq65ppr7Mdtw/4+/fRTHT9+vNDX79y5s8hl00eNGlWq+pTGiy++KEm699571b17d4dzt99+u6699lqZzWZNmzbNfnzfvn2SpO7duxdY/MPPz09du3ZVUFBQgfK9evUqcP+IiAinnytnDh06ZF8G3dm8qcpwxRVX6N577y1w3M/PT0OGDJEkpz1uH374ocxms6677jrVrFnTfnzSpEnKzs7WxIkTlZqa6vCaunXr2uflvfrqq+X5NgCAVf4AoCxuvfVW+xClvBo2bKjPPvtM1apVK3CusGW9v/76a0nWIVjOxMfHq3Hjxvrtt9/0119/lXr1wOPHj2vhwoXauHGjDh48qDNnzkiSMjIyJEl//PFHqa4nyelS5sHBwWrQoIE2btyoPXv2KDExscTXe+uttyRJgwcPVkDAuf9FdejQQS1bttSWLVv0ySef2ANWfsUtm37xxReXuC6lkZOTo9WrV0uSw9y1vG6//XZ99dVXWrZsmf1Yhw4dJEnPP/+8atasqWuvvVY1atQo9D4XX3yxfvvtNw0aNEiPP/64Lr30Uoevkzcpqp2GDh2qiRMn6pNPPtHUqVNVtWpV+zlnw/0sFosWLlwoqfDvn/bt2ys0NFQbN27U6dOnVaVKlfJ4GwBAoAKAssi7D1VQUJBiYmJ06aWXqmfPnoX+olvY5rL//POPJKlz587F3vfAgQOlClRffvmlbr31Vh06dKjQMrZ5X6VRp04dp8fDw8MlWZeRL6kDBw7oiy++kFRwbozt2IMPPqh33nmn0EDlrmXTDx06ZH+v9evXd1rGtlrinj177MeSk5M1evRovfDCCxoyZIhMJpMaN26sTp06KSUlRb1793boqZw8ebJ++eUXLVy4UAsXLlTVqlV10UUXKTk5WYMGDVLz5s1LVN+aNWvKz89PFotF+/fvd/Vtl0lRmyw3aNBAXbt2VVpamubNm6eBAwdKkjZu3Kiff/5ZtWvXVo8ePezlDx06ZP/8liTAHzp0SPHx8WV7AwBwFoEKAMog/z5UJZH3r+152YZg3XDDDU57tvLKO9SpOHv27FH//v116tQpPfrooxo0aJDq1aun0NBQ+fn56ZtvvtFVV11V6mGEkhx+2S+r999/X2azWQEBARo2bFiB87ahfmvWrNHvv/+uZs2aldu93WnKlCm6++679eWXX2rVqlVavXq1ZsyYoRkzZqhDhw5atmyZ/fMQGxurDRs2aPny5fr222+1evVqff/991q9erWeffZZTZ48WaNHjy72ngEBAWrdurU2bdqk9evX65Zbbin392X7PBemsO8Dm9tuu01paWmaOXOmPVDZeqcGDx4sf39/p/eyDRcsijsW4QDguwhUAOAhEhMT9ddff2n06NFq3759uV33yy+/1KlTp3T99dfrueeeK3D+r7/+Krd7lYVtjkve4XNFlX3hhRcqo1olUrNmTQUHBys7O1v//POPWrduXaCMrQfSWc9IvXr1NGLECI0YMUKStH79et18881av369nn/+efvS+JJ1ufLk5GQlJydLsvYCzpw5U8OHD9fjjz+uG264oUR7h6WkpGjTpk365JNP9MILL5Q6ZAQGBspsNisrK0thYWEFzu/cubNU18uvb9++uu+++7R06VL7vLpZs2ZJUoFhtlFRUapatapOnTqlF198UVFRUWW6NwCUBotSAICHuPrqqyWp0H2hCmNbtCD/HlU2hw8flmSdmJ+fYRj2X1Ldae3atfrtt98UHBysI0eOyDAMpw/b/lbvv/9+oe/XHQICAuzL1Bc25NC2ZPrll19e7PU6dOhgX7Bh06ZNRZatUqWK7r77brVu3VoWi0W//PJLieo8YsQIRUREaP/+/SXq1Vq5cqXDc1sw3Lp1a4Gyv/zyS6kXJMkvJCRE/fv3l8Vi0Xvvvacvv/xShw4dUqdOnQoMd/X397cvBFLa7x8AKCsCFQB4iEceeUSRkZF6+eWX9dJLL9kXjchr+/btBTYoTUhIkCT9+uuvTq9rm1czZ84c+wIUkpSbm6unnnrKI/Z2svVOpaSkKDIystByPXr0UGxsrPbt26evvvqqkmpXMg899JAkafr06Vq6dKnDuZkzZ+qLL75QYGCgRo4caT8+b948rVixosDwOLPZrEWLFklyDMIvvvii0tPTC9z7999/t/c0OgvOzthWjvTz89O0adM0bNgwp/Op9uzZo/vuu099+vRxON6tWzdJ1g2ls7Oz7cd37NihIUOGuDSEND/bXLqZM2faA6mzRWAk655oQUFBeuSRR/Tuu+86HXK4ZcsWzZ07t8z1AoC8TEZ5/MQDgPNMvXr1tHPnTs2YMaPEc6hsm/cW9WN3xYoV6tu3rw4ePKiYmBi1bNlScXFxOnbsmLZu3aq///5bl1xyidatW2d/zb59+9SwYUOdOHFCnTp1UuPGjeXv769OnTrp1ltvVU5Oji699FL9+OOPCg0NVdeuXVWtWjV9//33+vfff/Xggw/queeesy8CYLNjxw7Vr19fdevW1Y4dO0r1XpKTk7V8+XItW7bMPjStMMePH1dcXJyOHz+ur7/+2umS4Hk99NBDevnll3Xttdfqyy+/lGT9hfvWW28tdpU/SXrjjTcUEhJSZBlJSktLs/cm5V8aPq+4uDj7JsZjx47VxIkTZTKZ1KlTJ9WpU0e///67fvrpJ/n7++vNN990WHBj1KhRmjZtmqKiotS2bVvFxMQoKytL69at0/79+xUfH69169bZQ3NkZKSOHTumZs2aqXnz5qpatar+/fdfrVq1Sjk5ORo8eHCRG+I6s3DhQg0ePFgHDx5UQECA2rdvr7p16yonJ0d///23fv75ZxmGoUsvvVRr1661v2779u266KKLdPToUdWpU0cdOnTQgQMHtH79enXq1EknT57UmjVrCnwGSvPZkKQWLVrYe8GqVaumvXv3KjQ01GnZ2bNna+jQoTp58qQSEhLUokULRUdH6/Dhw9q8ebN2796t/v376+OPPy7V1wgAiuSGva8AwOsVtbFvYVTCTWL37dtnjB071rjooouMsLAwIygoyEhISDA6duxojBs3zvjll18KvGbFihVGt27djOrVq9s3bB0yZIj9fFZWlvH4448bTZs2NapUqWLExMQYffr0MTZs2GDfwLZr164O1yzJxr6FKWxTV2fefvttQ5IRGxtr5OTkFFt+06ZNhiTD39/f2LNnj2EY5zb2LcnjyJEjxd7DMBw39i3qkf/rs3DhQqNXr15GzZo1jYCAACM2Ntbo169fgY2TDcMwNm7caDz22GPGZZddZsTHxxtBQUFGdHS00a5dO+PZZ5912LjWMAzjgw8+MG699VajZcuWRo0aNYzg4GCjbt26xtVXX23MmzfPsFgsJXpv+WVlZRmvvPKK0b17dyM2NtYICgoyQkJCjCZNmhg333yz8dVXXzm99m+//WakpqYa1atXN4KDg42mTZsaEydONM6cOVPsxr4l+WwYhmE8//zz9q913s90YbZv32488MADRsuWLY1q1aoZVapUMerWrWskJycbU6ZMMbZt21ai+wJASdFDBQAAAAAuYg4VAAAAALiIQAUAAAAALiJQAQAAAICLPD5QTZkyRSaTSaNGjSqy3OzZs9WsWTNVqVJFrVq1su9VAgAAAAAVxaMD1fr16/Xf//7X6Y7zea1Zs0YDBgzQ7bffro0bN6pPnz7q06ePtmzZUkk1BQAAAHA+8thV/o4fP66LLrpIb7zxhiZOnKgLL7xQU6dOdVq2f//+OnHihMMmj5deeqkuvPBC/ec//6mkGgMAAAA43wS4uwKFGT58uK655hp169ZNEydOLLLs2rVr9eCDDzocu+qqq/T5558X+prs7GyHnd0tFosOHz6smjVr2jesBAAAAHD+MQxDWVlZql27tvz8ih7U55GB6uOPP9ZPP/2k9evXl6j83r17VatWLYdjtWrV0t69ewt9zeTJkzVhwoQy1RMAAACA79q1a5cSEhKKLONxgWrXrl0aOXKklixZoipVqlTYfcaMGePQq3Xs2DHVqVNH27dvV1hYWIXdtyhms1nLli3T5ZdfrsDAQLfUobz5vfii/KdMUeYNQ5Q451W9rFG6XTNkVK0q06lTWlRzoPofmq733stRr14eOfq0zHyxXUG7+ira1ffQpr6JdvVNntSuWVlZql+/folygccFqh9//FH79+/XRRddZD+Wm5urFStW6PXXX1d2drb8/f0dXhMbG6t9+/Y5HNu3b59iY2MLvU9wcLCCg4MLHK9Ro4bCw8PL+C5cYzabFRISopo1a7r9Q1Ruzn4tA4MCJIUrUobCJSkkRDp1StX8AiWFKzxcqlnTjfWsQD7ZrqBdfRTt6ntoU99Eu/omT2pX2/1LMhXI41b5u/LKK7V582Zt2rTJ/mjfvr0GDRqkTZs2FQhTkpSUlKSlS5c6HFuyZImSkpIqq9ooTIA1s/sZOZKkYJ2dt1a16tnjFrdUCwAAACgPHtdDFRYWppYtWzocq1atmmrWrGk/PnjwYMXHx2vy5MmSpJEjR6pr16566aWXdM011+jjjz/Whg0b9Oabb1Z6/ZHP2UDlb8kXqEJCJEl+ynVLtQAAAIDy4HE9VCWRnp6ujIwM+/OOHTtq1qxZevPNN9WmTRvNmTNHn3/+eYFgBjew9VDlD1T5eqg8c/F+AAAAoGge10PlTFpaWpHPJalfv37q169f5VQIJXc2UJkKC1RiyB8AALDKzc2V2WwutpzZbFZAQIBOnz6t3FxGu/iKym7XwMBAp9OJSssrAhW8mC1Q5eQLVGdXcDQRqAAAgKTjx49r9+7dMkowbMUwDMXGxmrXrl3sH+pDKrtdTSaTEhISFBoaWqbrEKhQsc4GKp0NVEE6Y33OohQAAOCs3Nxc7d69WyEhIYqOji72l2mLxaLjx48rNDS02E1X4T0qs10Nw9CBAwe0e/duNW7cuEw9VQQqVKx8gYpFKQAAQH5ms1mGYSg6OlpVz/7RtSgWi0VnzpxRlSpVCFQ+pLLbNTo6Wjt27JDZbC5ToOITiIpVWKBiUQoAAJAPw/dQmcrr80agQsUqJlAxhwoAAADejECFilVcDxWBCgAAAF6MQIWKVeyQP+ZQAQCA8pGbK6WlSR99ZP2vN66oXq9ePU2dOrXE5dPS0mQymXT06NEKqxOKRqBCxSrhHCoAAICymDtXqldPuvxyaeBA63/r1bMerwgmk6nIx/jx41267vr163XnnXeWuHzHjh2VkZGhiIgIl+5XUgS3wrHKHypWCedQsSgFAABw1dy50g03FPx9Ys8e6/E5c6TU1PK9Z0ZGhv3fn3zyiZ566in98ccf9mN59zYyDEO5ubkKCCj+V+/o6OhS1SMoKEixsbGleg3KFz1UqFgOgcpQMPtQAQCAYhiGdOJEyR6ZmdL99zv/46zt2MiR1nIluV5J/8gbGxtrf0RERMhkMtmf//777woLC9PChQvVrl07BQcHa9WqVfr777+VkpKiWrVqKTQ0VB06dNC3337rcN38Q/5MJpPeeustXX/99QoJCVHjxo31xRdf2M/n7zmaOXOmIiMjtXjxYjVv3lyhoaHq2bOnQwDMycnR/fffr8jISNWsWVOjR4/WkCFD1KdPn5K9eSeOHDmiwYMHq3r16goJCdHVV1+tv/76y35+586d6t27t6pXr65q1arpggsu0IIFC+yvHTRokGrVqqW4uDg1bdpUM2bMcLkulY1AhYplC1RmswJlPnecRSkAAEAhTp6UQkMLf4SH+ykhIVLh4X6KiLD2RBXGMKTdu6WIiKKvaXucPFl+7+Oxxx7TlClTtHXrVrVu3VrHjx9Xr169tHTpUm3cuFE9e/ZU7969lZ6eXuR1JkyYoBtvvFG//PKLevXqpUGDBunw4cOFlj958qRefPFFvf/++1qxYoXS09P18MMP288/99xz+vDDDzVjxgytXr1amZmZ+vzzz8v0XocOHaoNGzboiy++0Nq1a2UYhnr16iWz2fr73/Dhw5Wdna0VK1Zo8+bNeu655+y9eGPHjtVvv/2mr7/+Wt9//73+7//+T1FRUWWqT2ViyB8qVp4eKvtwP4mNfQEAgM97+umn1b17d/vzGjVqqE2bNvbnzzzzjObNm6cvvvhC9913X6HXGTp0qAYMGCBJevbZZ/Xqq6/qhx9+UM+ePZ2WN5vN+s9//qOGDRtKku677z49/fTT9vOvvfaaxowZo+uvv16S9Prrr9t7i1zx119/6YsvvtDq1avVsWNHSdKHH36oxMREff755+rXr5/S09PVt29ftWrVSpLUoEED++vT09PVtm1btW/fXpmZmWrZsqVXbdjsPTWFdyosUFWpIkkyMeQPAADkExIiHT9e+CMz06Ldu48qM9OikuaABQuKvqbtcfZvvuWiffv2Ds+PHz+uhx9+WM2bN1dkZKRCQ0O1devWYnuoWrdubf93tWrVFB4erv379xdaPiQkxB6mJCkuLs5e/tixY9q3b58uvvhi+3l/f3+1a9euVO8tr61btyogIECXXHKJ/VjNmjXVtGlTbd26VZJ0//33a+LEierUqZPGjRunX375xV72nnvu0ccff6yLLrpITz31lNasWeNyXdyBQIWKlSdQVTkbqHLkLwUGSmLIHwAAKMhkkqpVK9mjRw8pIcH6msKulZhoLVeS6xV2HVdUq1bN4fnDDz+sefPm6dlnn9XKlSu1adMmtWrVSmfOnCnyOoFnf286955MslgK/x3KWXnDzSuADRs2TP/8849uueUWbd68We3bt9drr70mSbr66qu1c+dOjRw5Unv37lX37t0dhih6OgIVKlaeQBUWZA1U2QqWznbj2nqoWOUPAAC4wt9fmjbN+u/8Ycj2fOpUazl3W716tYYOHarrr79erVq1UmxsrHbs2FGpdYiIiFCtWrW0fv16+7Hc3Fz99NNPLl+zefPmysnJ0ffff28/dujQIf3xxx9q0aKF/VhiYqLuvvtuzZ07Vw899JD+97//2c9FR0dryJAhevPNN/Xyyy/rzTffdLk+lY05VKhYeQJVtYBs6Yw1UFU7G6j8mUMFAADKKDXVujT6yJHWBShsEhKsYaq8l0x3VePGjTV37lz17t1bJpNJY8eOLbKnqaKMGDFCkydPVqNGjdSsWTO99tprOnLkiEwl6J7bvHmzwsLC7M9NJpPatGmjlJQU3XHHHfrvf/+rsLAwPfbYY4qPj1dKSookadSoUbr66qvVpEkTHTlyRMuWLVPz5s0lSU899ZTatWun5s2b69ChQ/r666/t57wBgQoVK3+g0tkeqrN/JmIOFQAAKA+pqVJKirRypZSRIcXFSZ07e0bPlM3LL7+s2267TR07dlRUVJRGjx6tzMzMSq/H6NGjtXfvXg0ePFj+/v668847ddVVV8m/BF+sLl26ODz39/dXTk6OZsyYoZEjR+raa6/VmTNn1KVLFy1YsMA+/DA3N1fDhw/X7t27FR4erp49e+qVV16RZN1La8yYMdqxY4eqVKmizp076+OPPy7/N15BCFSoWLYxvDk5Cgu2rkMaoBxp82ZJzKECAADlx99fSk6u/PsOHTpUQ4cOtT9PTk52OmepXr16+u677xyODR8+3OF5/iGAzq5j23PK2b3y10WS+vTp41AmICBAr732mn0Ok8ViUfPmzXXjjTc6fX9FvSeb6tWr67333iv0vO1ezjz55JN68sknZbFYlJmZqfDwcK9a5Y9AhYpl66E6c0YzT/eRJEXroHR2omGo+YibKgYAAHB+2rlzp7755ht17dpV2dnZev3117V9+3YNHDjQ3VXzSt4T/eCd8gz5q2k5UOB0rTO7dL3msigFAABAJfHz89PMmTPVoUMHderUSZs3b9a3337rVfOWPAk9VKhYeSY3FjbNcapG6YfcFEkeNMgZAADARyUmJmr16tXurobPoIcKFSvPkpzOmCTV0S5F/76ycuoDAAAAlCMCFSrWgYLD/JypciSjgisCAAAAlD8CFSpWfHyJip2uHlfBFQEAAADKH3OoULG6di3ytCFplxK1v1nnyqkPAAAAUI7ooUKFyvUPsv+7sIX8RmmqLCxIAQAAAC9EoEKFWrnKpJyzYSnXSWg6rOqap1Rt3VrZNQMAAADKjkCFCpWRIeXYR5Za+6ie1NO6TW9Jks4oWJJ0hP19AQBAWeXmSmlp0kcfWf+bm+vuGhUrOTlZo0aNsj+vV6+epk6dWuRrTCaTPv/88zLfu7yuc74jUKFCxcWdC1QBskiSZug2/aBLJEl+Z49Vr+6e+gEAAB8xd65Ur550+eXSwIHW/9arZz1eAXr37q2ePXs6Pbdy5UqZTCb98ssvpb7u+vXrdeedd5a1eg7Gjx+vCy+8sMDxjIwMXX311eV6r/xmzpypyMjICr2HuxGoUKE6d5ZyTefWPsmVn/aplixnP3r+sv7lqFkzt1QPAAD4grlzpRtukHbvdjy+Z4/1eAWEqttvv11LlizR7vz3lDRjxgy1b99erVu3LvV1o6OjFRISUh5VLFZsbKyCg4Mr5V6+jECFCuXvL1UJPReo9ipWuQqwz6ey9VD58UkEAAA2hiGdOFGyR2amdP/91tc4u44kjRxpLVeS6zm7jhPXXnutoqOjNXPmTIfjx48f1+zZs3X77bfr0KFDGjBggOLj4xUSEqJWrVrpo48+KvK6+Yf8/fXXX+rSpYuqVKmiFi1aaMmSJQVeM3r0aDVp0kQhISFq0KCBxo4dK7PZLMnaQzRhwgT9/PPPMplMMplM9jrnH/K3efNmXXHFFapatapq1qypO++8U8ePH7efHzp0qPr06aMXX3xRcXFxqlmzpoYPH26/lyvS09OVkpKi0NBQRUZG6tZbb9W+ffvs53/++WddfvnlCgsLU3h4uNq1a6cNGzZIknbu3KnevXurevXqqlatmi644AItWLDA5bq4imXTUeGCQwKkLOu/98i6L5WthyrAz6KzmQoAAMDq5EkpNLTQ036SIkt6LcOw9lxFRJSs/PHjUrVqxRYLCAjQ4MGDNXPmTD3xxBMymUySpNmzZys3N1cDBgzQ8ePH1a5dO40ePVrh4eH6+uuvdcstt6hhw4a6+OKLi72HxWJRamqqatWqpe+//17Hjh1zmG9lExYWppkzZ6p27dravHmz7rjjDoWFhenRRx9V//79tWXLFi1atEjffvutJCnCydfixIkTuuqqq5SUlKT169dr//79GjZsmO677z6H0Lhs2TLFxcVp2bJl2rZtm/r3768LL7xQd9xxR7Hvx9n7s4Wp5cuX68yZM7r33ns1YMAApaWlSZIGDRqktm3bavr06fL399emTZsUGBgoSRo+fLjOnDmjFStWqFq1avrtt98UWsTnpqIQqFDxAs59zOp3TtCse6T6Fj/pZsnfRJoCAADe6bbbbtMLL7yg5cuXKzk5WZJ1uF/fvn0VERGhiIgIPfzww/byI0aM0OLFi/Xpp5+WKFB9++23+v3337V48WLVrl1bkvTss88WmPf05JNP2v9dr149Pfzww/r444/16KOPqmrVqgoNDVVAQIBiY2MLvdesWbN0+vRpvffee6p2NlC+/vrr6t27t5577jnVqlVLklS9enW9/vrr8vf3V7NmzXTNNddo6dKlLgWqpUuXavPmzdq+fbsSExNlsVg0ffp0e6jr0KGD0tPT9cgjj6jZ2fkhjRs3tr8+PT1dffv2VatWrSRJDRo0KHUdygMDrVDx8gSq6DbxGjBAurSj9aPnZ3j+6jsAAKCShYRYe4oKeVgyM3V0925ZMjOlkg7xWrCgyGvaH6WYv9SsWTN17NhR77zzjiRp27ZtWrlypW6//XZJUm5urp555hm1atVKNWrUUGhoqBYvXqz09PQSXX/r1q1KTEy0hylJSkpKKlDuk08+UadOnRQbG6vQ0FA9+eSTJb5H3nu1adPGHqYkqVOnTrJYLPrjjz/sxy644AL5+5/bCicuLk779+8v1b3y3jMxMVGJiYn2Y82aNVNkZKS2nt1T58EHH9SwYcPUrVs3TZkyRX///be97P3336+JEyeqU6dOGjdunEuLgJQHAhUqXp5ApXjrkD/5O86hAgAAsDOZrMPuSvLo0UNKSLC+prBrJSZay5XkeoVdpxC33367PvvsM2VlZWnGjBlq2LChunbtKkl64YUXNG3aNI0ePVrLli3Tpk2bdNVVV+nMmTNl/QrZrV27VoMGDVKvXr301VdfaePGjXriiSfK9R552Ybb2ZhMJlksFff73Pjx4/Xrr7/qmmuu0XfffacWLVpo3rx5kqRhw4bpn3/+0S233KLNmzerffv2eu211yqsLoUhUKHiOQtUZ1ehMBnWb8ASzv8EAABw5O8vTZtm/Xf+MGR7PnWq/Y+55e3GG2+Un5+fZs2apffee0+33XabfT7V6tWrlZKSoptvvllt2rRRgwYN9Oeff5b42s2bN9euXbuUkZFhP7Zu3TqHMmvWrFHdunX1xBNPqH379mrcuLF27tzpUCYoKEi5xezJ1bx5c/388886ceKE/djq1avl5+enpk2blrjOpWF7f7t27bIf+/3333X06FG1aNHCfqxJkyZ64IEH9M033yg1NVUzZsywn0tMTNTdd9+tuXPn6qGHHtL//ve/CqlrUQhUqHhFBCp6qAAAQJmlpkpz5pz7PcMmIcF6PDW1wm4dGhqq/v37a8yYMcrIyNDQoUPt5xo3bqwlS5ZozZo12rp1q+666y6HFeyK061bNzVp0kRDhgzRzz//rJUrV+qJJ55wKNO4cWOlp6fr448/1t9//61XX33V3oNjU69ePW3fvl2bNm3SwYMHlZ2dXeBegwYNUpUqVTRkyBBt2bJFy5Yt04gRI3TLLbfY50+5Kjc3V5s2bXJ4bN26Vd26dVOrVq00aNAg/fTTT/rhhx90zz33qGvXrmrfvr1OnTql++67T2lpadq5c6dWr16t9evXq3nz5pKkUaNGafHixdq+fbt++uknLVu2zH6uMhGoUPFK0EMFAABQJqmp0o4d0rJl0qxZ1v9u316hYcrm9ttv15EjR3TVVVc5zHd68sknddFFF+mqq65ScnKyYmNj1adPnxJf18/PT/PmzdOpU6d08cUXa9iwYZo0aZJDmeuuu04PPPCA7rvvPl144YVas2aNxo4d61Cmb9++6tmzpy6//HJFR0c7Xbo9JCREixcv1uHDh9WhQwfdcMMNuvLKK/X666+X7ovhxPHjx9W2bVuHR+/evWUymTR//nxVr15dXbp0UY8ePVSvXj17/fz9/XXo0CENHjxYTZo00Y033qirr75aEyZMkGQNasOHD1fz5s3Vs2dPNWnSRG+88UaZ61taJsNgsJUkZWZmKiIiQseOHVN4eLhb6mA2m7VgwQL16tWrwPhUr9a+vfTjj9Z/Z2VZl0Hdv186+9cOkyz6+GOT+vd3Yx0rkM+263mOdvVNtKvvoU29w+nTp7V9+3bVr19fVapUKba8xWJRZmamwsPD5cdmlj6jstu1qM9dabIBn0BUPFsPVXj4uT0l8oxjNolMDwAAAO9EoELFys217jouSdWrW59L9iF/knUeFf2kAAAA8EYEKlScuXOlevWkLVusz3futD6fO7dAoAIAAAC8UUDxRQAXzJ0r3XBDwfXQ9+yxHn//ffshf7G5LwAAALwTPVQof7m50siRzjeXsh177DH7IXqoAAAA4K0IVCh/K1dKu3cXft4wHM4TqAAAAOCtCFQof3l28y4JFqUAAACAtyJQofzFxZWqOD1UAAAA8FYsSoHy17mzlJBgXYDCWdeTyWQ9v2uXJBalAAAAZXc6N1ezDxzQ5wcP6pDZrJqBgeoTFaV+0dGqkmf/S6C80UOF8ufvL02bZv23yeR4zvZ86lT7IXqoAABAWXxx8KBqr12rwb//rs8PHtTyY8f0+cGDGvz776q9dq2+PHjQ3VU8b9SrV09T8/yedz4gUKFipKZKc+ZI8fGOxxMSrMdTU+17URGoAACAq744eFB9tmzR0ZwcSbL/VmH779GcHKVs2aIvKiBUDR06VCaTyf6oWbOmevbsqV9++aXc7jF+/HhdeOGFJSqXty62R7NmzcqtLhVl7ty56t69u2rVqqU6deqoU6dOWrx4sUOZrKwsjRo1SnXr1lXVqlXVsWNHrV+/vsB1evTooZo1a8pkMmnTpk2VUn+PDFTTp09X69atFR4ervDwcCUlJWnhwoVFvmbq1Klq2rSpqlatqsTERD3wwAM6ffp0JdUYTqWmSjt2SMuWSbNmWf+7fbv1uESgAgAAZXLaYtHQ33+XJBW2vpXt+NDff9fp3PKfZtCzZ09lZGQoIyNDS5cuVUBAgK699tpyv09JXHDBBfa62B6rVq1yS11KY8WKFerevbu++uorLVu2TMnJyerdu7c2btxoLzNs2DAtWbJE77//vjZv3qwePXqoW7du2rNnj73MiRMndNlll+m5556r1Pp7ZKBKSEjQlClT9OOPP2rDhg264oorlJKSol9//dVp+VmzZumxxx7TuHHjtHXrVr399tv65JNP9Pjjj1dyzVGAv7+UnCwNGGD9b94xzGcDlb9yWeUPAACU2uwDB3QkJ6fQMGVjSDqSk6M5Bw6Uex2Cg4MVGxur2NhYXXjhhXrssce0a9cuHchzr127dunGG29UZGSkatSooZSUFO3YscN+Pi0tTRdffLGqVaumyMhIderUSTt37tTMmTM1YcIE/fzzz/Yep5kzZxZal4CAAHtdbI+oqCj7+Xr16umZZ57RgAEDVK1aNcXHx+v//u//HK6Rnp6ulJQUhYaGKjw8XDfeeKP27dvnUObLL79Uhw4dVKVKFUVFRen66693OH/y5EnddtttCgsLU506dfTmm28W+TWcOnWqHn30UXXo0EENGzbUpEmT1LhxY3355ZeSpFOnTumzzz7T888/ry5duqhRo0YaP368GjVqpOnTp9uvc8stt+ipp55St27dirxfefPIRSl69+7t8HzSpEmaPn261q1bpwsuuKBA+TVr1qhTp04aOHCgJOuHZcCAAfr+++8LvUd2drays7PtzzMzMyVJZrNZZrO5PN5Gqdnu6677V7YAf3+ZZO2hysnJkdnsm6nqfGvX8wXt6ptoV99Dm3oHs9kswzBksVhksRQ/csU4+5fY+QcPyk8q0VgXP0lzDx7UwJiYMtU1fz1s9Zak48eP6/3331ejRo1UvXp1WSwWmc1mXXXVVbr00ku1fPlyBQQEaNKkSerZs6c2bdokPz8/9enTR8OGDdOHH36oM2fO6IcffpBhGOrXr582b96sxYsX65tvvpEkRUREOP0a2b4mxX39XnjhBY0ZM0bjxo3TN998o5EjR6pRo0bq3r27LBaLPUwtW7ZMOTk5GjFihPr376/vvvtOkvT111/r+uuv1+OPP66ZM2fqzJkzWrhwocN9X3rpJT399NN67LHH9Nlnn+mee+5R586d1bRpU0nSFVdcobp162rGjBlO30Nubq6ysrLsX8MzZ84oNzdXQUFBDvepWrWqVq1aVeA9254X93myWCwyDENms1n++RYuKc3PDI8MVHnl5uZq9uzZOnHihJKSkpyW6dixoz744AP98MMPuvjii/XPP/9owYIFuuWWWwq97uTJkzVhwoQCx7/55huFhISUW/1dsWTJErfev7JcYxgKkDVQbdq0SRERe4p9jTc7X9r1fEO7+iba1ffQpp7N1rNy/PhxnTlzpsSv25+dXeKJAxZJB06ftv8RvTyYzWZ9/fXXCg8Pl2QdchYbG6uPP/5Yx48flyR98sknysnJ0UsvvSTT2cW5pk6dqnr16mnBggVq27atjh07pssvv1zR0dGSZO/xMZvNCgwMlMlksv9+Wtgf/7Ozs7V582Z7XWz69eunV155xfo1sFh08cUX65577pEkDR48WGlpaXrxxRd1ySWXaNmyZdq8ebM2bdqkhIQESdLrr7+upKQkpaWl6aKLLtIzzzyj1NRUPfjgg/Z73Hvvvfavq8ViUbdu3TRo0CBJ0t13361XXnlFCxcuVNzZrXViY2NVo0aNQtvi2WefVVZWlnr27Gkv06FDB02YMEEJCQmKiYnRnDlztHbtWjVo0KDAdWxf+xMnThTZ3mfOnNGpU6e0YsUK5Zydg2dz8uTJQl+Xn8cGqs2bNyspKUmnT59WaGio5s2bpxYtWjgtO3DgQB08eFCXXXaZDMNQTk6O7r777iKH/I0ZM8bhg5CZmanExET16NGjwAexspjNZi1ZskTdu3dXYGCgW+pQmfwDA6XTp+Uniy688EL16tXG3VWqEOdbu54vaFffRLv6HtrUO5w+fVq7du1SaGioqlSpUmx5wzCUlZWlmOBg+R0/XuIequgqVcr197zAwEAlJyfrjTfekCQdOXJE06dP14033qh169apbt26+uuvv/TPP/8oMTHR4bWnT59WRkaG+vTpoyFDhqhv377q1q2bunXrpn79+tnDR3BwsPz9/Yutd3BwsJo2barPP//c4bhtTQJJ8vPzU+fOnR2u1aVLF02bNk3h4eFKT09XYmKiw+/cF198sSIjI5Wenq7k5GRt2bJFd911V6H18fPzU7t27RzOx8XFKSsry35s1qxZTl9rGIbeeecdPf/885o3b54aNmxoP/fhhx9q2LBhatGihfz9/XXRRRfppptu0k8//VSgLqGhoZKkatWqFfl1O336tKpWraouXboU+NyVJnh7bKBq2rSpNm3apGPHjmnOnDkaMmSIli9f7jRUpaWl6dlnn9Ubb7yhSy65RNu2bdPIkSP1zDPPaOzYsU6vHxwcrODg4ALHAwMD3f4D1xPqUCnyLEoREBAgX3/L5027nmdoV99Eu/oe2tSz5ebmymQyyc/PT35+xU/xtw3jSomK0rxDh0p0D4uk1KioEl2/pEwmk0JDQ9WkSRP7sfbt2ysiIkJvv/22Jk6cqBMnTqhdu3b68MMPC7w+Ojpafn5+mjlzpkaOHKlFixbp008/1dixY7VkyRJdeuml9l6t4uptMpkUFBTkUJfCyuW9Vt7rF3UvW9tUrVq12HYKCgoqcA/DMIp9D7NmzdLIkSP1ySefqEePHg7nGjdurOXLl9t7neLi4tS/f381aNCgwHVtz4urp+09O/v5UJqfFx65KIVkbYhGjRqpXbt2mjx5stq0aaNptr2N8hk7dqxuueUWDRs2TK1atdL111+vZ599VpMnTy7ROFy4CYtSAACAMugXHa3qAQEyFVPOJKl6QIBuODukriLZAsupU6ckSRdddJH++usvxcTEqFGjRg6PiIgI++vatm2rMWPGaM2aNWrZsqW9FycoKEi55bg64bp16wo8b968uSSpefPm2rVrl3bt2mU//9tvv+no0aP2To3WrVtr6dKl5VYfm48++ki333673nrrLV1zzTWFlqtWrZri4uJ05MgRLV68WCkpKeVel9Ly2B6q/CwWi8MiEnmdPHmyQPq0TSwz+E3dc51tI5ZNBwAArqji56d3mzVTypYtMsn50um2sPVus2aqkm/hgfKQnZ2tvXv3SrIO+Xv99dd1/Phx+yJrgwYN0gsvvKCUlBQ9/fTTSkhI0M6dOzV37lw9+uijMpvNevPNN3Xdddepdu3a+uOPP/TXX39p8ODBkqyLrW3fvt0+ryksLMzpKCtJysnJsdfF/v5NJtWqVcv+fPXq1Xr++efVp08fLVmyRLNnz9bXX38tSerWrZtatWqlQYMGaerUqcrJydG9996rrl27qn379pKkcePG6corr1TDhg110003KScnRwsWLNDo0aNL/DUbPHiw4uPjNXnyZEnWnqkhQ4Zo6tSpateunfbu3WvvDbOFzsWLF8swDDVt2lTbtm3TI488ombNmunWW2+1X/fw4cNKT0/Xv//+K0n6448/JMm+4mFF8cgeqjFjxmjFihXasWOHNm/erDFjxigtLc0+uW3w4MEaM2aMvXzv3r01ffp0ffzxx9q+fbuWLFmisWPHqnfv3gVW7IAHYR8qAABQRr2jovR5y5aKDLD2E9h+ubX9NzIgQPNbtlTvPMuHl6dFixYpLi5OcXFxuuSSS7R+/XrNnj1bycnJkqSQkBCtWLFCderUUWpqqpo3b67bb79dp0+fVnh4uEJCQvT777+rb9++atKkie68804NHz5cd911lySpb9++6tmzp33Rio8++qjQuvz666/2utgedevWdSjz0EMPacOGDWrbtq0mTpyol19+WVdddZUka/iaP3++qlevri5duqhbt25q0KCBPvnkE/vrk5OTNXv2bH3xxRe68MILdcUVV+iHH34o1dcsPT1dGRkZ9udvvvmmcnJydN9996lZs2aKj49XXFycRo4caS9z7NgxDR8+XM2aNdPgwYN12WWXafHixQ5D87744gu1bdvW3sN10003qW3btvrPf/5TqvqVlkf2UO3fv1+DBw9WRkaGIiIi1Lp1ay1evFjdu3eXZG2EvD1STz75pEwmk5588knt2bNH0dHR6t27tyZNmuSut4CSIFABAIBycF1UlP5NStKcAwc07+BBHTabVSMwUNdHRemG6OgK6ZmSpJkzZxa5L5RNbGys3n33XafnwsPDNW/evEJfGxwcrDlz5hR7j/Hjx2v8+PHFlgsPD9enn35a6Pk6depo/vz5RV4jNTVVqampTs/l3V/LZtOmTQ7P09LSnD63WCzKzMxUeHh4gdFnN954o2688cYi6zV06FANHTq0yDIVwSMD1dtvv13k+fyNEBAQoHHjxmncuHEVWCuUuzxzqAAAAMqiir+/bo6N1c0VOLQLcMYjh/zhPJFnDhVT3QAAAOCNPLKHCucJhvwBAABUGmfD8VB29FDBfQhUAAAA8HIEKrgPgQoAAOTBdjeoTOX1eSNQwX1YlAIAAOjc/qFnzpxxc01wPrF93sq6zRJzqOA+LEoBAABkXbE5JCREBw4cUGBgYIEls/OzWCw6c+aMTp8+XWxZeI/KbFeLxaIDBw4oJCREAQFli0QEKrgPQ/4AAICsG8rGxcVp+/bt2rlzZ7HlDcPQqVOnVLVqVZlMpkqoISpDZbern5+f6tSpU+Z7EajgPgQqAABwVlBQkBo3blyiYX9ms1krVqxQly5dFBgYWAm1Q2Wo7HYNCgoql54wAhXchzlUAAAgDz8/P1WpUqXYcv7+/srJyVGVKlUIVD7EW9uVQadwH3qoAAAA4OUIVHCfPItSAAAAAN6IQAX3ydNDxSp/AAAA8EYEKrgPQ/4AAADg5QhUcB8WpQAAAICXI1DBfZhDBQAAAC9HoIL7MOQPAAAAXo5ABfdhUQoAAAB4OQIV3Ic5VAAAAPByBCq4D0P+AAAA4OUIVHAfFqUAAACAlyNQwX3ooQIAAICXI1DBfViUAgAAAF6OQAX3YVEKAAAAeDkCFdyHOVQAAADwcgQquA9zqAAAAODlCFRwHwIVAAAAvByBCu7DohQAAADwcgQquA+LUgAAAMDLEajgPixKAQAAAC9HoIL7MIcKAAAAXo5ABfchUAEAAMDLEajgPsyhAgAAgJcjUMF98syhYpU/AAAAeCMCFdyHIX8AAADwcgQquA+BCgAAAF6OQAX3IVABAADAyxGo4D4sSgEAAAAvR6CC+7AoBQAAALwcgQruw5A/AAAAeDkCFdyHQAUAAAAvR6CC+zCHCgAAAF6OQAX3yTOHCgAAAPBGBCq4T54hfyxKAQAAAG9EoIL7MIcKAAAAXo5ABfchUAEAAMDLEajgPixKAQAAAC9HoIL7sCgFAAAAvByBCu7DkD8AAAB4OQIV3IdV/gAAAODlPDJQTZ8+Xa1bt1Z4eLjCw8OVlJSkhQsXFvmao0ePavjw4YqLi1NwcLCaNGmiBQsWVFKN4RLmUAEAAMDLBbi7As4kJCRoypQpaty4sQzD0LvvvquUlBRt3LhRF1xwQYHyZ86cUffu3RUTE6M5c+YoPj5eO3fuVGRkZOVXHiXHHCoAAAB4OY8MVL1793Z4PmnSJE2fPl3r1q1zGqjeeecdHT58WGvWrFFgYKAkqV69epVRVZQFc6gAAADg5TwyUOWVm5ur2bNn68SJE0pKSnJa5osvvlBSUpKGDx+u+fPnKzo6WgMHDtTo0aPlf7YXJL/s7GxlZ2fbn2dmZkqSzGazzGZz+b+RErDd1133r2x+hiF/WQNVbm6OzGbfnEh1vrXr+YJ29U20q++hTX0T7eqbPKldS1MHjw1UmzdvVlJSkk6fPq3Q0FDNmzdPLVq0cFr2n3/+0XfffadBgwZpwYIF2rZtm+69916ZzWaNGzfO6WsmT56sCRMmFDj+zTffKCQkpFzfS2ktWbLErfevLPW3blVrWQPVL79s0YIFO91dpQp1vrTr+YZ29U20q++hTX0T7eqbPKFdT548WeKyJsPwzPXVzpw5o/T0dB07dkxz5szRW2+9peXLlzsNVU2aNNHp06e1fft2e4/Uyy+/rBdeeEEZGRlOr++shyoxMVEHDx5UeHh4xbypYpjNZi1ZskTdu3e3D130ZX7Tp8t/5EjN1g06+MZHGjbMIz+KZXa+tev5gnb1TbSr76FNfRPt6ps8qV0zMzMVFRWlY8eOFZsNPLaHKigoSI0aNZIktWvXTuvXr9e0adP03//+t0DZuLg4BQYGOgzva968ufbu3aszZ84oKCiowGuCg4MVHBxc4HhgYKDbG9AT6lApzraLnyzy9w+Qr7/l86ZdzzO0q2+iXX0PbeqbaFff5AntWpr7e+Sy6c5YLBaHHqW8OnXqpG3btsliObe4wZ9//qm4uDinYQoegkUpAAAA4OU8MlCNGTNGK1as0I4dO7R582aNGTNGaWlpGjRokCRp8ODBGjNmjL38Pffco8OHD2vkyJH6888/9fXXX+vZZ5/V8OHD3fUWUBIEKgAAAHg5jxzyt3//fg0ePFgZGRmKiIhQ69attXjxYnXv3l2SlJ6eLj+/c1kwMTFRixcv1gMPPKDWrVsrPj5eI0eO1OjRo931FlASeTb29cyZfAAAAEDRPDJQvf3220WeT0tLK3AsKSlJ69atq6AaoUKwsS8AAAC8nEcO+cN5giF/AAAA8HIEKrgPgQoAAABejkAF9yFQAQAAwMsRqOA+LEoBAAAAL0eggvuwKAUAAAC8HIEK7sOQPwAAAHg5AhXch0AFAAAAL0eggvvkmUMFAAAAeCMCFdyHOVQAAADwcgQquE+eIX+s8gcAAABvRKCC+zCHCgAAAF6OQAX3IVABAADAyxGo4D4sSgEAAAAvR6CC+7AoBQAAALwcgQruw6IUAAAA8HIEKrgPc6gAAADg5QhUcB/mUAEAAMDLEajgPvRQAQAAwMsRqOA+LEoBAAAAL0eggvuwKAUAAAC8HIEK7sOQPwAAAHg5AhXch0UpAAAA4OUIVHAf5lABAADAyxGo4D4M+QMAAICXI1DBfViUAgAAAF6OQAX3YQ4VAAAAvByBCu7DkD8AAAB4OQIV3IdFKQAAAODlCFRwH3qoAAAA4OUIVHAfAhUAAAC8HIEK7pNnUQpW+QMAAIA3IlDBfZhDBQAAAC9HoIL72HuoCFQAAADwTgQquI9fno8fY/4AAADghQhUcJ88gcpkYXNfAAAAeB8CFdwnbw+VhWF/AAAA8D4EKrjP2UUpJMlkEKgAAADgfQhUcJ+8Q/4IVAAAAPBCBCq4D4EKAAAAXo5ABfdhUQoAAAB4OQIV3CfPHCoWpQAAAIA3IlDBfRjyBwAAAC9HoIL7mEzn/kmgAgAAgBciUMF9TCZZZA1VzKECAACANyJQwa0Mk/UjSA8VAAAAvBGBCm5lMVkXpiBQAQAAwBsRqOBWth4qVvkDAACANyJQwa0sYsgfAAAAvBeBCm5ln0PFohQAAADwQgQquBVzqAAAAODNPDJQTZ8+Xa1bt1Z4eLjCw8OVlJSkhQsXlui1H3/8sUwmk/r06VOxlUS5YJU/AAAAeDOPDFQJCQmaMmWKfvzxR23YsEFXXHGFUlJS9Ouvvxb5uh07dujhhx9W586dK6mmKCsWpQAAAIA388hA1bt3b/Xq1UuNGzdWkyZNNGnSJIWGhmrdunWFviY3N1eDBg3ShAkT1KBBg0qsLcqCRSkAAADgzQLcXYHi5Obmavbs2Tpx4oSSkpIKLff0008rJiZGt99+u1auXFnsdbOzs5WdnW1/npmZKUkym80ym81lr7gLbPd11/3dwdZDZTGf8dn3fT626/mAdvVNtKvvoU19E+3qmzypXUtTB48NVJs3b1ZSUpJOnz6t0NBQzZs3Ty1atHBadtWqVXr77be1adOmEl9/8uTJmjBhQoHj33zzjUJCQlytdrlYsmSJW+9fmTqe7Zja8c82LViQ7t7KVLDzqV3PJ7Srb6JdfQ9t6ptoV9/kCe168uTJEpf12EDVtGlTbdq0SceOHdOcOXM0ZMgQLV++vECoysrK0i233KL//e9/ioqKKvH1x4wZowcffND+PDMzU4mJierRo4fCw8PL7X2Uhtls1pIlS9S9e3cFBga6pQ6VLdPf+j4b1KunXr0udG9lKsj52K7nA9rVN9Guvoc29U20q2/ypHa1jV4rCY8NVEFBQWrUqJEkqV27dlq/fr2mTZum//73vw7l/v77b+3YsUO9e/e2H7OcXeAgICBAf/zxhxo2bFjg+sHBwQoODi5wPDAw0O0N6Al1qCy2IX/+JpPPv+fzqV3PJ7Srb6JdfQ9t6ptoV9/kCe1amvt7bKDKz2KxOMx5smnWrJk2b97scOzJJ59UVlaWpk2bpsTExMqqIlxgYWNfAAAAeDGPDFRjxozR1VdfrTp16igrK0uzZs1SWlqaFi9eLEkaPHiw4uPjNXnyZFWpUkUtW7Z0eH1kZKQkFTgOz2OwsS8AAAC8mEcGqv3792vw4MHKyMhQRESEWrdurcWLF6t79+6SpPT0dPn5eeSK7yglCxv7AgAAwIt5ZKB6++23izyflpZW5PmZM2eWX2VQoQwCFQAAALwY3TxwK8P2EbQQqAAAAOB9CFRwK4NFKQAAAODFCFRwKwuLUgAAAMCLEajgVixKAQAAAG9GoIJbsSgFAAAAvBmBCm5lW5SCOVQAAADwRgQquJXFzzqHSvRQAQAAwAsRqOA2ublSttn6Edz3r0W5dFIBAADAyxCo4BZz50r16klHjlk/gt8stqhePetxAAAAwFsQqFDp5s6VbrhB2r1bspz9CPrJoj17rMcJVQAAAPAWBCpUqtxcaeRIyTCsz22Byl+59mOjRonhfwAAAPAKBCpUqpUrrT1TNrmyLkrhJ+uiFIYh7dplLQcAAAB4OgIVKlVGhuPzvEP+iioHAAAAeCICFSpVXJzj88ICVf5yAAAAgCciUKFSde4sJSRIJpP1ed45VJL1eGKitRwAAADg6QhUqFT+/tK0adZ/m0yOc6hsIWvqVGs5AAAAwNMRqFDpUlOlOXOk+HjHIX8xMdbjqaluriAAAABQQgQquEVqqrRjh9Sx07lANWUKYQoAAADehUAFt/H3l6JizgWqv/5yc4UAAACAUiJQwX1yc6VDhyRJzbRV2/5gN18AAAB4FwIV3GPuXKlePWnFCknSA5qmaV/Usx4HAAAAvESAuyuA89DcudINN0iG4XA4xrxbRt++Mg0dKnXrZl21onNnlvwDAACAxyJQoXLl5kojRxYIU1Ke7tKZM60PSapRQxoxwhqs9u6VDhyQoqPPhS1JWrlSysiw7gZMAAMAAEAlKlOgys3N1YkTJxQSEqKAgHOXOnXqlJ5//nlt2rRJ9erV0yOPPKLatWuXubLwAStXSrt3l7z84cPShAnOz4WFWYPZ8ePnjhUXwAhbAAAAKEdlClRPP/20Jk6cqLS0NHU+21tgGIaSk5O1YcMGGYYhk8mkuXPnatOmTapevXq5VBpeLCOj/K6VlVXwWFEBLG/Y2r+/+B6t3Fx6vwAAAFCkMgWqpUuXKjY21h6mJOnLL7/U+vXr1aRJE917771auHChvvnmG/3vf//To48+WuYKw8vFxbnv3s7CVvXqUkqKdc5WbKz12P790l9/Sf/7n2NvWkKCNG0am2UBAADArkyBavv27WrWrJnDsfnz58tkMunDDz9Uu3btdO+99yohIUFz5swhUMHay5OQIO3Z43QeVaU7csRxzlZRdu+W+va1hrInnqC3CgAAAGVbNv3QoUOKtf1V/6zVq1crPj5e7dq1kyQFBATo0ksvVXp6elluBV/h72/t5ZHkAXHKNePGWZd8Z4l3AACA816ZAlVAQIBOnDhhf37kyBH99ddf6tSpk0O5sLAwHTt2rCy3gi9JTZXmzFF2VLy7a+I6W2/V7NnWuVZpadJHH1n/m8sGxQAAAOeLMg35a9CggdatWyeLxSI/Pz999dVXMgxDl112mUO5/fv3Kzo6ukwVhY9JTVXgtSl6IWqSHs4aJ0Peucu00b+/TNWqFb7S4P79MkVHE7IAAAB8VJl+h73uuuu0f/9+paSkaNq0aRo9erT8/f3Vu3dvexnDMLRx40bVr1+/zJWFb/EP8lfDmU+prz7THiW4uzouMeVftl06t/hFt27SwIEK6N5dVw8ZIr+JEwlWAAAAPqZMPVSPPvqo5s+fr6+//lpff/21JOmxxx5TnTp17GVWrVqlgwcPFui1AqSzC+Z9lqp2d6SoxeGVqq096qalStF81dRhd1ev3AQdPy49/bT08svSQw+xTxYAAICPKFOgCg8P1w8//KA5c+Zo37596tChg7p27epQ5tChQxo5cqT69+9fporCd6WmSikp/po0KVnTpkkfHR6kO5SrzlqpOGVon2IkSbHaqxgdUF3t0CDNUowO2K+RK5P8vWGZi+PHC90ny0hIkKmoZdnZFwsAAMDjlClQSVLVqlV1yy23FHq+T58+6tOnT1lvAx/n7y899ZR1NfKVK6U9e/x14ECyoqOt20OtXCm99pp1NJ0kPayX7IErQ3FarY7qpDUFApg39XYZZxe6sIwdJ/+mja29VzVrSocOSTt2SLNmWY/ZsC8WAACA25U5UBXl2LFjCg8Pl8lkqsjbwIf4+0vJyQWPX3mlNHbsuQ6amBh/Scn2UXO315QOHUrWjh3S0jy54yMV7O3qrJW6X6+VOGQZkirjE2yb0Oj/jPMerAJsKw1++qnUr1+F1QsAAACFK1Og2rJli7777jv17NlTTZo0sR9ftmyZbrvtNqWnp6tGjRp64YUXNHTo0LLWFee5wsJWfi+9ZOvlkpYulebP99fyw+deuExXaqLGqrOsc7ZidEAHFK0MxToNWwdVQzV0VP6ylP+bKg833SStXi316eM4DJAhggAAABWuTIHq1Vdf1TvvvKPrr7/efuzQoUPq06ePsrKy7M+HDRumNm3aqG3btmWrLVACeYPXoEGOuSImxjZ80DFk2eQNW7bhhCvVWddrnmarn2cu726xWIf+TZsmVa8upaRIEREMEQQAAKgEZfrdcPXq1brggguUmJhoP/b+++8rKytLd911l44ePar33ntPFotFr732WpkrC7jCFrAGDLAOHRw/Xtq/X1q2TPrgA+mVV6SRI62L7UmSRf5armR9rAFarmRZ5K/PdIN3LO9+5Ig0c6Y1OOUNU5LjZsQAAAAoF2Xqodq3b5+SkpIcji1ZskT+/v6aOHGiwsPDdfPNN+uVV17R2rVry1RRoDw5Gz6Yd6igs/Ug5h1I1XylqLNW6jrN18360DtXGuzfX/rtN+nJJxkCCAAAUEZlClSZmZmKiIhwOPb999/rwgsvVM2aNe3HGjdurAULFpTlVkCFK2qO1rmwZV19cMeOZDV950W1ySp8pcHOWqmH9ZLCdNz5Rd3FMKzddK+9Jr35pnUIIPOtAAAAXFLmfaj27Nljf75161YdPnxYgwYNKlCWlf7gzZz3aJ3bO8u2nPtynStkm4/1uCZplKZ53NLtxqFDMvXtK11+ufTzz+fehCRFRUkDB0r165/rqmMDYgAAgALKFKguvPBCrVy5Utu2bVOjRo309ttvy2QyFdjcd/v27YqLiytTRQFPU3DvLOtQwbx7Z730kr8mHn9Kz+qJAhsV99aXGqVpblvowv4njmXLCp48eFB69VXnL2RxCwAAALsyBaq77rpL3333ndq1a6cGDRrol19+UUxMjK655hp7maysLG3atEm9e/cuc2UBT1Tc3llPP52rl1/O1fLj5wpFR0utBlyp/9vcRdenjVS8sbtE97LN0HJrf69tcYsJE6xpkt4qAABwHitToOrXr5+2bt2q5557Tj///LPq1aun9957T8HBwfYyn376qcxmc4FeK+B84O8vPfmkRW3aLFR4+DU6cCAg3xSlVOWeSdG7d63Uopnneq9itVcxOqCDqqkoHdIBRWuP4hWlg3pFDyhRJQtgFWrcOGtP1YgR1je0fz/zrwAAwHmnTIFKkp566ik99thjyszMVFRUVIHz3bt318aNG9WwYcOy3grwWv7+UteuhgIDnZwL8teQGckK621dvn13MVlpnq63rzR4u95RhDIrptIlcfiwtacqL4YEAgCA80iZA5UkBQUFOQ1TklSnTh3VqVOnPG4D+LTUVOuevM6Wbs87L8u2KfFyJesRvVjkohduGSJoGxJ4//3S9dfTYwUAAHxauQQqSTpz5ox+/PFH+6p/8fHxateunYKCgsrrFoDPK2rpduncvKyVK6X586V33vHXxMxzi17U1h7F6IDDEMH/6G5F6VClvQe7V1+1PqKipJtvtqZFwhUAAPAxZQ5UOTk5mjBhgl577TVlZWU5nAsLC9P999+vp556SgEB5ZbdgPOaLXQlJ0svvihNmiRNm2bttXJmnq4/24s1VTV1pDKranXwoDR1qvVRo4Z1XCOLWQAAAB9RptWaLRaLrrvuOj377LPKzMxUZGSk2rZtq7Zt2yoyMlKZmZmaNGmSUlJSZLFYyqvOAM6yLd2+f7919fMPPpBeecX632+/ta4bERLqr4l6SjE6oLGaIEOS274bDx+2VioyUnr6aeuGwgAAAF6sTIHqrbfe0qJFi1S3bl3NmTNHhw4d0oYNG7RhwwYdOnRIn332merWratFixbp7bffLq86A8jH1ms1aJA0apT1v1deKY0fLx09al03IrKGNVj11WfaowSn18lUqI4pvOIrfPy4NVjZeqymTpU+/FBKSyNkAQAAr1KmQPXee++patWq+u6775TqZEWv66+/XkuXLlVwcLDefffdEl93+vTpat26tcLDwxUeHq6kpCQtXLiw0PL/+9//1LlzZ1WvXl3Vq1dXt27d9MMPP7j0ngBfk78Xq+6oVLUO26FkLdNAfaBRekWD9IGStUzVdVQ1dNjh3M16T+/oVhk6t8hFucnMtM6zeuAB6zyryy+XYmLovQIAAF6jTBObtmzZouTkZNWrV6/QMvXr19cVV1yhVatWlfi6CQkJmjJliho3bizDMPTuu+8qJSVFGzdu1AUXXFCgfFpamgYMGKCOHTuqSpUqeu6559SjRw/9+uuvio+Pd+WtAT7Hce6VvyZNSta0adZRePktV7LD8w91i77StXpTd1b8Ahe2YYGvviq9+SbLrwMAAI9Wph6q7OxsRUREFFsuLCxM2dnZJb5u79691atXLzVu3FhNmjTRpEmTFBoaqnXr1jkt/+GHH+ree+/VhRdeqGbNmumtt96SxWLR0qVLS3xP4HxS2NyrkSOtS7Q7M0+pqqV9GqsJOqQaFV5H49Ah6/Lr9FYBAAAPVqYeqsTERK1du1a5ubnyL2TFrtzcXK1bt04JCc7nbBQnNzdXs2fP1okTJ5SUlFSi15w8eVJms1k1ahT+S192drZDyMvMtG6OajabZTabXaprWdnu6677o2J4ert26uT4fMoUadUqk/bskQ4eNKlGDUOHD5u0Y4f07rt+mph1bpn2OGWoif7Q+LOLXZTpLzT52PfOGjdOxtSpsowYIcuYMR6zOqCntytcQ7v6HtrUN9GuvsmT2rU0dTAZhuHytIgRI0bojTfe0D333KNXXnlFgYGBDufPnDmjBx54QP/5z380fPhwvfrqqyW+9ubNm5WUlKTTp08rNDRUs2bNUq9evUr02nvvvVeLFy/Wr7/+qipVqjgtM378eE2YMKHA8VmzZikkJKTE9QTOJ7m50uzZTfTVVw11/Pi5Peau11xN00glaneF3t9cpYq2paToUIsWqnLsmE5Xr65DLVp4TMgCAAC+4eTJkxo4cKCOHTum8PCiF+wqU6Das2ePWrduraNHj6p27dq66aabVL9+fUnSP//8o08++UT//vuvatSooU2bNpVqPtOZM2eUnp6uY8eOac6cOXrrrbe0fPlytWjRosjXTZkyRc8//7zS0tLUunXrQss566FKTEzUwYMHi/2iVRSz2awlS5aoe/fuBcIpvJcvtmturrUX64svTJo5009ZWSb5Kddhc+G62qHbNEMRyqzQuhjVq8syfLiMzp2lffukuDgZl11W4SHLF9sVtKsvok19E+3qmzypXTMzMxUVFVWiQFWmIX/x8fFatGiR+vXrp/T0dL388ssO5w3DUJ06dfTZZ5+VenGIoKAgNWrUSJLUrl07rV+/XtOmTdN///vfQl/z4osvasqUKfr222+LDFOSFBwcrODg4ALHAwMD3d6AnlAHlD9fatfAQKlbN+vjlVcK31z4Yb2kxzVJj+oFhel4hdTFdOSI/CdOdDyYkCBNm1YpC1r4UrviHNrV99Cmvol29U2e0K6luX+Zpzx06NBBf/75p95//33ddtttuuqqq3TVVVfptttu0/vvv68///xT/v7+WrFiRZnuY7FYilzY4vnnn9czzzyjRYsWqX379mW6F4CSK2qBi9Bw695XkTpaaYtZSJKxe7d1QYvZsyvlfgAA4PxVph4qm6CgIA0aNEiDBg1yev6ee+7R+vXrlZOTU6LrjRkzRldffbXq1KmjrKwszZo1S2lpaVq8eLEkafDgwYqPj9fkyZMlSc8995yeeuopzZo1S/Xq1dPevXslSaGhoQoNDS2HdwigOLZl2fN66SVr79ULL/hr4vFzi1lcp/m6WR8qRgcqpC72BS3695d++0168knmWQEAgApRnotyFak0U7X279+vwYMHq2nTprryyiu1fv16LV68WN27d5ckpaenKyMjw15++vTpOnPmjG644QbFxcXZHy+++GK5vw8AJWfrvTp6VJowQYqs4a/lStZDekVxyrBvIPyObtUhVS//ChiGNH68VKuWNHdu+V8fAACc98qlh6q8vf3220WeT0tLc3i+Y8eOiqsMgDKzBasnnpBWrpQyMqQ//vDXhAnJkqSPNEh36H96XJP0tMZJytPLVA5se1rtGDpBdd54TP7fr7FWIi5O6tyZ3isAAOAyjwxUAHxT/mGBrVtb51rt3i1ZZJ1v9ata6k3dqSgdKrf72sJZ/ZnjZJk5XlKeHvNKXMACAAD4nkob8gcA+aWmSjt2OC5mkTAiVXGmfRqrCcpS+c+B9FO+4ce2BSyeftq6HjwAAEApEKgAuJWt12rQIGnUKOnVV6WPPnXD6oDjxkkxMQQrAABQKgQqAB7nhhukzz6Tqte0BqsY7VeylmmAZukKfatxGlcxIevwYWuwioy0LmaxdKn00UdSWhohCwAAOFWqOVTvvfeeSzc5cKBilkYG4LtSU6WUFOebBi/TlZqoseqslYpThproD43XBEnltJjF8ePWZQnzqlHDOuHriSdYxAIAANiVKlANHTpUJlPpf10xDMOl1wE4vzlbHTAmxvrv115zDFmb1brcF7NwYOu9evVV6c03pd69K+Y+AADAq5QqUNWpU4dgBKDS5V8d8MorpbFjrcFqzx7r3Kt5B1M1Xyl6XJM0SlNVU0cqpjJnl2A3ffSRFBQk0/Ll0oEDLMEOAMB5qlSBiv2eAHiKvCGralXrvCuLYZ1z9ayeqLA9rez3HzhQvYKDFXD69LmDLMEOAMB5h0UpAHi91FRpzhxrnpHO7WnVV5/pkGpWyD1NhqHAvGFKOrcE++zZzl+Um2td4IKFLgAA8Bls7AvAJ9gWsbANAzxwQIqOTtX//ZGinGcmaZSmqaYO28vbdqOqkEHMN91krUiDBlJ0tBQba5v4ZZ2LZZO3Rys399xEMYYPAgDgNQhUAHxG/rlWZ49q7oVPqfkdT6jFYeuqgBmKU5QO6j+6u2IWsbBYrOGpOLYerRtvlL79tvCwBQAAPBaBCoDPs/Ze+WvSpGRNnSodObtexTxdr8c1SY/qBYXpuPsq+OmnBY/Zwta4cVLTptZeq44dpTVrzi13KEl799q646T4eHq2AACoZAQqAOeFvEuwT5pkzSm2uVa2RSzyDwv0CHn3wzKZJMMovKzEflkAAFQyFqUAcF6xBavPPpNqnl2vwhasYrRfyVqmAZqlK/StxmuscitmlpVrigtT0rn9smrUkB54gMUvAACoYPRQATgv2RaxmDTJOlXp8GFrsFquZHuZZbpSW9Ras9VPhrzsL1CZmdLUqdZHjRrSiBHW4YC2IYI1a1r31LItmiFJ+/cXviAGi2YAAOAUgQrAeSvvMEDb6oBLl0ozZpwr85luUF99pmkaqUTtdl9ly+LwYcehg8WpXt2aNrt1K9kKhQAAnMcIVADOe3lXBxw0SLr2Wus0pN1n89M8pWq+UtRZK3Wd5utmfagYHbC/3lAFLb/uLkeOSDNnWh+F2bPHupvynDmEKgDAec2rRrAAQGVITZV27JCWLbMGK+nccMCH9IrilGGfazVWE7RbCW6tr1sYhvUxciRztAAA5zV6qADACVuvVXKy1KWLY49V/rlWz+qJQnuvnPGpHq3du6X+/aXLLnOcl+VsCXfmYQEAfBCBCgCKYVvAYuVKaf586cMPres62NgC1nIl6xG9qM5aqdraoxgd0AFFK0PWRR9qab8a6S/dqf8VOh/LK8PWZ59ZH/lFRUkDB0r161u7/GbNcvzC5Z2rxR5aAAAvRaACgBLI22P14ovSsmU5+vrrTUpIaKuYGH8dOmTNDDNm+Gt5ZnKR17L1aMUpQ/tk3aC3JGHL6xw8KL36auHn88/VyhvAymMVQokeMQBAhSNQAUAp+ftLXbsaOnFij3r1aqPAwHO/pL/0kuNS7M7kHzKYV/6w1Vkrdb9ec9hw+KBq6FWN0Cp1Vqz2qpuWKkXzPW9T4tIqLoDZlGQVwtBQyc/Puny8jbNNj/MGsZgYmXJzFb9ihUzVqlnHeq5ZU3GBjCGQAOATCFQAUI7yL8V+9vd0SQW3gHI2Cs7ZXlgTNdYesjIUp5XqLIvO/eL9kQbpDuXa53HdrncUoTxBwteUZBXC48cLHrNtevzSS9LQoVJWlnUMZ54gFiCpvSS9/LJkMjluppy3B62weWJSyYLS3LmOE/Mkx6XobdfYs8f6Acl7P8n7euMIjwB8GIEKACpA3qXYi/LSS46/NzsLW0X1aNnkn8f1uCZplKZ5f69VRcjMLFlPWN4wJTnvQcu/afLSpQVCWoEg9tdfzvcF271b6ttXuvFG6dtvnXdxhoVZ65U3MDrrjcvfiyedS/R5h1E62+g5f3DLG+qKel1h55z95YB9zAD4EAIVALhRUcGrsLBl+93V2Ug3yRquJuopp3O1YrVXMTqgg6qpKB2yL5rhbGghSqAkmyaXdCijzaefFn4uK6vgMWe9cSXpxSuKs+BWntjHDIAPIVABgIcqrpfryiulsWMdQ1dpe7Zs8g4tdLZC4bX6Sg9oqgyxgeF5wVlwK0+GYR1SOWqUtSeN4X8AvBiBCgC8mLPQlb9nq7geLZuiAtgyXalV6qxpGuk7qxDCvQxD2rXL+sHM/yGuiDlXubnS6tXM4wJQ7ghUAOBjCuvZctajVZqpL/OUqvlKKdUmxkCxXnvNGnZsH8Di9iwrahn9/EGsY0eZVqzQBW+/rYBhw6zDL23yrvooFb4ICKELQDEIVABwHinpYhk2+Xu7qlf318iRyVp+rOAmxrZ5WXW1Q4M0y2nYOqpw+cuiMFXQ3Bx4n7lzrY+iFDUnzBa2IiIKBjGTSQGGoUbOrmlb9XHyZCkoyHFRDxsWzwBQAgQqAEChnAWwsDDregIWo/Ahgg/rpQLzsfYoXitlXT0u77miAhhQLFvYcib/So3OnD5tfThjW3lx6FDpiitKt9k0gPMGgQoAUCqpqdbF2fJvo5RXcQti5D+XP4C5ugphrkzy17lfoukRQ7kobsVEV/cQ69ix4ObR+csQ1gCPR6ACAJRaaqp1lFVRy7pLhW99lH+7puICWHGrENbSfmUoTqvVUZ20xmETZEnFzvs6qBp6VSNkkgoEt0yFypBfhW2WbEgyVciVUWlsPVm9eknr1zsOO7Rxtl9Y/s2jnS1Xn3cfM2ffZEX1krGhMlApCFQAAJeUdj5WXoMGOf6uFxNTtlUI83JWJu+mx4UNRbTI+oumLbg5C2W1tUfdtFQpml9sb1lJg1iu/BWgXKfnDqq65itFmYooMCyS3jgPtGBB4eec7emVf0iis+XqS7qPWXy8dOedUuPG1vB08KD0wAOO3ch5N6K2BbG8vWQx1v3qSrTgR3G9aYQ5nEcIVAAAt8gfyMq6CmFJlCSUFVbGduwjDdIdyi100+Si5osV16tmu5btXN6gZxsWaQt6RfXGObtf3vrlP1bUgiK2UPeduhX6uqKueVA19bIeUpQOsIdZRdqzx7rIRlGcbUSdv5csL1sAkwr+tcNZj1tRC4TkXakx/wqKpQ1rgIchUAEAPEZZVyEsybDD4nrCSqI0mya72qtWknsWFfxclT+45Q11rjqpapqjvgxv9ERFLdzhLIDZOOtxK2qBkPwrNdqGMmZlOY7/lQof+njzzdZQZgtceZWkR4xeM1QQAhUAwGu5MuywqJ6w3NwcLVy4SQkJbRUT419oOCsquJWkBy08XLJYHH9fLMkUm7xcPVec0oTFkpqnVN2gz/Sm7lSUDpXrteGlihrKWNjQx6lTrY/q1eXXu7fia9SQqUoVae3agn8lyTu80dnETan4+WmFTQIt6Q+Bwq6Vf5GSohYwcfbXIvZI8zgEKgDAeaewIGY2Gzp1ao969WqjwMCy/7KSvwetqAXfilsELv/0ltKeq4jfDUsTLOcdsG4M/bgmaZSmFTsHrSLmhJ1QVfnJUFUVskw6vMORI/J/7z21l6wBy5mietdsSjo/rSI4+wtKSc7Z5A2DzhYpcbaCZFEBrBQ9fKczMjQ7Lk6f16ypQzk5qhkYqD5RUeoXHa0qhd3Dx3sHCVQAAFSQ4nrQnJ3Lf6y0ry/Jucr20ktSWpp0443+mnj4KT2rJ8o0B62oZfSdBbGDqqFpGqln9YQkOQ11+xStWRqgCGXpVs2QITHnCxXH2ZDJkpyzKS4M5u+mLqo3ztk46EJ6+L5o1kxDH3tMRyT5HTggi5+f/AxDcw8e1MitW/WuxaLeJemuzzunLs9fY/z27lX87t0yVasmXX6514Quk2G4OijAt2RmZioiIkLHjh1TeHi4W+pgNpu1YMEC9erVS4GBgW6pA8of7eqbaFffRLtWrLlzrZtCS64PSbTxO7swSFGbRxc1D8wvz8Ii+ctcr7mappFKVCEbrQHnoS86dlSfZ56RJBl+Bf/cYLJYJEmfjx2r69asKfsN8+7v5galyQb0UAEAgEpRkk2hS6q0m0eX5vXzZB2eWNhm0430l8ZrAr1YOG+cDgzU0Mcek+Q8TNmOmywWDX3sMf3bt6+qmM1lu+nu3da/wMyZ47ZQVVIEKgAAUGnybgpdkjleztYSiI6WBgxwvkCcq/IvFFJcYNus1oX2YuUdPph/zzJnQxKd7VlW0RtKA6UxOzlZR8LCii1n+PnpSFiY5nTtqpu//bZ8bj5qlPWHhgcP/yNQAQCASlWa1RnzbwJd1P6x+cNZzZrS/v252r17o3r2vFABAQFOF9RwtlBIcZtNO+vFcrZR9B1OhhZKBYckFnWsrL1kLFePsvr8ssvkl5srSwlCjV9uruZ17lw+gcowpF27rN+MnjQxNB8CFQAA8GhFBbDiwpnZbNGCBXt0xRVtVJJpcSXdbHrlSumll/y1/HgRN1fxG0W7cszGWS+Zs56tXUrQ/3SHtqmx9inG6aIeB1VDr2qETFKhC35IUq5M8te5CXBFrcZYESs1wj0OhYWVKExJksXfX4dL0JtVKhkZ5Xu9ckagAgAAKERhgc0WtiZNss6bL49hh84427PMJm8vWXG9XXkX5VimKzVRYwstk/ecbRXGWtqvDMVptTqqk9YU2ZNW1EqNdbVDgzRLMSq4QVthQx/9latqOlU+X1C4pGZWVql6qGo420usDHJj4sq4vXjFIlABAAC4wN9feuop6Yknih52WBH7v9rKf/mlv6ZNSy5Qt7IsyuHKgh+lKf+wXirVCo2S86Xuncnfg+YM89NKr8+qVZrbpUuJylr8/XX9ypXlcl+LTNqtBP2jzuW83Xj5IlABAACUQWnmhLmqsOtfeaXUpUv5rJxYWVwJbBN1bv+y/PuSSc570Eqzx1ne+Wm2a+Z9Xf5jRZUv7Fw3LS2wSElRC5jYzu1Uffs1i+rhq0j90tI0csQIHa1WrdBV/iTr0umRJ07ohuXLy3xPy9n/jtJU9dvvyf1TBCoAAACvVtzKiWXpJStN+bKec7b/a17FBTGbkpQpTbny8pEGOV2kpKgFTPLvnyY57+ErbMPr0vbGFdbDV8Vs1ruTJytl4kSZLJYi96F6d/Jkp0uml3ZO3W4lapSmap5SdX9ciV7iNgQqAAAAL1cZvWSV4aWXCh/eWNgy+t6kLEMtiytX2Nw4qfjeuBL18K2NVouxfvr1sQApLEfKleQvmXINGf4m+Z8wqflkiz5aO1xLdaPDfQrrGSysR9BW3jD5KzHh3NBXT0WgAgAAgEcoLhjaltFftixHr766Q2vXNtTBg+cWha9e3dpbFxFRsLfLdq5bt+JDmrPFQGz7n9WvX/JetcKuVVFKs6pkYYosu0ZS31yp6wGp80EpzCwjK1BaGaWc5dHabPbX5rJcPx+TpKlTPXoLKkkeGqimT5+u6dOna8eOHZKkCy64QE899ZSuvvrqQl8ze/ZsjR07Vjt27FDjxo313HPPqVevXpVUYwAAAFQGf3+pa1dDJ078qk8/rat16wKd7lFm6+1ydi6vwvY6k0r2+vy9akXtcVZYz1t5DM0sat+0cmX2l76NtT4qUGKiNUylplbobcqFRwaqhIQETZkyRY0bN5ZhGHr33XeVkpKijRs36oILLihQfs2aNRowYIAmT56sa6+9VrNmzVKfPn30008/qWXLlm54BwAAAKhoZdmjrCRlS/L6kt6noodkFrVvmlSx8+DK61p791o34r7mmgt1+eUBHt8zZeORgap3794OzydNmqTp06dr3bp1TgPVtGnT1LNnTz3yyCOSpGeeeUZLlizR66+/rv/85z+VUmcAAADAnbx9Lp1tI+6uXdt4TZiSPDRQ5ZWbm6vZs2frxIkTSkpKclpm7dq1evDBBx2OXXXVVfr8888LvW52drays7PtzzMzraufmM1mmZ2sTFIZbPd11/1RMWhX30S7+iba1ffQpr6JdvVNntSupamDxwaqzZs3KykpSadPn1ZoaKjmzZunFi1aOC27d+9e1apVy+FYrVq1tHfv3kKvP3nyZE2YMKHA8W+++UYhISFlq3wZLVmyxK33R8WgXX0T7eqbaFffQ5v6JtrVN3lCu548ebLEZT02UDVt2lSbNm3SsWPHNGfOHA0ZMkTLly8vNFSV1pgxYxx6tTIzM5WYmKgePXooPDy8XO5RWmazWUuWLFH37t0VGBjoljqg/NGuvol29U20q++hTX0T7eqbPKldbaPXSsJjA1VQUJAaNWokSWrXrp3Wr1+vadOm6b///W+BsrGxsdq3b5/DsX379ik2tvDVR4KDgxUcHFzgeGBgoNsb0BPqgPJHu/om2tU30a6+hzb1TbSrb/KEdi3N/Qtuc+yhLBaLw5ynvJKSkrR06VKHY0uWLCl0zhUAAAAAlAeP7KEaM2aMrr76atWpU0dZWVmaNWuW0tLStHjxYknS4MGDFR8fr8mTJ0uSRo4cqa5du+qll17SNddco48//lgbNmzQm2++6c63AQAAAMDHeWSg2r9/vwYPHqyMjAxFRESodevWWrx4sbp37y5JSk9Pl5/fuc61jh07atasWXryySf1+OOPq3Hjxvr888/ZgwoAAABAhfLIQPX2228XeT4tLa3AsX79+qlfv34VVCMAAAAAKMhr5lABAAAAgKchUAEAAACAiwhUAAAAAOAiAhUAAAAAuIhABQAAAAAuIlABAAAAgIsIVAAAAADgIgIVAAAAALiIQAUAAAAALiJQAQAAAICLCFQAAAAA4CICFQAAAAC4iEAFAAAAAC4iUAEAAACAiwhUAAAAAOAiAhUAAAAAuIhABQAAAAAuIlABAAAAgIsIVAAAAADgIgIVAAAAALiIQAUAAAAALiJQAQAAAICLCFQAAAAA4CICFQAAAAC4iEAFAAAAAC4iUAEAAACAiwhUAAAAAOAiAhUAAAAAuIhABQAAAAAuIlABAAAAgIsIVAAAAADgIgIVAAAAALiIQAUAAAAALiJQAQAAAICLCFQAAAAA4CICFQAAAAC4iEAFAAAAAC4iUAEAAACAiwhUAAAAAOAiAhUAAAAAuIhABQAAAAAuIlABAAAAgIsIVAAAAADgIgIVAAAAALiIQAUAAAAALiJQAQAAAICLCFQAAAAA4CICFQAAAAC4iEAFAAAAAC4iUAEAAACAizwyUE2ePFkdOnRQWFiYYmJi1KdPH/3xxx/Fvm7q1Klq2rSpqlatqsTERD3wwAM6ffp0JdQYAAAAwPnIIwPV8uXLNXz4cK1bt05LliyR2WxWjx49dOLEiUJfM2vWLD322GMaN26ctm7dqrfffluffPKJHn/88UqsOQAAAIDzSYC7K+DMokWLHJ7PnDlTMTEx+vHHH9WlSxenr1mzZo06deqkgQMHSpLq1aunAQMG6Pvvv6/w+gIAAAA4P3lkoMrv2LFjkqQaNWoUWqZjx4764IMP9MMPP+jiiy/WP//8owULFuiWW25xWj47O1vZ2dn255mZmZIks9kss9lcjrUvOdt93XV/VAza1TfRrr6JdvU9tKlvol19kye1a2nqYDIMw6jAupSZxWLRddddp6NHj2rVqlVFln311Vf18MMPyzAM5eTk6O6779b06dOdlh0/frwmTJhQ4PisWbMUEhJSLnUHAAAA4H1OnjypgQMH6tixYwoPDy+yrMcHqnvuuUcLFy7UqlWrlJCQUGi5tLQ03XTTTZo4caIuueQSbdu2TSNHjtQdd9yhsWPHFijvrIcqMTFRBw8eLPaLVlHMZrOWLFmi7t27KzAw0C11QPmjXX0T7eqbaFffQ5v6JtrVN3lSu2ZmZioqKqpEgcqjh/zdd999+uqrr7RixYoiw5QkjR07VrfccouGDRsmSWrVqpVOnDihO++8U0888YT8/BzX3wgODlZwcHCB6wQGBrq9AT2hDih/tKtvol19E+3qe2hT30S7+iZPaNfS3N8jA5VhGBoxYoTmzZuntLQ01a9fv9jXnDx5skBo8vf3t18PAAAAAMqbRwaq4cOHa9asWZo/f77CwsK0d+9eSVJERISqVq0qSRo8eLDi4+M1efJkSVLv3r318ssvq23btvYhf2PHjlXv3r3twQoAAAAAypNHBirbQhLJyckOx2fMmKGhQ4dKktLT0x16pJ588kmZTCY9+eST2rNnj6Kjo9W7d29NmjSpsqoNAAAA4DzjkYGqJEP00tLSHJ4HBARo3LhxGjduXAXVCgAAAAAc+RVfBAAAAADgDIEKAAAAAFxEoAIAAAAAFxGoAAAAAMBFBCoAAAAAcBGBCgAAAABcRKACAAAAABcRqAAAAADARQQqAAAAAHARgQoAAAAAXESgAgAAAAAXEagAAAAAwEUEKgAAAABwEYEKAAAAAFxEoAIAAAAAFxGoAAAAAMBFBCoAAAAAcBGBCgAAAABcRKACAAAAABcRqAAAAADARQQqAAAAAHARgQoAAAAAXESgAgAAAAAXEagAAAAAwEUEKgAAAABwEYEKAAAAAFxEoAIAAAAAFxGoAAAAAMBFBCoAAAAAcBGBCgAAAABcRKACAAAAABcRqAAAAADARQQqAAAAAHARgQoAAAAAXESgAgAAAAAXEagAAAAAwEUEKgAAAABwEYEKAAAAAFxEoAIAAAAAFxGoAAAAAMBFBCoAAAAAcBGBCgAAAABcRKACAAAAABcRqAAAAADARQQqAAAAAHARgQoAAAAAXESgAgAAAAAXEagAAAAAwEUEKgAAAABwEYEKAAAAAFzkkYFq8uTJ6tChg8LCwhQTE6M+ffrojz/+KPZ1R48e1fDhwxUXF6fg4GA1adJECxYsqIQaAwAAADgfBbi7As4sX75cw4cPV4cOHZSTk6PHH39cPXr00G+//aZq1ao5fc2ZM2fUvXt3xcTEaM6cOYqPj9fOnTsVGRlZuZUHAAAAcN7wyEC1aNEih+czZ85UTEyMfvzxR3Xp0sXpa9555x0dPnxYa9asUWBgoCSpXr16FV1VAAAAAOcxjwxU+R07dkySVKNGjULLfPHFF0pKStLw4cM1f/58RUdHa+DAgRo9erT8/f0LlM/OzlZ2drb9eWZmpiTJbDbLbDaX8zsoGdt93XV/VAza1TfRrr6JdvU9tKlvol19kye1a2nqYDIMw6jAupSZxWLRddddp6NHj2rVqlWFlmvWrJl27NihQYMG6d5779W2bdt077336v7779e4ceMKlB8/frwmTJhQ4PisWbMUEhJSru8BAAAAgPc4efKkBg4cqGPHjik8PLzIsh4fqO655x4tXLhQq1atUkJCQqHlmjRpotOnT2v79u32HqmXX35ZL7zwgjIyMgqUd9ZDlZiYqIMHDxb7RasoZrNZS5YsUffu3e3DFuH9aFffRLv6JtrV99Cmvol29U2e1K6ZmZmKiooqUaDy6CF/9913n7766iutWLGiyDAlSXFxcQoMDHQY3te8eXPt3btXZ86cUVBQkEP54OBgBQcHF7hOYGCg2xvQE+qA8ke7+iba1TfRrr6HNvVNtKtv8oR2Lc39PXLZdMMwdN9992nevHn67rvvVL9+/WJf06lTJ23btk0Wi8V+7M8//1RcXFyBMAUAAAAA5cEjA9Xw4cP1wQcfaNasWQoLC9PevXu1d+9enTp1yl5m8ODBGjNmjP35Pffco8OHD2vkyJH6888/9fXXX+vZZ5/V8OHD3fEWAAAAAJwHPHLI3/Tp0yVJycnJDsdnzJihoUOHSpLS09Pl53cuDyYmJmrx4sV64IEH1Lp1a8XHx2vkyJEaPXp0ZVUbAAAAwHnGIwNVSdbJSEtLK3AsKSlJ69atq4AaAQAAAEBBHjnkDwAAAAC8AYEKAAAAAFxEoAIAAAAAFxGoAAAAAMBFBCoAAAAAcBGBCgAAAABcRKACAAAAABcRqAAAAADARQQqAAAAAHARgQoAAAAAXESgAgAAAAAXEagAAAAAwEUEKgAAAABwEYEKAAAAAFxEoAIAAAAAFxGoAAAAAMBFBCoAAAAAcBGBCgAAAABcRKACAAAAABcRqAAAAADARQQqAAAAAHARgQoAAAAAXESgAgAAAAAXEagAAAAAwEUEKgAAAABwEYEKAAAAAFxEoAIAAAAAFxGoAAAAAMBFBCoAAAAAcFGAuyvgKQzDkCRlZma6rQ5ms1knT55UZmamAgMD3VYPlC/a1TfRrr6JdvU9tKlvol19kye1qy0T2DJCUQhUZ2VlZUmSEhMT3VwTAAAAAJ4gKytLERERRZYxGSWJXecBi8Wif//9V2FhYTKZTG6pQ2ZmphITE7Vr1y6Fh4e7pQ4of7Srb6JdfRPt6ntoU99Eu/omT2pXwzCUlZWl2rVry8+v6FlS9FCd5efnp4SEBHdXQ5IUHh7u9g8Ryh/t6ptoV99Eu/oe2tQ30a6+yVPatbieKRsWpQAAAAAAFxGoAAAAAMBFBCoPEhwcrHHjxik4ONjdVUE5ol19E+3qm2hX30Ob+iba1Td5a7uyKAUAAAAAuIgeKgAAAABwEYEKAAAAAFxEoAIAAAAAFxGoAAAAAMBFBCoP8n//93+qV6+eqlSpoksuuUQ//PCDu6uEEho/frxMJpPDo1mzZvbzp0+f1vDhw1WzZk2Fhoaqb9++2rdvnxtrDGdWrFih3r17q3bt2jKZTPr8888dzhuGoaeeekpxcXGqWrWqunXrpr/++suhzOHDhzVo0CCFh4crMjJSt99+u44fP16J7wL5FdeuQ4cOLfD927NnT4cytKtnmTx5sjp06KCwsDDFxMSoT58++uOPPxzKlOTnbnp6uq655hqFhIQoJiZGjzzyiHJycirzrSCPkrRrcnJyge/Xu+++26EM7epZpk+frtatW9s3601KStLChQvt533he5VA5SE++eQTPfjggxo3bpx++ukntWnTRldddZX279/v7qqhhC644AJlZGTYH6tWrbKfe+CBB/Tll19q9uzZWr58uf7991+lpqa6sbZw5sSJE2rTpo3+7//+z+n5559/Xq+++qr+85//6Pvvv1e1atV01VVX6fTp0/YygwYN0q+//qolS5boq6++0ooVK3TnnXdW1luAE8W1qyT17NnT4fv3o48+cjhPu3qW5cuXa/jw4Vq3bp2WLFkis9msHj166MSJE/Yyxf3czc3N1TXXXKMzZ85ozZo1evfddzVz5kw99dRT7nhLUMnaVZLuuOMOh+/X559/3n6OdvU8CQkJmjJlin788Udt2LBBV1xxhVJSUvTrr79K8pHvVQMe4eKLLzaGDx9uf56bm2vUrl3bmDx5shtrhZIaN26c0aZNG6fnjh49agQGBhqzZ8+2H9u6dashyVi7dm0l1RClJcmYN2+e/bnFYjFiY2ONF154wX7s6NGjRnBwsPHRRx8ZhmEYv/32myHJWL9+vb3MwoULDZPJZOzZs6fS6o7C5W9XwzCMIUOGGCkpKYW+hnb1fPv37zckGcuXLzcMo2Q/dxcsWGD4+fkZe/futZeZPn26ER4ebmRnZ1fuG4BT+dvVMAyja9euxsiRIwt9De3qHapXr2689dZbPvO9Sg+VBzhz5ox+/PFHdevWzX7Mz89P3bp109q1a91YM5TGX3/9pdq1a6tBgwYaNGiQ0tPTJUk//vijzGazQ/s2a9ZMderUoX29yPbt27V3716HdoyIiNAll1xib8e1a9cqMjJS7du3t5fp1q2b/Pz89P3331d6nVFyaWlpiomJUdOmTXXPPffo0KFD9nO0q+c7duyYJKlGjRqSSvZzd+3atWrVqpVq1aplL3PVVVcpMzPT/pdzuFf+drX58MMPFRUVpZYtW2rMmDE6efKk/Rzt6tlyc3P18ccf68SJE0pKSvKZ79UAd1cA0sGDB5Wbm+vwQZGkWrVq6ffff3dTrVAal1xyiWbOnKmmTZsqIyNDEyZMUOfOnbVlyxbt3btXQUFBioyMdHhNrVq1tHfvXvdUGKVmaytn36e2c3v37lVMTIzD+YCAANWoUYO29mA9e/ZUamqq6tevr7///luPP/64rr76aq1du1b+/v60q4ezWCwaNWqUOnXqpJYtW0pSiX7u7t271+n3s+0c3MtZu0rSwIEDVbduXdWuXVu//PKLRo8erT/++ENz586VRLt6qs2bNyspKUmnT59WaGio5s2bpxYtWmjTpk0+8b1KoALKwdVXX23/d+vWrXXJJZeobt26+vTTT1W1alU31gxAcW666Sb7v1u1aqXWrVurYcOGSktL05VXXunGmqEkhg8fri1btjjMW4X3K6xd885dbNWqleLi4nTllVfq77//VsOGDSu7miihpk2batOmTTp27JjmzJmjIUOGaPny5e6uVrlhyJ8HiIqKkr+/f4EVTfbt26fY2Fg31QplERkZqSZNmmjbtm2KjY3VmTNndPToUYcytK93sbVVUd+nsbGxBRaSycnJ0eHDh2lrL9KgQQNFRUVp27ZtkmhXT3bffffpq6++0rJly5SQkGA/XpKfu7GxsU6/n23n4D6Ftaszl1xyiSQ5fL/Srp4nKChIjRo1Urt27TR58mS1adNG06ZN85nvVQKVBwgKClK7du20dOlS+zGLxaKlS5cqKSnJjTWDq44fP66///5bcXFxateunQIDAx3a948//lB6ejrt60Xq16+v2NhYh3bMzMzU999/b2/HpKQkHT16VD/++KO9zHfffSeLxWL/nz483+7du3Xo0CHFxcVJol09kWEYuu+++zRv3jx99913ql+/vsP5kvzcTUpK0ubNmx3C8pIlSxQeHq4WLVpUzhuBg+La1ZlNmzZJksP3K+3q+SwWi7Kzs33ne9Xdq2LA6uOPPzaCg4ONmTNnGr/99ptx5513GpGRkQ4rmsBzPfTQQ0ZaWpqxfft2Y/Xq1Ua3bt2MqKgoY//+/YZhGMbdd99t1KlTx/juu++MDRs2GElJSUZSUpKba438srKyjI0bNxobN240JBkvv/yysXHjRmPnzp2GYRjGlClTjMjISGP+/PnGL7/8YqSkpBj169c3Tp06Zb9Gz549jbZt2xrff/+9sWrVKqNx48bGgAED3PWWYBTdrllZWcbDDz9srF271ti+fbvx7bffGhdddJHRuHFj4/Tp0/Zr0K6e5Z577jEiIiKMtLQ0IyMjw/44efKkvUxxP3dzcnKMli1bGj169DA2bdpkLFq0yIiOjjbGjBnjjrcEo/h23bZtm/H0008bGzZsMLZv327Mnz/faNCggdGlSxf7NWhXz/PYY48Zy5cvN7Zv32788ssvxmOPPWaYTCbjm2++MQzDN75XCVQe5LXXXjPq1KljBAUFGRdffLGxbt06d1cJJdS/f38jLi7OCAoKMuLj443+/fsb27Zts58/deqUce+99xrVq1c3QkJCjOuvv97IyMhwY43hzLJlywxJBR5DhgwxDMO6dPrYsWONWrVqGcHBwcaVV15p/PHHHw7XOHTokDFgwAAjNDTUCA8PN2699VYjKyvLDe8GNkW168mTJ40ePXoY0dHRRmBgoFG3bl3jjjvuKPDHLNrVszhrT0nGjBkz7GVK8nN3x44dxtVXX21UrVrViIqKMh566CHDbDZX8ruBTXHtmp6ebnTp0sWoUaOGERwcbDRq1Mh45JFHjGPHjjlch3b1LLfddptRt25dIygoyIiOjjauvPJKe5gyDN/4XjUZhmFUXn8YAAAAAPgO5lABAAAAgIsIVAAAAADgIgIVAAAAALiIQAUAAAAALiJQAQAAAICLCFQAAAAA4CICFQAAAAC4iEAFAAAAAC4iUAEAPE69evVkMpmKfcycOdPdVS0xW50BAL4lwN0VAACgMJ06dVKjRo0KPV/UOQAAKgOBCgDgsYYNG6ahQ4e6uxoAABSKIX8AAAAA4CICFQDAJ+Sdo/S///1P7dq1U7Vq1RQZGalevXpp3bp1hb728OHDevzxx3XBBRcoJCREYWFhateunZ5//nmdOnWq0Nft2bNHjzzyiFq1aqWwsDBVq1ZNTZo00dChQ7VmzZpCX/fZZ5/psssuU3h4uKpVq6ZOnTppwYIFrr95AIDbEKgAAD7lwQcf1F133aWQkBClpKQoMTFRCxcuVOfOnTVv3rwC5f/55x9ddNFFmjx5sg4cOKBevXrpiiuu0F9//aXRo0frsssu05EjRwq8bunSpWrZsqVefPFF7d+/X1deeaWuueYaRUZGatasWXrzzTed1m/cuHHq16+fJKlXr15q3Lix1qxZo2uvvdZp/QAAHs4AAMDD1K1b15BkzJgxo8SvkWRIMqpWrWosXbrU4dzzzz9vSDIiIiKMffv2OZy75JJLDEnGddddZxw/ftx+fP/+/cZFF11kSDIGDhzo8Jr09HQjIiLCkGQ89thjRnZ2tsP5ffv2GStXrnRav8jISGPdunUO58aNG2dIMpo0aVLi9wsA8AwmwzAMd4U5AACcqVevnnbu3FlsuSNHjigyMlKS7MP9Ro0apVdeeaVA2Q4dOmjDhg2aNGmSHn/8cUnSqlWr1LlzZ4WEhOiff/5RrVq1HF7z448/qn379vLz89POnTuVkJAgSXrggQc0depU9e7dW1988UWJ3pOtfq+++qpGjBjhcC47O1u1atXSsWPHlJ6ersTExBJdEwDgfqzyBwDwWMUtmx4UFFTg2JAhQ5yWHTx4sDZs2KC0tDR7oEpLS5Mk9ezZs0CYkqR27dqpTZs2+vnnn7V8+XINGjRIkrRo0SJJ0p133lmq9yNJvXv3LnAsODhYDRo00MaNG7Vnzx4CFQB4EQIVAMBjubJsev369Ys8vnv3bvuxPXv2FPkaSWrYsKF+/vlne1lJ9t6zZs2alapuklSnTh2nx8PDwyVJp0+fLvU1AQDuw6IUAIDzirtHuvv58b9eAPAl/FQHAPiU7du3Oz2+Y8cOSbLPg5Kk+Ph4SdaV/gpjO2crK53rZfr999/LVFcAgPcjUAEAfMr7779f5PHk5GT7Mdu/Fy1apH379hV4zcaNG7Vp0yb5+fmpS5cu9uM9e/aUZN3vCgBwfiNQAQB8yvTp0+2LTdi88sor+uGHHxQWFqbbb7/dfvyyyy7TJZdcolOnTumuu+7SyZMn7ecOHjyou+66S5J00003OSwU8eCDDyosLExffPGFnnzySZnNZof77d+/X6tWraqAdwcA8DQsSgEA8FhvvfVWgXCUV48ePTRw4ECHY3fddZeuuOIKde7cWfHx8dqyZYs2b94sf39/vfPOO4qNjXUoP2vWLF1xxRWaP3++6tevry5dushsNmvZsmXKzMzURRddpNdff93hNXXq1NGcOXN0ww03aNKkSXrrrbeUlJSkwMBA7dy5Uxs3btTAgQN12WWXldvXAgDgmQhUAACPtXr1aq1evbrQ85GRkQUC1SuvvKKmTZvqv//9r9avX6/AwED17NlTY8eOVceOHQtco0GDBvrpp5/04osv6vPPP9dXX30lPz8/NW3aVP3799f999+vqlWrFnhdjx49tGXLFr388statGiRFi1apICAANWuXVu33HKL7rjjjrJ/AQAAHo+NfQEAPsG2cS7/WwMAVCbmUAEAAACAiwhUAAAAAOAiAhUAAAAAuIhFKQAAPoG5UwAAd6CHCgAAAABcRKACAAAAABcRqAAAAADARQQqAAAAAHARgQoAAAAAXESgAgAAAAAXEagAAAAAwEUEKgAAAABw0f8DTQ8CoJPVS5gAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create a plot of the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_epoch_loss_list) + 1), train_epoch_loss_list, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "# Plot validation loss\n",
    "plt.plot(range(1, len(test_epoch_loss_list) + 1), test_epoch_loss_list, marker='o', linestyle='-', color='r', label='Validation Loss')\n",
    "# Mark the best epoch\n",
    "plt.plot(best_epoch, best_val_loss , marker='o', markersize=8, linestyle='', color='c', label=f'Best Epoch:{best_epoch}')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Pretain AE Loss Curve', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(top=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac20b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67883d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
