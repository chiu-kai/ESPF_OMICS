{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "  def __init__(self, hidden_size, max_len=50):\n",
    "    super(SinusoidalPositionalEncoding, self).__init__()\n",
    "    pe = torch.zeros(max_len, hidden_size) # torch.Size([50, 128])\n",
    "    position = torch.arange(0, max_len).float().unsqueeze(1) # torch.Size([50, 1]) # 0~50\n",
    "    div_term = torch.exp(torch.arange(0, hidden_size, 2).float() *\n",
    "                         (-torch.log(torch.Tensor([10000])) / hidden_size)) # [max_len / 2]\n",
    "                        #ç”Ÿæˆä¸€å€‹æ•¸ä½ [0ï¼Œ 2ï¼Œ 4ï¼Œ ...ï¼Œ hidden_size-2] çš„å¼µé‡ï¼ˆå¶æ•¸ç´¢å¼•ï¼‰ã€‚\n",
    "    pe[:, 0::2] = torch.sin(position * div_term) #å°‡ sine å‡½æ•¸æ‡‰ç”¨æ–¼ä½ç½®ç·¨ç¢¼å¼µé‡ ï¼ˆpeï¼‰ çš„å¶æ•¸ç¶­æ•¸ã€‚\n",
    "    pe[:, 1::2] = torch.cos(position * div_term) #å°‡é¤˜å¼¦å‡½æ•¸æ‡‰ç”¨æ–¼ä½ç½®ç·¨ç¢¼å¼µé‡ ï¼ˆpeï¼‰ çš„å¥‡æ•¸ç¶­æ•¸ã€‚\n",
    "    pe = pe.unsqueeze(0) # torch.Size([1, 50, 128])\n",
    "    # register pe to buffer and require no grads#ç·©è¡å€çš„åƒæ•¸åœ¨è¨“ç·´æœŸé–“ä¸æœƒæ›´æ–°\n",
    "    self.register_buffer('pe', pe)\n",
    "  def forward(self, x):\n",
    "    # x: [batch, seq_len, hidden_size]\n",
    "    # we can add positional encoding to x directly, and ignore other dimension\n",
    "    return x + self.pe[:,:x.size(1)].to(x.device)# x.size(1)= 50\n",
    "  \n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size, max_len):#(128, 50)\n",
    "        super().__init__()\n",
    "        self.position_embeddings = nn.Embedding(max_len, hidden_size)#(50,128) # 50å€‹pos id(0~50)ç”¨128ç¶­vectorä¾†è¡¨ç¤ºä½ç½®è³‡è¨Š\n",
    "\n",
    "    def forward(self, x):# x: torch.Size([bsz, 50]) # 50å€‹å­çµæ§‹id\n",
    "            # seq_length = seq.size(1) #seq:(batchsize=64,50)# seq_length:50 # 50å€‹onehot categorical id ğœ–(0~2585)\n",
    "            # position_ids = torch.arange(seq_length, dtype=torch.long, device=seq.device) #position_ids:torch.Size([50]) (0~50)\n",
    "            # position_ids = position_ids.unsqueeze(0).expand_as(seq)#position_ids:torch.Size([bsz, 50])\n",
    "        seq_length = x.size(1) #x:(batchsize=64,50)# 50å€‹onehot categorical id ğœ–(0~2585)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=x.device) #position_ids:torch.Size([50]) (0~50)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(x) #position_ids =>torch.Size([bsz, 50])\n",
    "        return self.position_embeddings(position_ids)# generate torch.Size([bsz, 50, 128])ä½ç½®ç‰¹å¾µï¼Œæ¯ä¸€å€‹ä½ç½®éƒ½ç”¨128ç‚ºä¾†æè¿°\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, variance_epsilon=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = variance_epsilon\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta\n",
    "\n",
    "class Embeddings(nn.Module): # word embedding + positional encoding\n",
    "    \"\"\"Construct the embeddings from protein/target, position embeddings.\"\"\"\n",
    "    def __init__(self, hidden_size,max_drug_len,hidden_dropout_prob, pos_emb_type,substructure_size):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(substructure_size, hidden_size)#(2586,128)# 50å€‹onehot categorical id(0~2585)ç”¨128ç¶­ä¾†è¡¨ç¤ºé¡åˆ¥è³‡è¨Š\n",
    "        self.pos_emb_type = pos_emb_type\n",
    "        if pos_emb_type == \"learned\":#Learned Positional Embedding\n",
    "            self.position_embeddings = LearnedPositionalEncoding(hidden_size, max_len=max_drug_len)#(128,50)\n",
    "                #self.position_embeddings = nn.Embedding(max_drug_len, hidden_size)#(50, 128)# 50å€‹pos id(0~50)ç”¨128ç¶­vectorä¾†è¡¨ç¤ºä½ç½®è³‡è¨Š\n",
    "        elif pos_emb_type == \"sinusoidal\":#Sinusoidal Position Encoding\n",
    "            self.position_embeddings = SinusoidalPositionalEncoding(hidden_size, max_len=max_drug_len)#(128,50)\n",
    "            \n",
    "        self.LayerNorm = LayerNorm(hidden_size)#128\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)#0.1\n",
    "    def forward(self, seq):# torch.Size([bsz, 50]) # 50å€‹å­çµæ§‹id \n",
    "        words_embeddings = self.word_embeddings(seq) #seq:(bsz=64,50)# generate(bze,50,128)é¡åˆ¥ç‰¹å¾µ\n",
    "        # words_embeddings: torch.Size([bsz, 50, 128])50å€‹sub,å…¶å°æ‡‰çš„representation\n",
    "    #Learned Positional Embedding\n",
    "        if self.pos_emb_type == \"learned\":\n",
    "            position_embeddings = self.position_embeddings(seq)# generate torch.Size([bsz, 50, 128])ä½ç½®ç‰¹å¾µ\n",
    "            #position_embeddings: torch.Size([bsz, 50, 128])\n",
    "            embeddings = words_embeddings + position_embeddings # embeddings:torch.Size([bsz, 50, 128])\n",
    "    #Sinusoidal Position Encoding\n",
    "        elif self.pos_emb_type == \"sinusoidal\":\n",
    "            embeddings = self.position_embeddings(words_embeddings)  # Shape: [bsz, 50, 128] \n",
    "         \n",
    "        embeddings = self.LayerNorm(embeddings)#LayerNorm embeddings torch.Size([bsz, 50, 128])\n",
    "        embeddings = self.dropout(embeddings)#dropout embeddings torch.Size([bsz, 50, 128])\n",
    "        return embeddings # emb.shape:torch.Size([bsz, 50, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nusage:\\ninit:\\n    self.type_encoding=Type_Encoding(num_types = 2, hidden_size=128 )\\nforward:\\n    type_embeddings = self.type_encoding(append_embeddings) #append_embeddings: torch.Size([bsz, 50+c, 128]) # type_embeddings:torch.Size([bsz, 50+c, 128])\\n    append_embeddings = append_embeddings + type_embeddings # embeddings:torch.Size([bsz, 50+c, 128])\\n\\nä½†æˆ‘è¦ºå¾—ç”¨128ç¶­ä¾†è¡¨ç¤ºtypeè³‡è¨Šå¤ªå¤šäº†ã€‚\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Type_Encoding(nn.Module):\n",
    "    def __init__(self,hidden_size, num_types = 2):\n",
    "    \n",
    "        self.type_embedding = nn.Embedding(num_types, hidden_size) #(2,128)\n",
    "        \n",
    "    def forward(self, append_embeddings):\n",
    "        type_ids = torch.cat([torch.zeros(50), torch.ones(2)])# [0, 0, ..., 0, 1, 1] #torch.Size([52])\n",
    "        type_ids = type_ids.unsqueeze(0).expand(append_embeddings.size(0), -1)  # [bsz, 52], -1:è¡¨ç¤ºä¿æŒåŸæœ¬çš„ 52\n",
    "        return self.type_embedding(type_ids)# generate torch.Size([bsz, 52, 128])ä½ç½®ç‰¹å¾µï¼Œæ¯ä¸€å€‹ä½ç½®éƒ½ç”¨128ç‚ºä¾†æè¿°\n",
    "'''\n",
    "usage:\n",
    "init:\n",
    "    self.type_encoding=Type_Encoding(num_types = 2, hidden_size=128 )\n",
    "forward:\n",
    "    type_embeddings = self.type_encoding(append_embeddings) #append_embeddings: torch.Size([bsz, 50+c, 128]) # type_embeddings:torch.Size([bsz, 50+c, 128])\n",
    "    append_embeddings = append_embeddings + type_embeddings # embeddings:torch.Size([bsz, 50+c, 128])\n",
    "\n",
    "ä½†æˆ‘è¦ºå¾—ç”¨128ç¶­ä¾†è¡¨ç¤ºtypeè³‡è¨Šå¤ªå¤šäº†ã€‚\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob):\n",
    "        super(SelfAttention, self).__init__()# (128,8,0.1)\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads #8 é ­æ•¸\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)#128/8=16 é ­çš„ç¶­åº¦\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size#8*16=128 é ­çš„ç¶­åº¦ç¸½å’Œç­‰æ–¼featureæ•¸\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "\n",
    "        self.dropout = nn.Dropout(attention_probs_dropout_prob)#0.1\n",
    "\n",
    "    def transpose_for_scores(self, x): # x: torch.Size([bsz, 50, 128]) # diveide the whole 128 features into 8 heads, result 16 features per head\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) # (8,16)\n",
    "        # x.size()[:-1] torch.Size([bsz, 50]) # new_x_shape: torch.Size([bsz, 50, 8, 16])\n",
    "        x = x.view(*new_x_shape) # changes the shape of x to the new_x_shape # x torch.Size([bsz, 50, 8, 16])\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask): \n",
    "        # hidden_states:emb.shape:torch.Size([bsz, 50, 128]); attention_mask: ex_e_mask:torch.Size([bsz, 1, 1, 50])\n",
    "        mixed_query_layer = self.query(hidden_states) #hidden_states: torch.Size([bsz, 50, 128])\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states) # mixed_value_layer: torch.Size([bsz, 50, 128])\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer) #value_layer:torch.Size([bsz, 8, 50, 16])\n",
    "        \n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))# key_layer.transpose(-1, -2):torch.Size([bsz, 8, 16, 50])\n",
    "        # attention_scores:torch.Size([bsz, 8, 50, 50])\n",
    "        # Scaled Dot-Product: Prevent the dot products from growing too large, causing gradient Vanishing.\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size) # /16\n",
    "        # attention_scores:torch.Size([bsz, 8, 50, 50])\n",
    "        attention_scores = attention_scores + attention_mask #torch.Size([bsz, 1, 1, 50])[-0,-0,-0,-0,....,-10000,-10000,....]\n",
    "        # attention_scores+ attention_mask:torch.Size([bsz, 8, 50, 50])\n",
    "        \n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs_0 = nn.Softmax(dim=-1)(attention_scores) # attSention_probs:torch.Size([bsz, 8, 50, 50])\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs_drop = self.dropout(attention_probs_0)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs_drop, value_layer) #context_layer:torch.Size([bsz, 8, 50, 16])\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous() #context_layer:torch.Size([bsz, 50, 8, 16])\n",
    "        # context_layer.size()[:-2] torch.Size([bsz, 50])\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) #new_context_layer_shape:torch.Size([bsz, 50, 128]) #(128,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape) #context_layer:torch.Size([bsz, 50, 128])\n",
    "        return context_layer, attention_probs_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfOutput(nn.Module): # apply linear and skip conneaction and LayerNorm and dropout after self-attention\n",
    "    def __init__(self, hidden_size, dropout_prob):\n",
    "        super(SelfOutput, self).__init__()# (128,0.1)\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n",
    "        super(Attention, self).__init__()\n",
    "        self.selfAttention = SelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        self.output = SelfOutput(hidden_size, hidden_dropout_prob)# apply linear and skip conneaction and LayerNorm and dropout after self-attention\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        # input_tensor:emb.shape:torch.Size([64, 50, 128]); attention_mask: ex_e_mask:torch.Size([64, 1, 1, 50])\n",
    "        self_output, attention_probs_0 = self.selfAttention(input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output, attention_probs_0     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intermediate(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super(Intermediate, self).__init__()# (128,512)\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.relu(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class Output(nn.Module):# do linear, skip connection, LayerNorm, dropout after intermediate(Feed Forward block)\n",
    "    def __init__(self, intermediate_size, hidden_size, hidden_dropout_prob):\n",
    "        super(Output, self).__init__()# (512,128,0.1)\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states # transformer æœ€å¾Œçš„è¼¸å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  # Transformer Encoder for drug feature # Drug_SelfAttention\n",
    "    def __init__(self, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):#(128,512,8,0.1,0.1)\n",
    "        super(Encoder, self).__init__() # (128,512,8,0.1,0.1)\n",
    "        self.attention = Attention(hidden_size, num_attention_heads,\n",
    "                                   attention_probs_dropout_prob, hidden_dropout_prob)# (128,8,0.1,0.1)\n",
    "        self.intermediate = Intermediate(hidden_size, intermediate_size)# (128,512)\n",
    "        self.output = Output(intermediate_size, hidden_size, hidden_dropout_prob)# (512,128,0.1)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        # hidden_states:emb.shape:torch.Size([64, 50, 128]); attention_mask: ex_e_mask:torch.Size([64, 1, 1, 50])\n",
    "        attention_output,attention_probs_0 = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output , attention_probs_0    # transformer æœ€å¾Œçš„è¼¸å‡º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Omics_DCSA_Model(nn.Module):\n",
    "    def __init__(self,omics_encode_dim_dict,drug_encode_dims, activation_func,activation_func_final,dense_layer_dim, device, ESPF, Drug_SelfAttention, pos_emb_type,\n",
    "                 hidden_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeaetures_dict, max_drug_len, TCGA_pretrain_weight_path_dict=None):\n",
    "        super(Omics_DCSA_Model, self).__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        def load_TCGA_pretrain_weight(model, pretrained_weights_path, device):\n",
    "            state_dict = torch.load(pretrained_weights_path, map_location=device)  # Load the state_dict\n",
    "            encoder_state_dict = {key[len(\"encoder.\"):]: value for key, value in state_dict.items() if key.startswith('encoder')}  # Extract encoder weights\n",
    "            model.load_state_dict(encoder_state_dict)  # Load only the encoder part\n",
    "            model_keys = set(model.state_dict().keys())  # Check if the keys match\n",
    "            loaded_keys = set(encoder_state_dict.keys())\n",
    "            if model_keys == loaded_keys:\n",
    "                print(f\"State_dict for {model} loaded successfully.\")\n",
    "            else:\n",
    "                print(f\"State_dict does not match the model's architecture for {model}.\")\n",
    "                print(\"Model keys: \", model_keys, \" Loaded keys: \", loaded_keys)\n",
    "        \n",
    "        self.MLP4omics_dict = nn.ModuleDict()\n",
    "        for omic_type in omics_numfeaetures_dict.keys():\n",
    "            self.MLP4omics_dict[omic_type] = nn.Sequential(\n",
    "                nn.Linear(omics_numfeaetures_dict[omic_type], omics_encode_dim_dict[omic_type][0]),\n",
    "                activation_func,\n",
    "                nn.Linear(omics_encode_dim_dict[omic_type][0], omics_encode_dim_dict[omic_type][1]),\n",
    "                activation_func_final,\n",
    "                nn.Linear(omics_encode_dim_dict[omic_type][1], omics_encode_dim_dict[omic_type][2])\n",
    "            )\n",
    "            # Initialize with TCGA pretrain weight\n",
    "            if TCGA_pretrain_weight_path_dict is not None:\n",
    "                load_TCGA_pretrain_weight(self.MLP4omics_dict[omic_type], TCGA_pretrain_weight_path_dict[omic_type], device)\n",
    "            else: # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "                self._init_weights(self.MLP4omics_dict[omic_type])\n",
    "\n",
    "            #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "            self.match_drug_dim = nn.Linear(omics_encode_dim_dict[omic_type][2], hidden_size)\n",
    "            self._init_weights(self.match_drug_dim)\n",
    "        \n",
    "#ESPF            \n",
    "        self.emb_f = Embeddings(hidden_size,max_drug_len,hidden_dropout_prob, pos_emb_type,substructure_size = 2586)#(128,50,0.1,2586)\n",
    "        \n",
    "        if Drug_SelfAttention is False: \n",
    "            self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "        # self.output = SelfOutput(hidden_size, hidden_dropout_prob) # (128,0.1) # apply linear and skip conneaction and LayerNorm and dropout after attention\n",
    "# if attention is True  \n",
    "        elif Drug_SelfAttention is True: \n",
    "            self.TransformerEncoder = Encoder(hidden_size, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob)#(128,512,8,0.1,0.1)\n",
    "\n",
    "# Drug_Cell_SelfAttention\n",
    "        self.Drug_Cell_SelfAttention = Encoder(hidden_size+num_attention_heads, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob)#(128+8,512,8,0.1,0.1)\n",
    "\n",
    "# Define the final prediction network \n",
    "        dense_layer_dim=7064\n",
    "        self.model_final_add = nn.Sequential(\n",
    "            nn.Linear(7064, 700),\n",
    "            activation_func,\n",
    "            nn.Dropout(p=0),\n",
    "            nn.Linear(700, 70),\n",
    "            activation_func,\n",
    "            nn.Dropout(p=0),\n",
    "            nn.Linear(70, 1),\n",
    "            activation_func_final)\n",
    "        # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "        self._init_weights(self.model_final_add)\n",
    "\n",
    "        self.print_flag = True\n",
    "        self.attention_probs = None # store Attention score matrix\n",
    "    \n",
    "    def _init_weights(self, model):\n",
    "        if isinstance(model, nn.Linear):  # ç›´æ¥åˆå§‹åŒ– nn.Linear å±¤\n",
    "            init.kaiming_uniform_(model.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "            if model.bias is not None:\n",
    "                init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.ModuleList) or isinstance(model, nn.Sequential):  # éæ­·å­å±¤\n",
    "            for layer in model:\n",
    "                self._init_weights(layer)\n",
    "\n",
    "    def forward(self, omics_tensor_dict,drug, device, **kwargs):\n",
    "        Drug_SelfAttention = kwargs['Drug_SelfAttention']\n",
    "        omic_embeddings_ls = []\n",
    "        # Loop through each omic type and pass through its respective model\n",
    "        for omic_type, omic_tensor in omics_tensor_dict.items():\n",
    "            omic_embed = self.MLP4omics_dict[omic_type](omic_tensor.to(device=device)) #(bsz, 50)\n",
    "            #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "            omic_embed = self.match_drug_dim(omic_embed) #(bsz, 128)\n",
    "            omic_embeddings_ls.append(omic_embed)\n",
    "        # omic_embeddings = torch.cat(omic_embeddings_ls, dim=1)  # change list to tensor, because omic_embeddings need to be tensor to torch.cat([omic_embeddings, drug_emb_masked], dim=1) \n",
    "\n",
    "    #ESPF encoding        \n",
    "        mask = drug[:, 1, :].to(device=device) # torch.Size([bsz, 50]),dytpe(long)\n",
    "        drug_embed = drug[:, 0, :].to(device=device) # drug_embed :torch.Size([bsz, 50]),dytpe(long)\n",
    "        drug_embed = self.emb_f(drug_embed) # (bsz, 50, 128) #word embeddingã€position encodingã€LayerNormã€dropout\n",
    "        # Embeddings take int inputs, so no need to convert to float like nn.Linear layer\n",
    "        \n",
    "# mask for Drug Cell SelfAttention\n",
    "        omics_items = torch.ones(mask.size(0), len(omic_embeddings_ls), dtype=mask.dtype, device=mask.device)  # Shape: [bsz, len(omic_embeddings_ls)]\n",
    "        DrugCell_mask = torch.cat([mask, omics_items], dim=1)  # Shape: [bsz, 50 + len(omic_embeddings_ls)]\n",
    "\n",
    "        if Drug_SelfAttention is False:\n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n Drug_SelfAttention is not applied \\n\")\n",
    "                self.print_flag  = False\n",
    "            # to apply mask to emb, treat mask like attention score matrix (weight), then do softmax and dropout, then multiply with emb\n",
    "            mask_weight =mask.clone().float().unsqueeze(1).repeat(1, 50, 1)# (bsz, 50)->(bsz,50,50)\n",
    "            mask_weight = (1.0 - mask_weight) * -10000.0\n",
    "            mask_weight = nn.Softmax(dim=-1)(mask_weight)\n",
    "            mask_weight = self.dropout(mask_weight)\n",
    "            drug_emb_masked = torch.matmul(mask_weight, drug_embed) # emb_masked: torch.Size([bsz, 50, 128])\n",
    "            # æ²’åš: class SelfOutput(nn.Module): # apply linear and skip conneaction and LayerNorm and dropout after \n",
    "            # æ²’åš: positional encoding\n",
    "            \n",
    "        elif Drug_SelfAttention is True:\n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n Drug_SelfAttention is applied \\n\")\n",
    "                self.print_flag  = False\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2) # mask.shape: torch.Size([bsz, 1, 1, 50])\n",
    "            mask = (1.0 - mask) * -10000.0\n",
    "            drug_emb_masked, AttenScorMat_DrugSelf  = self.TransformerEncoder(drug_embed, mask)# hidden_states:drug_embed.shape:torch.Size([bsz, 50, 128]); mask: ex_e_mask:torch.Size([bsz, 1, 1, 50])\n",
    "            # drug_emb_masked: torch.Size([bsz, 50, 128]) \n",
    "            # attention_probs_0 = nn.Softmax(dim=-1)(attention_scores) # attention_probs_0:torch.Size([bsz, 8, 50, 50])(without dropout)\n",
    "        elif Drug_SelfAttention is None:\n",
    "                print(\"\\n Drug_SelfAttention is assign to None , please assign to False or True \\n\")\n",
    "\n",
    "# Drug_Cell_SelfAttention\n",
    "        # omic_embeddings_ls:[(bsz,128),(bsz,128)] \n",
    "        # # drug_emb_masked:[bsz,50,128] #å·²ç¶“åšå®Œword embeddingå’Œposition encoding\n",
    "\n",
    "        omic_embeddings = torch.stack(omic_embeddings_ls, dim=1) #shape:[bsz,c,128] #Stack omic_embeddings_ls along the second dimension, c: number of omic types\n",
    "\n",
    "        \n",
    "        append_embeddings = torch.cat([drug_emb_masked, omic_embeddings], dim=1) #shape:[bsz,50+c,128] #Concatenate along the second dimension\n",
    "# Type encoding (to distinguish between drug and omics)\n",
    "        drug_type_encoding = torch.ones_like(drug_emb_masked[..., :1])  # Shape: [bsz, 50, 1]\n",
    "        omics_type_encoding = torch.zeros_like(omic_embeddings[..., :1])  # Shape: [bsz, i, 1]\n",
    "        # Concatenate type encoding with the respective data\n",
    "        drug_emb_masked = torch.cat([drug_emb_masked, drug_type_encoding], dim=-1)  # Shape: [bsz, 50, 129]\n",
    "        omic_embeddings = torch.cat([omic_embeddings, omics_type_encoding], dim=-1)  # Shape: [bsz, c, 129]\n",
    "\n",
    "        # Final concatenated tensor (drug sequence and omics data with type encoding)\n",
    "        append_embeddings = torch.cat([drug_emb_masked, omic_embeddings], dim=1)  # Shape: [bsz, 50+c, 129]\n",
    "\n",
    "        padding_dim = self.num_attention_heads - 1  # Extra dimensions to add # padding_dim=7\n",
    "\n",
    "        pad = torch.zeros(append_embeddings.size(0), append_embeddings.size(1), padding_dim, device=append_embeddings.device)\n",
    "        append_embeddings = torch.cat([append_embeddings, pad], dim=-1)  # New shape: [bsz, 50+i, new_hidden_size]\n",
    "        \n",
    "        DrugCell_mask = DrugCell_mask.unsqueeze(1).unsqueeze(2) \n",
    "        DrugCell_mask = (1.0 - DrugCell_mask) * -10000.0 # mask.shape: torch.Size([bsz, 1, 1, 50+c])\n",
    "        append_embeddings, AttenScorMat_DrugCellSelf  = self.Drug_Cell_SelfAttention(append_embeddings, DrugCell_mask)\n",
    "        # append_embeddings: torch.Size([bsz, 50+c, 136]) # AttenScorMat_DrugCellSelf:torch.Size([bsz, 8, 50+c, 50+c])(without dropout)\n",
    "\n",
    "        #skip connect the omics embeddings # not as necessary as skip connect the drug embeddings \n",
    "        append_embeddings = torch.cat([ torch.cat(omic_embeddings_ls, dim=1), append_embeddings.reshape(append_embeddings.size(0), -1)], dim=1) # dim=1: turn into 1D \n",
    "        #omic_embeddings_ls(bsz, 128) , append_embeddings(bsz, 50+c, 136)\n",
    "        # drug æœ‰50*128ï¼Œomicesæœ‰i*128ï¼Œå¯èƒ½æœƒå·®å¤ªå¤šï¼Œçœ‹drugè¦ä¸è¦å…ˆé™ç¶­æ ¹omicsä¸€æ¨£i*128 # å…ˆä¸è¦\n",
    "    \n",
    "    # Final MLP\n",
    "        output = self.model_final_add(append_embeddings)\n",
    "\n",
    "        return output, AttenScorMat_DrugSelf, AttenScorMat_DrugCellSelf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n",
      "Exp tensor shape: torch.Size([76, 4692])\n",
      "Exp num_features 4692\n",
      "batch_size 3 num_epoch: 2\n",
      "drug_df (42, 9)\n",
      "AUC_df (76, 42)\n",
      "kfoldCV 2\n",
      "num_ccl,num_drug:  76 42\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "# import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader, Subset\n",
    "import torch.nn.init as init\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import importlib.util\n",
    "\n",
    "from ESPF_drug2emb import drug2emb_encoder\n",
    "from Model import Omics_DrugESPF_Model, Omics_DCSA_Model\n",
    "from split_data_id import split_id,repeat_func\n",
    "from create_dataloader import OmicsDrugDataset\n",
    "from train import train, evaluation\n",
    "from correlation import correlation_func\n",
    "from plot import loss_curve, correlation_density,Density_Plot_of_AUC_Values\n",
    "from tools import get_data_value_range,set_seed,get_vram_usage\n",
    "from Metrics import MetricsCalculator\n",
    "\n",
    "\n",
    "from Loss import Custom_LossFunction,Custom_Weighted_LossFunction\n",
    "from Custom_Activation_Function import ScaledSigmoid\n",
    "\n",
    "test = True #False, True: batch_size = 3, num_epoch = 2, full dataset\n",
    "\n",
    "omics_files = {\n",
    "    'Mut': \"../../../data/CCLE/CCLE_match_TCGAgene_PRISMandEXPsample_binary_mutation_476_6009.txt\",\n",
    "    'Exp': \"../../../data/CCLE/CCLE_exp_476samples_4692genes.txt\",\n",
    "    # Add more omics types and paths as needed\n",
    "    }\n",
    "omics_dict = {'Mut':0,'Exp':1,'CN':2, 'Eff':3, 'Dep':4, 'Met':5}\n",
    "omics_data_dict = {}\n",
    "omics_data_tensor_dict = {}\n",
    "omics_numfeatures_dict = {}\n",
    "omics_encode_dim_dict ={'Mut':[1000,100,50],'Exp':[1000,100,50], # Dr.Chiu:exp[500,200,50]\n",
    "                        'CN':[100,50,30], 'Eff':[100,50,30], 'Dep':[100,50,30], 'Met':[100,50,30]}\n",
    "\n",
    "TCGA_pretrain_weight_path_dict = {'Mut': \"../../results/Encoder_tcga_mut_1000_100_50_best_loss_0.0066.pt\",\n",
    "                                  'Exp': \"../../results/Encoder_tcga_exp_1000_100_50_best_loss_0.7.pt\",\n",
    "                                  # Add more omics types and paths as needed\n",
    "                                }\n",
    "seed = 42\n",
    "#hyperparameter\n",
    "model_name = \"Omics_DCSA_Model\" # Omics_DCSA_Model\n",
    "AUCtransform = \"-log2\" #\"-log2\"\n",
    "splitType= 'byDrug' # byCCL byDrug\n",
    "kfoldCV = 5\n",
    "include_omics = ['Exp']\n",
    "max_drug_len=50 # ä¸å¤ è£œé›¶è£œåˆ°50 / è¶…éå–å‰50å€‹subwords(index) !!!!é ˆæ”¹æ–¹æ³•!!!!\n",
    "drug_embedding_feature_size = 128\n",
    "ESPF = True # False True\n",
    "Drug_SelfAttention = True\n",
    "pos_emb_type = 'sinusoidal' # 'learned' 'sinusoidal'\n",
    "#éœ€å†ä¿®æ”¹-----------\n",
    "\n",
    "intermediate_size =512\n",
    "num_attention_heads = 8        \n",
    "attention_probs_dropout_prob = 0.1\n",
    "hidden_dropout_prob = 0.1\n",
    "\n",
    "if ESPF is True:\n",
    "    \n",
    "    drug_encode_dims =[1600,400,100] # 50*128\n",
    "    dense_layer_dim = sum(omics_encode_dim_dict[omic_type][2] for omic_type in include_omics) + drug_encode_dims[2] # MLPDim\n",
    "elif ESPF is False:\n",
    "    \n",
    "    drug_encode_dims =[110,55,22]\n",
    "    dense_layer_dim = sum(omics_encode_dim_dict[omic_type][2] for omic_type in include_omics) + drug_encode_dims[2] # MLPDim\n",
    "#éœ€å†ä¿®æ”¹-------------\n",
    "TrackGradient = False # False True\n",
    "\n",
    "activation_func = nn.ReLU()  # ReLU activation function # Leaky ReLu\n",
    "activation_func_final = ScaledSigmoid(scale=8) # GroundT range ( 0 ~ scale )\n",
    "#nn.Sigmoid()or ReLU() or Linear/identity(when -log2AUC)\n",
    "batch_size = 200\n",
    "num_epoch = 200 # for k fold CV \n",
    "patience = 20\n",
    "warmup_iters = 60\n",
    "Decrease_percent = 0.9\n",
    "continuous = True\n",
    "learning_rate=1e-05\n",
    "criterion = Custom_Weighted_LossFunction(loss_type=\"weighted_MSE\", loss_lambda=1.0, regular_type=\"L2\", regular_lambda=1e-05) #nn.MSELoss()#\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "set_seed(seed)\n",
    "for omic_type in include_omics:\n",
    "    # Read the file\n",
    "    omics_data_dict[omic_type] = pd.read_csv(omics_files[omic_type], sep='\\t', index_col=0)\n",
    "\n",
    "    if test is True:\n",
    "        # Specify the index as needed\n",
    "        omics_data_dict[omic_type] = omics_data_dict[omic_type][:76]  # Adjust the row selection as needed\n",
    "    omics_data_tensor_dict[omic_type]  = torch.tensor(omics_data_dict[omic_type].values, dtype=torch.float32)\n",
    "    omics_numfeatures_dict[omic_type] = omics_data_tensor_dict[omic_type].shape[1]\n",
    "    print(f\"{omic_type} tensor shape:\", omics_data_tensor_dict[omic_type].shape)\n",
    "    print(f\"{omic_type} num_features\",omics_numfeatures_dict[omic_type])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "#load data\n",
    "# data_mut, gene_names_mut,ccl_names_mut  = load_ccl(\"/root/data/CCLE/CCLE_match_TCGAgene_PRISMandEXPsample_binary_mutation_476_6009.txt\")\n",
    "drug_df= pd.read_csv(\"../../../data/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/MACCS(Secondary_Screen_treatment_info)_union_NOrepeat.csv\", sep=',', index_col=0)\n",
    "AUC_df = pd.read_csv(\"../../../data/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/Drug_sensitivity_AUC_(PRISM_Repurposing_Secondary_Screen)_subsetted_NOrepeat.csv\", sep=',', index_col=0)\n",
    "# data_AUC_matrix, drug_names_AUC, ccl_names_AUC = load_AUC_matrix(splitType,\"/root/Winnie/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/Drug_sensitivity_AUC_(PRISM_Repurposing_Secondary_Screen)_subsetted.csv\") # splitType = \"byCCL\" or \"byDrug\" æ±ºå®šAUCmatrixè¦ä¸è¦è½‰ç½®\n",
    "\n",
    "# matched AUCfile and omics_data samples\n",
    "matched_samples = sorted(set(AUC_df.T.columns) & set(list(omics_data_dict.values())[0].T.columns))\n",
    "\n",
    "AUC_df= (AUC_df.T[matched_samples]).T\n",
    "if AUCtransform == \"-log2\":\n",
    "    AUC_df = -np.log2(AUC_df)\n",
    "if AUCtransform == \"-log10\":\n",
    "    AUC_df = -np.log10(AUC_df)\n",
    "    \n",
    "if test is True:\n",
    "    batch_size = 3\n",
    "    num_epoch = 2\n",
    "    print(\"batch_size\",batch_size,\"num_epoch:\",num_epoch)\n",
    "    drug_df=drug_df[:42]\n",
    "    AUC_df=AUC_df.iloc[:76,:42]\n",
    "    print(\"drug_df\",drug_df.shape)\n",
    "    print(\"AUC_df\",AUC_df.shape)\n",
    "    kfoldCV = 2\n",
    "    print(\"kfoldCV\",kfoldCV)\n",
    "\n",
    "if 'weighted' in criterion.loss_type :    \n",
    "    # Set threshold based on the 90th percentile # å°‡é«˜æ–¼thresholdçš„AUCæ¬Šé‡å¢åŠ \n",
    "    weighted_threshold = np.nanpercentile(AUC_df.values, 90)    \n",
    "    total_samples = (~np.isnan(AUC_df.values)).sum().item()\n",
    "    few_samples = (AUC_df.values > weighted_threshold).sum().item()\n",
    "    more_samples = total_samples - few_samples\n",
    "    few_weight = total_samples / (2 * few_samples)  \n",
    "    more_weight = total_samples / (2 * more_samples)   \n",
    "    # print(\"weighted_threshold\",weighted_threshold)\n",
    "    # print(\"total_samples\",total_samples)\n",
    "    # print(\"few_samples\",few_samples)\n",
    "    # print(\"more_samples\",more_samples)\n",
    "    # print(\"few_weight\",few_weight)\n",
    "    # print(\"more_weight\",more_weight)\n",
    "else:\n",
    "    weighted_threshold = None\n",
    "    few_weight = None\n",
    "    more_weight = None\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# æª¢æŸ¥æœ‰ç„¡é‡è¤‡çš„SMILES\n",
    "if ESPF is True:\n",
    "    drug_smiles =drug_df[\"smiles\"] # \n",
    "    drug_names =drug_df.index\n",
    "    # æŒ‘å‡ºé‡è¤‡çš„SMILES\n",
    "    duplicate =  drug_smiles[drug_smiles.duplicated(keep=False)]\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------\n",
    "    #ESPF\n",
    "    vocab_path = \"../../ESPF/drug_codes_chembl_freq_1500.txt\" # token\n",
    "    sub_csv = pd.read_csv(\"../../ESPF/subword_units_map_chembl_freq_1500.csv\")# token with frequency\n",
    "\n",
    "    # drug_encode = pd.Series(drug_smiles.unique()).apply(drug2emb_encoder, args=(vocab_path, sub_csv, max_drug_len))# å°‡drug_smiles ä½¿ç”¨_drug2emb_encoder functionç·¨ç¢¼æˆsubword vector\n",
    "    drug_encode = pd.Series(drug_smiles).apply(drug2emb_encoder, args=(vocab_path, sub_csv, max_drug_len))\n",
    "    # uniq_smile_dict = dict(zip(drug_smiles.unique(),drug_encode))# zip drug_smileså’Œå…¶subword vectorç·¨ç¢¼ æˆå­—å…¸\n",
    "\n",
    "    # print(type(smile_encode))\n",
    "    # print(smile_encode.shape)\n",
    "    # print(type(smile_encode.index))\n",
    "    # print((drug_encode.index.values).shape)#(42,)\n",
    "    # print((drug_encode).shape)#(42,)\n",
    "    # print(type(drug_encode))#<class 'pandas.core.series.Series'>\n",
    "    #print((drug_encode.values).shape)#(42,)\n",
    "    # print(drug_encode.values.tolist())\n",
    "    # Convert your data to tensors if they're in numpy\n",
    "    drug_features_tensor = torch.tensor(np.array(drug_encode.values.tolist()), dtype=torch.long)\n",
    "else:\n",
    "    drug_encode = drug_df[\"MACCS166bits\"]\n",
    "    drug_encode_list = [list(map(int, item.split(','))) for item in drug_encode.values]\n",
    "    print(\"MACCS166bits_drug_encode_list type: \",type(drug_encode_list))\n",
    "    # Convert your data to tensors if they're in numpy\n",
    "    drug_features_tensor = torch.tensor(np.array(drug_encode_list), dtype=torch.long)\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "num_ccl = list(omics_data_dict.values())[0].shape[0]\n",
    "num_drug = drug_encode.shape[0]\n",
    "print(\"num_ccl,num_drug: \",num_ccl,num_drug)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# k-fold run\n",
    "kfold_losses= {}\n",
    "best_fold_train_epoch_loss_list = []#  for train every epoch loss plot (best_fold)\n",
    "best_fold_val_epoch_loss_list = []#  for validation every epoch loss plot (best_fold)\n",
    "best_test_loss = float('inf')\n",
    "best_fold_best_weight=None\n",
    "set_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State_dict for Sequential(\n",
      "  (0): Linear(in_features=4692, out_features=1000, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (3): ScaledSigmoid(scale=8)\n",
      "  (4): Linear(in_features=100, out_features=50, bias=True)\n",
      ") loaded successfully.\n",
      "Omics_DCSA_model(\n",
      "  (MLP4omics_dict): ModuleDict(\n",
      "    (Exp): Sequential(\n",
      "      (0): Linear(in_features=4692, out_features=1000, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "      (3): ScaledSigmoid(scale=8)\n",
      "      (4): Linear(in_features=100, out_features=50, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (match_drug_dim): Linear(in_features=50, out_features=128, bias=True)\n",
      "  (emb_f): Embeddings(\n",
      "    (word_embeddings): Embedding(2586, 128)\n",
      "    (position_embeddings): SinusoidalPositionalEncoding()\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (TransformerEncoder): Encoder(\n",
      "    (attention): Attention(\n",
      "      (selfAttention): SelfAttention(\n",
      "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): SelfOutput(\n",
      "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (LayerNorm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): Intermediate(\n",
      "      (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "    )\n",
      "    (output): Output(\n",
      "      (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (Drug_Cell_SelfAttention): Encoder(\n",
      "    (attention): Attention(\n",
      "      (selfAttention): SelfAttention(\n",
      "        (query): Linear(in_features=136, out_features=136, bias=True)\n",
      "        (key): Linear(in_features=136, out_features=136, bias=True)\n",
      "        (value): Linear(in_features=136, out_features=136, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): SelfOutput(\n",
      "        (dense): Linear(in_features=136, out_features=136, bias=True)\n",
      "        (LayerNorm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): Intermediate(\n",
      "      (dense): Linear(in_features=136, out_features=512, bias=True)\n",
      "    )\n",
      "    (output): Output(\n",
      "      (dense): Linear(in_features=512, out_features=136, bias=True)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (model_final_add): Sequential(\n",
      "    (0): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0, inplace=False)\n",
      "    (3): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0, inplace=False)\n",
      "    (6): Linear(in_features=150, out_features=1, bias=True)\n",
      "    (7): ScaledSigmoid(scale=8)\n",
      "  )\n",
      ")\n",
      "ModuleDict(\n",
      "  (Exp): Sequential(\n",
      "    (0): Linear(in_features=4692, out_features=1000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (3): ScaledSigmoid(scale=8)\n",
      "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4692, out_features=1000, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (3): ScaledSigmoid(scale=8)\n",
      "  (4): Linear(in_features=100, out_features=50, bias=True)\n",
      ")\n",
      "Linear(in_features=4692, out_features=1000, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=1000, out_features=100, bias=True)\n",
      "ScaledSigmoid(scale=8)\n",
      "Sigmoid()\n",
      "Linear(in_features=100, out_features=50, bias=True)\n",
      "Linear(in_features=50, out_features=128, bias=True)\n",
      "Embeddings(\n",
      "  (word_embeddings): Embedding(2586, 128)\n",
      "  (position_embeddings): SinusoidalPositionalEncoding()\n",
      "  (LayerNorm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Embedding(2586, 128)\n",
      "SinusoidalPositionalEncoding()\n",
      "LayerNorm()\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Encoder(\n",
      "  (attention): Attention(\n",
      "    (selfAttention): SelfAttention(\n",
      "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): SelfOutput(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Intermediate(\n",
      "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "  )\n",
      "  (output): Output(\n",
      "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Attention(\n",
      "  (selfAttention): SelfAttention(\n",
      "    (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): SelfOutput(\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "SelfAttention(\n",
      "  (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "SelfOutput(\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (LayerNorm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "LayerNorm()\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Intermediate(\n",
      "  (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      ")\n",
      "Linear(in_features=128, out_features=512, bias=True)\n",
      "Output(\n",
      "  (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (LayerNorm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=512, out_features=128, bias=True)\n",
      "LayerNorm()\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Encoder(\n",
      "  (attention): Attention(\n",
      "    (selfAttention): SelfAttention(\n",
      "      (query): Linear(in_features=136, out_features=136, bias=True)\n",
      "      (key): Linear(in_features=136, out_features=136, bias=True)\n",
      "      (value): Linear(in_features=136, out_features=136, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): SelfOutput(\n",
      "      (dense): Linear(in_features=136, out_features=136, bias=True)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Intermediate(\n",
      "    (dense): Linear(in_features=136, out_features=512, bias=True)\n",
      "  )\n",
      "  (output): Output(\n",
      "    (dense): Linear(in_features=512, out_features=136, bias=True)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Attention(\n",
      "  (selfAttention): SelfAttention(\n",
      "    (query): Linear(in_features=136, out_features=136, bias=True)\n",
      "    (key): Linear(in_features=136, out_features=136, bias=True)\n",
      "    (value): Linear(in_features=136, out_features=136, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): SelfOutput(\n",
      "    (dense): Linear(in_features=136, out_features=136, bias=True)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "SelfAttention(\n",
      "  (query): Linear(in_features=136, out_features=136, bias=True)\n",
      "  (key): Linear(in_features=136, out_features=136, bias=True)\n",
      "  (value): Linear(in_features=136, out_features=136, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=136, out_features=136, bias=True)\n",
      "Linear(in_features=136, out_features=136, bias=True)\n",
      "Linear(in_features=136, out_features=136, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "SelfOutput(\n",
      "  (dense): Linear(in_features=136, out_features=136, bias=True)\n",
      "  (LayerNorm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=136, out_features=136, bias=True)\n",
      "LayerNorm()\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Intermediate(\n",
      "  (dense): Linear(in_features=136, out_features=512, bias=True)\n",
      ")\n",
      "Linear(in_features=136, out_features=512, bias=True)\n",
      "Output(\n",
      "  (dense): Linear(in_features=512, out_features=136, bias=True)\n",
      "  (LayerNorm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=512, out_features=136, bias=True)\n",
      "LayerNorm()\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=150, out_features=150, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0, inplace=False)\n",
      "  (3): Linear(in_features=150, out_features=150, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0, inplace=False)\n",
      "  (6): Linear(in_features=150, out_features=1, bias=True)\n",
      "  (7): ScaledSigmoid(scale=8)\n",
      ")\n",
      "Linear(in_features=150, out_features=150, bias=True)\n",
      "Dropout(p=0, inplace=False)\n",
      "Linear(in_features=150, out_features=150, bias=True)\n",
      "Dropout(p=0, inplace=False)\n",
      "Linear(in_features=150, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Omics_DCSA_Model(omics_encode_dim_dict, drug_encode_dims, activation_func, activation_func_final, dense_layer_dim, device, ESPF, Drug_SelfAttention, pos_emb_type,\n",
    "                            drug_embedding_feature_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeatures_dict, max_drug_len,\n",
    "                            TCGA_pretrain_weight_path_dict= TCGA_pretrain_weight_path_dict)\n",
    "\n",
    "for module in model.modules():\n",
    "    print(module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
