{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "  def __init__(self, hidden_size, max_len=50):\n",
    "    super(SinusoidalPositionalEncoding, self).__init__()\n",
    "    pe = torch.zeros(max_len, hidden_size) # torch.Size([50, 128])\n",
    "    position = torch.arange(0, max_len).float().unsqueeze(1) # torch.Size([50, 1]) # 0~50\n",
    "    div_term = torch.exp(torch.arange(0, hidden_size, 2).float() *\n",
    "                         (-torch.log(torch.Tensor([10000])) / hidden_size)) # [max_len / 2]\n",
    "                        #生成一個數位 [0， 2， 4， ...， hidden_size-2] 的張量（偶數索引）。\n",
    "    pe[:, 0::2] = torch.sin(position * div_term) #將 sine 函數應用於位置編碼張量 （pe） 的偶數維數。\n",
    "    pe[:, 1::2] = torch.cos(position * div_term) #將餘弦函數應用於位置編碼張量 （pe） 的奇數維數。\n",
    "    pe = pe.unsqueeze(0) # torch.Size([1, 50, 128])\n",
    "    # register pe to buffer and require no grads#緩衝區的參數在訓練期間不會更新\n",
    "    self.register_buffer('pe', pe)\n",
    "  def forward(self, x):\n",
    "    # x: [batch, seq_len, hidden_size]\n",
    "    # we can add positional encoding to x directly, and ignore other dimension\n",
    "    return x + self.pe[:,:x.size(1)].to(x.device)# x.size(1)= 50\n",
    "  \n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size, max_len):#(128, 50)\n",
    "        super().__init__()\n",
    "        self.position_embeddings = nn.Embedding(max_len, hidden_size)#(50,128) # 50個pos id(0~50)用128維vector來表示位置資訊\n",
    "\n",
    "    def forward(self, x):# x: torch.Size([bsz, 50]) # 50個子結構id\n",
    "            # seq_length = seq.size(1) #seq:(batchsize=64,50)# seq_length:50 # 50個onehot categorical id 𝜖(0~2585)\n",
    "            # position_ids = torch.arange(seq_length, dtype=torch.long, device=seq.device) #position_ids:torch.Size([50]) (0~50)\n",
    "            # position_ids = position_ids.unsqueeze(0).expand_as(seq)#position_ids:torch.Size([bsz, 50])\n",
    "        seq_length = x.size(1) #x:(batchsize=64,50)# 50個onehot categorical id 𝜖(0~2585)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=x.device) #position_ids:torch.Size([50]) (0~50)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(x) #position_ids =>torch.Size([bsz, 50])\n",
    "        return self.position_embeddings(position_ids)# generate torch.Size([bsz, 50, 128])位置特徵，每一個位置都用128為來描述\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, variance_epsilon=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = variance_epsilon\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta\n",
    "\n",
    "class Embeddings(nn.Module): # word embedding + positional encoding\n",
    "    \"\"\"Construct the embeddings from protein/target, position embeddings.\"\"\"\n",
    "    def __init__(self, hidden_size,max_drug_len,hidden_dropout_prob, pos_emb_type,substructure_size):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(substructure_size, hidden_size)#(2586,128)# 50個onehot categorical id(0~2585)用128維來表示類別資訊\n",
    "        self.pos_emb_type = pos_emb_type\n",
    "        if pos_emb_type == \"learned\":#Learned Positional Embedding\n",
    "            self.position_embeddings = LearnedPositionalEncoding(hidden_size, max_len=max_drug_len)#(128,50)\n",
    "                #self.position_embeddings = nn.Embedding(max_drug_len, hidden_size)#(50, 128)# 50個pos id(0~50)用128維vector來表示位置資訊\n",
    "        elif pos_emb_type == \"sinusoidal\":#Sinusoidal Position Encoding\n",
    "            self.position_embeddings = SinusoidalPositionalEncoding(hidden_size, max_len=max_drug_len)#(128,50)\n",
    "            \n",
    "        self.LayerNorm = LayerNorm(hidden_size)#128\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)#0.1\n",
    "    def forward(self, seq):# torch.Size([bsz, 50]) # 50個子結構id \n",
    "        words_embeddings = self.word_embeddings(seq) #seq:(bsz=64,50)# generate(bze,50,128)類別特徵\n",
    "        # words_embeddings: torch.Size([bsz, 50, 128])50個sub,其對應的representation\n",
    "    #Learned Positional Embedding\n",
    "        if self.pos_emb_type == \"learned\":\n",
    "            position_embeddings = self.position_embeddings(seq)# generate torch.Size([bsz, 50, 128])位置特徵\n",
    "            #position_embeddings: torch.Size([bsz, 50, 128])\n",
    "            embeddings = words_embeddings + position_embeddings # embeddings:torch.Size([bsz, 50, 128])\n",
    "    #Sinusoidal Position Encoding\n",
    "        elif self.pos_emb_type == \"sinusoidal\":\n",
    "            embeddings = self.position_embeddings(words_embeddings)  # Shape: [bsz, 50, 128] \n",
    "         \n",
    "        embeddings = self.LayerNorm(embeddings)#LayerNorm embeddings torch.Size([bsz, 50, 128])\n",
    "        embeddings = self.dropout(embeddings)#dropout embeddings torch.Size([bsz, 50, 128])\n",
    "        return embeddings # emb.shape:torch.Size([bsz, 50, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nusage:\\ninit:\\n    self.type_encoding=Type_Encoding(num_types = 2, hidden_size=128 )\\nforward:\\n    type_embeddings = self.type_encoding(append_embeddings) #append_embeddings: torch.Size([bsz, 50+c, 128]) # type_embeddings:torch.Size([bsz, 50+c, 128])\\n    append_embeddings = append_embeddings + type_embeddings # embeddings:torch.Size([bsz, 50+c, 128])\\n\\n但我覺得用128維來表示type資訊太多了。\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Type_Encoding(nn.Module):\n",
    "    def __init__(self,hidden_size, num_types = 2):\n",
    "    \n",
    "        self.type_embedding = nn.Embedding(num_types, hidden_size) #(2,128)\n",
    "        \n",
    "    def forward(self, append_embeddings):\n",
    "        type_ids = torch.cat([torch.zeros(50), torch.ones(2)])# [0, 0, ..., 0, 1, 1] #torch.Size([52])\n",
    "        type_ids = type_ids.unsqueeze(0).expand(append_embeddings.size(0), -1)  # [bsz, 52], -1:表示保持原本的 52\n",
    "        return self.type_embedding(type_ids)# generate torch.Size([bsz, 52, 128])位置特徵，每一個位置都用128為來描述\n",
    "'''\n",
    "usage:\n",
    "init:\n",
    "    self.type_encoding=Type_Encoding(num_types = 2, hidden_size=128 )\n",
    "forward:\n",
    "    type_embeddings = self.type_encoding(append_embeddings) #append_embeddings: torch.Size([bsz, 50+c, 128]) # type_embeddings:torch.Size([bsz, 50+c, 128])\n",
    "    append_embeddings = append_embeddings + type_embeddings # embeddings:torch.Size([bsz, 50+c, 128])\n",
    "\n",
    "但我覺得用128維來表示type資訊太多了。\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob):\n",
    "        super(SelfAttention, self).__init__()# (128,8,0.1)\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads #8 頭數\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)#128/8=16 頭的維度\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size#8*16=128 頭的維度總和等於feature數\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "\n",
    "        self.dropout = nn.Dropout(attention_probs_dropout_prob)#0.1\n",
    "\n",
    "    def transpose_for_scores(self, x): # x: torch.Size([bsz, 50, 128]) # diveide the whole 128 features into 8 heads, result 16 features per head\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) # (8,16)\n",
    "        # x.size()[:-1] torch.Size([bsz, 50]) # new_x_shape: torch.Size([bsz, 50, 8, 16])\n",
    "        x = x.view(*new_x_shape) # changes the shape of x to the new_x_shape # x torch.Size([bsz, 50, 8, 16])\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask): \n",
    "        # hidden_states:emb.shape:torch.Size([bsz, 50, 128]); attention_mask: ex_e_mask:torch.Size([bsz, 1, 1, 50])\n",
    "        mixed_query_layer = self.query(hidden_states) #hidden_states: torch.Size([bsz, 50, 128])\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states) # mixed_value_layer: torch.Size([bsz, 50, 128])\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer) #value_layer:torch.Size([bsz, 8, 50, 16])\n",
    "        \n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))# key_layer.transpose(-1, -2):torch.Size([bsz, 8, 16, 50])\n",
    "        # attention_scores:torch.Size([bsz, 8, 50, 50])\n",
    "        # Scaled Dot-Product: Prevent the dot products from growing too large, causing gradient Vanishing.\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size) # /16\n",
    "        # attention_scores:torch.Size([bsz, 8, 50, 50])\n",
    "        attention_scores = attention_scores + attention_mask #torch.Size([bsz, 1, 1, 50])[-0,-0,-0,-0,....,-10000,-10000,....]\n",
    "        # attention_scores+ attention_mask:torch.Size([bsz, 8, 50, 50])\n",
    "        \n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs_0 = nn.Softmax(dim=-1)(attention_scores) # attSention_probs:torch.Size([bsz, 8, 50, 50])\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs_drop = self.dropout(attention_probs_0)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs_drop, value_layer) #context_layer:torch.Size([bsz, 8, 50, 16])\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous() #context_layer:torch.Size([bsz, 50, 8, 16])\n",
    "        # context_layer.size()[:-2] torch.Size([bsz, 50])\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) #new_context_layer_shape:torch.Size([bsz, 50, 128]) #(128,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape) #context_layer:torch.Size([bsz, 50, 128])\n",
    "        return context_layer, attention_probs_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfOutput(nn.Module): # apply linear and skip conneaction and LayerNorm and dropout after self-attention\n",
    "    def __init__(self, hidden_size, dropout_prob):\n",
    "        super(SelfOutput, self).__init__()# (128,0.1)\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n",
    "        super(Attention, self).__init__()\n",
    "        self.selfAttention = SelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        self.output = SelfOutput(hidden_size, hidden_dropout_prob)# apply linear and skip conneaction and LayerNorm and dropout after self-attention\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        # input_tensor:emb.shape:torch.Size([64, 50, 128]); attention_mask: ex_e_mask:torch.Size([64, 1, 1, 50])\n",
    "        self_output, attention_probs_0 = self.selfAttention(input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output, attention_probs_0     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intermediate(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super(Intermediate, self).__init__()# (128,512)\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.relu(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class Output(nn.Module):# do linear, skip connection, LayerNorm, dropout after intermediate(Feed Forward block)\n",
    "    def __init__(self, intermediate_size, hidden_size, hidden_dropout_prob):\n",
    "        super(Output, self).__init__()# (512,128,0.1)\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states # transformer 最後的輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  # Transformer Encoder for drug feature # Drug_SelfAttention\n",
    "    def __init__(self, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):#(128,512,8,0.1,0.1)\n",
    "        super(Encoder, self).__init__() # (128,512,8,0.1,0.1)\n",
    "        self.attention = Attention(hidden_size, num_attention_heads,\n",
    "                                   attention_probs_dropout_prob, hidden_dropout_prob)# (128,8,0.1,0.1)\n",
    "        self.intermediate = Intermediate(hidden_size, intermediate_size)# (128,512)\n",
    "        self.output = Output(intermediate_size, hidden_size, hidden_dropout_prob)# (512,128,0.1)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        # hidden_states:emb.shape:torch.Size([64, 50, 128]); attention_mask: ex_e_mask:torch.Size([64, 1, 1, 50])\n",
    "        attention_output,attention_probs_0 = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output , attention_probs_0    # transformer 最後的輸出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Omics_DCSA_Model(nn.Module):\n",
    "    def __init__(self,omics_encode_dim_dict,drug_encode_dims, activation_func,activation_func_final,dense_layer_dim, device, ESPF, Drug_SelfAttention, pos_emb_type,\n",
    "                 hidden_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeaetures_dict, max_drug_len, TCGA_pretrain_weight_path_dict=None):\n",
    "        super(Omics_DCSA_Model, self).__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        def load_TCGA_pretrain_weight(model, pretrained_weights_path, device):\n",
    "            state_dict = torch.load(pretrained_weights_path, map_location=device)  # Load the state_dict\n",
    "            encoder_state_dict = {key[len(\"encoder.\"):]: value for key, value in state_dict.items() if key.startswith('encoder')}  # Extract encoder weights\n",
    "            model.load_state_dict(encoder_state_dict)  # Load only the encoder part\n",
    "            model_keys = set(model.state_dict().keys())  # Check if the keys match\n",
    "            loaded_keys = set(encoder_state_dict.keys())\n",
    "            if model_keys == loaded_keys:\n",
    "                print(f\"State_dict for {model} loaded successfully.\")\n",
    "            else:\n",
    "                print(f\"State_dict does not match the model's architecture for {model}.\")\n",
    "                print(\"Model keys: \", model_keys, \" Loaded keys: \", loaded_keys)\n",
    "        \n",
    "        self.MLP4omics_dict = nn.ModuleDict()\n",
    "        for omic_type in omics_numfeaetures_dict.keys():\n",
    "            self.MLP4omics_dict[omic_type] = nn.Sequential(\n",
    "                nn.Linear(omics_numfeaetures_dict[omic_type], omics_encode_dim_dict[omic_type][0]),\n",
    "                activation_func,\n",
    "                nn.Linear(omics_encode_dim_dict[omic_type][0], omics_encode_dim_dict[omic_type][1]),\n",
    "                activation_func_final,\n",
    "                nn.Linear(omics_encode_dim_dict[omic_type][1], omics_encode_dim_dict[omic_type][2])\n",
    "            )\n",
    "            # Initialize with TCGA pretrain weight\n",
    "            if TCGA_pretrain_weight_path_dict is not None:\n",
    "                load_TCGA_pretrain_weight(self.MLP4omics_dict[omic_type], TCGA_pretrain_weight_path_dict[omic_type], device)\n",
    "            else: # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "                self._init_weights(self.MLP4omics_dict[omic_type])\n",
    "\n",
    "            #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "            self.match_drug_dim = nn.Linear(omics_encode_dim_dict[omic_type][2], hidden_size)\n",
    "            self._init_weights(self.match_drug_dim)\n",
    "        \n",
    "#ESPF            \n",
    "        self.emb_f = Embeddings(hidden_size,max_drug_len,hidden_dropout_prob, pos_emb_type,substructure_size = 2586)#(128,50,0.1,2586)\n",
    "        \n",
    "        if Drug_SelfAttention is False: \n",
    "            self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "        # self.output = SelfOutput(hidden_size, hidden_dropout_prob) # (128,0.1) # apply linear and skip conneaction and LayerNorm and dropout after attention\n",
    "# if attention is True  \n",
    "        elif Drug_SelfAttention is True: \n",
    "            self.TransformerEncoder = Encoder(hidden_size, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob)#(128,512,8,0.1,0.1)\n",
    "\n",
    "# Drug_Cell_SelfAttention\n",
    "        self.Drug_Cell_SelfAttention = Encoder(hidden_size+num_attention_heads, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob)#(128+8,512,8,0.1,0.1)\n",
    "\n",
    "# Define the final prediction network \n",
    "        dense_layer_dim=7064\n",
    "        self.model_final_add = nn.Sequential(\n",
    "            nn.Linear(7064, 700),\n",
    "            activation_func,\n",
    "            nn.Dropout(p=0),\n",
    "            nn.Linear(700, 70),\n",
    "            activation_func,\n",
    "            nn.Dropout(p=0),\n",
    "            nn.Linear(70, 1),\n",
    "            activation_func_final)\n",
    "        # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "        self._init_weights(self.model_final_add)\n",
    "\n",
    "        self.print_flag = True\n",
    "        self.attention_probs = None # store Attention score matrix\n",
    "    \n",
    "    def _init_weights(self, model):\n",
    "        if isinstance(model, nn.Linear):  # 直接初始化 nn.Linear 層\n",
    "            init.kaiming_uniform_(model.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "            if model.bias is not None:\n",
    "                init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.ModuleList) or isinstance(model, nn.Sequential):  # 遍歷子層\n",
    "            for layer in model:\n",
    "                self._init_weights(layer)\n",
    "\n",
    "    def forward(self, omics_tensor_dict,drug, device, **kwargs):\n",
    "        Drug_SelfAttention = kwargs['Drug_SelfAttention']\n",
    "        omic_embeddings_ls = []\n",
    "        # Loop through each omic type and pass through its respective model\n",
    "        for omic_type, omic_tensor in omics_tensor_dict.items():\n",
    "            omic_embed = self.MLP4omics_dict[omic_type](omic_tensor.to(device=device)) #(bsz, 50)\n",
    "            #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "            omic_embed = self.match_drug_dim(omic_embed) #(bsz, 128)\n",
    "            omic_embeddings_ls.append(omic_embed)\n",
    "        # omic_embeddings = torch.cat(omic_embeddings_ls, dim=1)  # change list to tensor, because omic_embeddings need to be tensor to torch.cat([omic_embeddings, drug_emb_masked], dim=1) \n",
    "\n",
    "    #ESPF encoding        \n",
    "        mask = drug[:, 1, :].to(device=device) # torch.Size([bsz, 50]),dytpe(long)\n",
    "        drug_embed = drug[:, 0, :].to(device=device) # drug_embed :torch.Size([bsz, 50]),dytpe(long)\n",
    "        drug_embed = self.emb_f(drug_embed) # (bsz, 50, 128) #word embedding、position encoding、LayerNorm、dropout\n",
    "        # Embeddings take int inputs, so no need to convert to float like nn.Linear layer\n",
    "        \n",
    "# mask for Drug Cell SelfAttention\n",
    "        omics_items = torch.ones(mask.size(0), len(omic_embeddings_ls), dtype=mask.dtype, device=mask.device)  # Shape: [bsz, len(omic_embeddings_ls)]\n",
    "        DrugCell_mask = torch.cat([mask, omics_items], dim=1)  # Shape: [bsz, 50 + len(omic_embeddings_ls)]\n",
    "\n",
    "        if Drug_SelfAttention is False:\n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n Drug_SelfAttention is not applied \\n\")\n",
    "                self.print_flag  = False\n",
    "            # to apply mask to emb, treat mask like attention score matrix (weight), then do softmax and dropout, then multiply with emb\n",
    "            mask_weight =mask.clone().float().unsqueeze(1).repeat(1, 50, 1)# (bsz, 50)->(bsz,50,50)\n",
    "            mask_weight = (1.0 - mask_weight) * -10000.0\n",
    "            mask_weight = nn.Softmax(dim=-1)(mask_weight)\n",
    "            mask_weight = self.dropout(mask_weight)\n",
    "            drug_emb_masked = torch.matmul(mask_weight, drug_embed) # emb_masked: torch.Size([bsz, 50, 128])\n",
    "            # 沒做: class SelfOutput(nn.Module): # apply linear and skip conneaction and LayerNorm and dropout after \n",
    "            # 沒做: positional encoding\n",
    "            \n",
    "        elif Drug_SelfAttention is True:\n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n Drug_SelfAttention is applied \\n\")\n",
    "                self.print_flag  = False\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2) # mask.shape: torch.Size([bsz, 1, 1, 50])\n",
    "            mask = (1.0 - mask) * -10000.0\n",
    "            drug_emb_masked, AttenScorMat_DrugSelf  = self.TransformerEncoder(drug_embed, mask)# hidden_states:drug_embed.shape:torch.Size([bsz, 50, 128]); mask: ex_e_mask:torch.Size([bsz, 1, 1, 50])\n",
    "            # drug_emb_masked: torch.Size([bsz, 50, 128]) \n",
    "            # attention_probs_0 = nn.Softmax(dim=-1)(attention_scores) # attention_probs_0:torch.Size([bsz, 8, 50, 50])(without dropout)\n",
    "        elif Drug_SelfAttention is None:\n",
    "                print(\"\\n Drug_SelfAttention is assign to None , please assign to False or True \\n\")\n",
    "\n",
    "# Drug_Cell_SelfAttention\n",
    "        # omic_embeddings_ls:[(bsz,128),(bsz,128)] \n",
    "        # # drug_emb_masked:[bsz,50,128] #已經做完word embedding和position encoding\n",
    "\n",
    "        omic_embeddings = torch.stack(omic_embeddings_ls, dim=1) #shape:[bsz,c,128] #Stack omic_embeddings_ls along the second dimension, c: number of omic types\n",
    "\n",
    "        \n",
    "        append_embeddings = torch.cat([drug_emb_masked, omic_embeddings], dim=1) #shape:[bsz,50+c,128] #Concatenate along the second dimension\n",
    "# Type encoding (to distinguish between drug and omics)\n",
    "        drug_type_encoding = torch.ones_like(drug_emb_masked[..., :1])  # Shape: [bsz, 50, 1]\n",
    "        omics_type_encoding = torch.zeros_like(omic_embeddings[..., :1])  # Shape: [bsz, i, 1]\n",
    "        # Concatenate type encoding with the respective data\n",
    "        drug_emb_masked = torch.cat([drug_emb_masked, drug_type_encoding], dim=-1)  # Shape: [bsz, 50, 129]\n",
    "        omic_embeddings = torch.cat([omic_embeddings, omics_type_encoding], dim=-1)  # Shape: [bsz, c, 129]\n",
    "\n",
    "        # Final concatenated tensor (drug sequence and omics data with type encoding)\n",
    "        append_embeddings = torch.cat([drug_emb_masked, omic_embeddings], dim=1)  # Shape: [bsz, 50+c, 129]\n",
    "\n",
    "        padding_dim = self.num_attention_heads - 1  # Extra dimensions to add # padding_dim=7\n",
    "\n",
    "        pad = torch.zeros(append_embeddings.size(0), append_embeddings.size(1), padding_dim, device=append_embeddings.device)\n",
    "        append_embeddings = torch.cat([append_embeddings, pad], dim=-1)  # New shape: [bsz, 50+i, new_hidden_size]\n",
    "        \n",
    "        DrugCell_mask = DrugCell_mask.unsqueeze(1).unsqueeze(2) \n",
    "        DrugCell_mask = (1.0 - DrugCell_mask) * -10000.0 # mask.shape: torch.Size([bsz, 1, 1, 50+c])\n",
    "        append_embeddings, AttenScorMat_DrugCellSelf  = self.Drug_Cell_SelfAttention(append_embeddings, DrugCell_mask)\n",
    "        # append_embeddings: torch.Size([bsz, 50+c, 136]) # AttenScorMat_DrugCellSelf:torch.Size([bsz, 8, 50+c, 50+c])(without dropout)\n",
    "\n",
    "        #skip connect the omics embeddings # not as necessary as skip connect the drug embeddings \n",
    "        append_embeddings = torch.cat([ torch.cat(omic_embeddings_ls, dim=1), append_embeddings.reshape(append_embeddings.size(0), -1)], dim=1) # dim=1: turn into 1D \n",
    "        #omic_embeddings_ls(bsz, 128) , append_embeddings(bsz, 50+c, 136)\n",
    "        # drug 有50*128，omices有i*128，可能會差太多，看drug要不要先降維根omics一樣i*128 # 先不要\n",
    "    \n",
    "    # Final MLP\n",
    "        output = self.model_final_add(append_embeddings)\n",
    "\n",
    "        return output, AttenScorMat_DrugSelf, AttenScorMat_DrugCellSelf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n",
      "Exp tensor shape: torch.Size([76, 4692])\n",
      "Exp num_features 4692\n",
      "batch_size 3 num_epoch: 2\n",
      "drug_df (42, 9)\n",
      "AUC_df (76, 42)\n",
      "kfoldCV 2\n",
      "num_ccl,num_drug:  76 42\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "# import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader, Subset\n",
    "import torch.nn.init as init\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import importlib.util\n",
    "\n",
    "from ESPF_drug2emb import drug2emb_encoder\n",
    "from Model import Omics_DrugESPF_Model, Omics_DCSA_Model\n",
    "from split_data_id import split_id,repeat_func\n",
    "from create_dataloader import OmicsDrugDataset\n",
    "from train import train, evaluation\n",
    "from correlation import correlation_func\n",
    "from plot import loss_curve, correlation_density,Density_Plot_of_AUC_Values\n",
    "from tools import get_data_value_range,set_seed,get_vram_usage\n",
    "from Metrics import MetricsCalculator\n",
    "\n",
    "\n",
    "from Loss import Custom_LossFunction,Custom_Weighted_LossFunction\n",
    "from Custom_Activation_Function import ScaledSigmoid\n",
    "\n",
    "test = True #False, True: batch_size = 3, num_epoch = 2, full dataset\n",
    "\n",
    "omics_files = {\n",
    "    'Mut': \"../../../data/CCLE/CCLE_match_TCGAgene_PRISMandEXPsample_binary_mutation_476_6009.txt\",\n",
    "    'Exp': \"../../../data/CCLE/CCLE_exp_476samples_4692genes.txt\",\n",
    "    # Add more omics types and paths as needed\n",
    "    }\n",
    "omics_dict = {'Mut':0,'Exp':1,'CN':2, 'Eff':3, 'Dep':4, 'Met':5}\n",
    "omics_data_dict = {}\n",
    "omics_data_tensor_dict = {}\n",
    "omics_numfeatures_dict = {}\n",
    "omics_encode_dim_dict ={'Mut':[1000,100,50],'Exp':[1000,100,50], # Dr.Chiu:exp[500,200,50]\n",
    "                        'CN':[100,50,30], 'Eff':[100,50,30], 'Dep':[100,50,30], 'Met':[100,50,30]}\n",
    "\n",
    "TCGA_pretrain_weight_path_dict = {'Mut': \"../../results/Encoder_tcga_mut_1000_100_50_best_loss_0.0066.pt\",\n",
    "                                  'Exp': \"../../results/Encoder_tcga_exp_1000_100_50_best_loss_0.7.pt\",\n",
    "                                  # Add more omics types and paths as needed\n",
    "                                }\n",
    "seed = 42\n",
    "#hyperparameter\n",
    "model_name = \"Omics_DCSA_Model\" # Omics_DCSA_Model\n",
    "AUCtransform = \"-log2\" #\"-log2\"\n",
    "splitType= 'byDrug' # byCCL byDrug\n",
    "kfoldCV = 5\n",
    "include_omics = ['Exp']\n",
    "max_drug_len=50 # 不夠補零補到50 / 超過取前50個subwords(index) !!!!須改方法!!!!\n",
    "drug_embedding_feature_size = 128\n",
    "ESPF = True # False True\n",
    "Drug_SelfAttention = True\n",
    "pos_emb_type = 'sinusoidal' # 'learned' 'sinusoidal'\n",
    "#需再修改-----------\n",
    "\n",
    "intermediate_size =512\n",
    "num_attention_heads = 8        \n",
    "attention_probs_dropout_prob = 0.1\n",
    "hidden_dropout_prob = 0.1\n",
    "\n",
    "if ESPF is True:\n",
    "    \n",
    "    drug_encode_dims =[1600,400,100] # 50*128\n",
    "    dense_layer_dim = sum(omics_encode_dim_dict[omic_type][2] for omic_type in include_omics) + drug_encode_dims[2] # MLPDim\n",
    "elif ESPF is False:\n",
    "    \n",
    "    drug_encode_dims =[110,55,22]\n",
    "    dense_layer_dim = sum(omics_encode_dim_dict[omic_type][2] for omic_type in include_omics) + drug_encode_dims[2] # MLPDim\n",
    "#需再修改-------------\n",
    "TrackGradient = False # False True\n",
    "\n",
    "activation_func = nn.ReLU()  # ReLU activation function # Leaky ReLu\n",
    "activation_func_final = ScaledSigmoid(scale=8) # GroundT range ( 0 ~ scale )\n",
    "#nn.Sigmoid()or ReLU() or Linear/identity(when -log2AUC)\n",
    "batch_size = 200\n",
    "num_epoch = 200 # for k fold CV \n",
    "patience = 20\n",
    "warmup_iters = 60\n",
    "Decrease_percent = 0.9\n",
    "continuous = True\n",
    "learning_rate=1e-05\n",
    "criterion = Custom_Weighted_LossFunction(loss_type=\"weighted_MSE\", loss_lambda=1.0, regular_type=\"L2\", regular_lambda=1e-05) #nn.MSELoss()#\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "set_seed(seed)\n",
    "for omic_type in include_omics:\n",
    "    # Read the file\n",
    "    omics_data_dict[omic_type] = pd.read_csv(omics_files[omic_type], sep='\\t', index_col=0)\n",
    "\n",
    "    if test is True:\n",
    "        # Specify the index as needed\n",
    "        omics_data_dict[omic_type] = omics_data_dict[omic_type][:76]  # Adjust the row selection as needed\n",
    "    omics_data_tensor_dict[omic_type]  = torch.tensor(omics_data_dict[omic_type].values, dtype=torch.float32)\n",
    "    omics_numfeatures_dict[omic_type] = omics_data_tensor_dict[omic_type].shape[1]\n",
    "    print(f\"{omic_type} tensor shape:\", omics_data_tensor_dict[omic_type].shape)\n",
    "    print(f\"{omic_type} num_features\",omics_numfeatures_dict[omic_type])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "#load data\n",
    "# data_mut, gene_names_mut,ccl_names_mut  = load_ccl(\"/root/data/CCLE/CCLE_match_TCGAgene_PRISMandEXPsample_binary_mutation_476_6009.txt\")\n",
    "drug_df= pd.read_csv(\"../../../data/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/MACCS(Secondary_Screen_treatment_info)_union_NOrepeat.csv\", sep=',', index_col=0)\n",
    "AUC_df = pd.read_csv(\"../../../data/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/Drug_sensitivity_AUC_(PRISM_Repurposing_Secondary_Screen)_subsetted_NOrepeat.csv\", sep=',', index_col=0)\n",
    "# data_AUC_matrix, drug_names_AUC, ccl_names_AUC = load_AUC_matrix(splitType,\"/root/Winnie/no_Imputation_PRISM_Repurposing_Secondary_Screen_data/Drug_sensitivity_AUC_(PRISM_Repurposing_Secondary_Screen)_subsetted.csv\") # splitType = \"byCCL\" or \"byDrug\" 決定AUCmatrix要不要轉置\n",
    "\n",
    "# matched AUCfile and omics_data samples\n",
    "matched_samples = sorted(set(AUC_df.T.columns) & set(list(omics_data_dict.values())[0].T.columns))\n",
    "\n",
    "AUC_df= (AUC_df.T[matched_samples]).T\n",
    "if AUCtransform == \"-log2\":\n",
    "    AUC_df = -np.log2(AUC_df)\n",
    "if AUCtransform == \"-log10\":\n",
    "    AUC_df = -np.log10(AUC_df)\n",
    "    \n",
    "if test is True:\n",
    "    batch_size = 3\n",
    "    num_epoch = 2\n",
    "    print(\"batch_size\",batch_size,\"num_epoch:\",num_epoch)\n",
    "    drug_df=drug_df[:42]\n",
    "    AUC_df=AUC_df.iloc[:76,:42]\n",
    "    print(\"drug_df\",drug_df.shape)\n",
    "    print(\"AUC_df\",AUC_df.shape)\n",
    "    kfoldCV = 2\n",
    "    print(\"kfoldCV\",kfoldCV)\n",
    "\n",
    "if 'weighted' in criterion.loss_type :    \n",
    "    # Set threshold based on the 90th percentile # 將高於threshold的AUC權重增加\n",
    "    weighted_threshold = np.nanpercentile(AUC_df.values, 90)    \n",
    "    total_samples = (~np.isnan(AUC_df.values)).sum().item()\n",
    "    few_samples = (AUC_df.values > weighted_threshold).sum().item()\n",
    "    more_samples = total_samples - few_samples\n",
    "    few_weight = total_samples / (2 * few_samples)  \n",
    "    more_weight = total_samples / (2 * more_samples)   \n",
    "    # print(\"weighted_threshold\",weighted_threshold)\n",
    "    # print(\"total_samples\",total_samples)\n",
    "    # print(\"few_samples\",few_samples)\n",
    "    # print(\"more_samples\",more_samples)\n",
    "    # print(\"few_weight\",few_weight)\n",
    "    # print(\"more_weight\",more_weight)\n",
    "else:\n",
    "    weighted_threshold = None\n",
    "    few_weight = None\n",
    "    more_weight = None\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# 檢查有無重複的SMILES\n",
    "if ESPF is True:\n",
    "    drug_smiles =drug_df[\"smiles\"] # \n",
    "    drug_names =drug_df.index\n",
    "    # 挑出重複的SMILES\n",
    "    duplicate =  drug_smiles[drug_smiles.duplicated(keep=False)]\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------\n",
    "    #ESPF\n",
    "    vocab_path = \"../../ESPF/drug_codes_chembl_freq_1500.txt\" # token\n",
    "    sub_csv = pd.read_csv(\"../../ESPF/subword_units_map_chembl_freq_1500.csv\")# token with frequency\n",
    "\n",
    "    # drug_encode = pd.Series(drug_smiles.unique()).apply(drug2emb_encoder, args=(vocab_path, sub_csv, max_drug_len))# 將drug_smiles 使用_drug2emb_encoder function編碼成subword vector\n",
    "    drug_encode = pd.Series(drug_smiles).apply(drug2emb_encoder, args=(vocab_path, sub_csv, max_drug_len))\n",
    "    # uniq_smile_dict = dict(zip(drug_smiles.unique(),drug_encode))# zip drug_smiles和其subword vector編碼 成字典\n",
    "\n",
    "    # print(type(smile_encode))\n",
    "    # print(smile_encode.shape)\n",
    "    # print(type(smile_encode.index))\n",
    "    # print((drug_encode.index.values).shape)#(42,)\n",
    "    # print((drug_encode).shape)#(42,)\n",
    "    # print(type(drug_encode))#<class 'pandas.core.series.Series'>\n",
    "    #print((drug_encode.values).shape)#(42,)\n",
    "    # print(drug_encode.values.tolist())\n",
    "    # Convert your data to tensors if they're in numpy\n",
    "    drug_features_tensor = torch.tensor(np.array(drug_encode.values.tolist()), dtype=torch.long)\n",
    "else:\n",
    "    drug_encode = drug_df[\"MACCS166bits\"]\n",
    "    drug_encode_list = [list(map(int, item.split(','))) for item in drug_encode.values]\n",
    "    print(\"MACCS166bits_drug_encode_list type: \",type(drug_encode_list))\n",
    "    # Convert your data to tensors if they're in numpy\n",
    "    drug_features_tensor = torch.tensor(np.array(drug_encode_list), dtype=torch.long)\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "num_ccl = list(omics_data_dict.values())[0].shape[0]\n",
    "num_drug = drug_encode.shape[0]\n",
    "print(\"num_ccl,num_drug: \",num_ccl,num_drug)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# k-fold run\n",
    "kfold_losses= {}\n",
    "best_fold_train_epoch_loss_list = []#  for train every epoch loss plot (best_fold)\n",
    "best_fold_val_epoch_loss_list = []#  for validation every epoch loss plot (best_fold)\n",
    "best_test_loss = float('inf')\n",
    "best_fold_best_weight=None\n",
    "set_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State_dict for Sequential(\n",
      "  (0): Linear(in_features=4692, out_features=1000, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (3): ScaledSigmoid(scale=8)\n",
      "  (4): Linear(in_features=100, out_features=50, bias=True)\n",
      ") loaded successfully.\n",
      "Omics_DCSA_model(\n",
      "  (MLP4omics_dict): ModuleDict(\n",
      "    (Exp): Sequential(\n",
      "      (0): Linear(in_features=4692, out_features=1000, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "      (3): ScaledSigmoid(scale=8)\n",
      "      (4): Linear(in_features=100, out_features=50, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (match_drug_dim): Linear(in_features=50, out_features=128, bias=True)\n",
      "  (emb_f): Embeddings(\n",
      "    (word_embeddings): Embedding(2586, 128)\n",
      "    (position_embeddings): SinusoidalPositionalEncoding()\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (TransformerEncoder): Encoder(\n",
      "    (attention): Attention(\n",
      "      (selfAttention): SelfAttention(\n",
      "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): SelfOutput(\n",
      "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (LayerNorm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): Intermediate(\n",
      "      (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "    )\n",
      "    (output): Output(\n",
      "      (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (Drug_Cell_SelfAttention): Encoder(\n",
      "    (attention): Attention(\n",
      "      (selfAttention): SelfAttention(\n",
      "        (query): Linear(in_features=136, out_features=136, bias=True)\n",
      "        (key): Linear(in_features=136, out_features=136, bias=True)\n",
      "        (value): Linear(in_features=136, out_features=136, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): SelfOutput(\n",
      "        (dense): Linear(in_features=136, out_features=136, bias=True)\n",
      "        (LayerNorm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): Intermediate(\n",
      "      (dense): Linear(in_features=136, out_features=512, bias=True)\n",
      "    )\n",
      "    (output): Output(\n",
      "      (dense): Linear(in_features=512, out_features=136, bias=True)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (model_final_add): Sequential(\n",
      "    (0): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0, inplace=False)\n",
      "    (3): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0, inplace=False)\n",
      "    (6): Linear(in_features=150, out_features=1, bias=True)\n",
      "    (7): ScaledSigmoid(scale=8)\n",
      "  )\n",
      ")\n",
      "ModuleDict(\n",
      "  (Exp): Sequential(\n",
      "    (0): Linear(in_features=4692, out_features=1000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (3): ScaledSigmoid(scale=8)\n",
      "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4692, out_features=1000, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (3): ScaledSigmoid(scale=8)\n",
      "  (4): Linear(in_features=100, out_features=50, bias=True)\n",
      ")\n",
      "Linear(in_features=4692, out_features=1000, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=1000, out_features=100, bias=True)\n",
      "ScaledSigmoid(scale=8)\n",
      "Sigmoid()\n",
      "Linear(in_features=100, out_features=50, bias=True)\n",
      "Linear(in_features=50, out_features=128, bias=True)\n",
      "Embeddings(\n",
      "  (word_embeddings): Embedding(2586, 128)\n",
      "  (position_embeddings): SinusoidalPositionalEncoding()\n",
      "  (LayerNorm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Embedding(2586, 128)\n",
      "SinusoidalPositionalEncoding()\n",
      "LayerNorm()\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Encoder(\n",
      "  (attention): Attention(\n",
      "    (selfAttention): SelfAttention(\n",
      "      (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): SelfOutput(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Intermediate(\n",
      "    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "  )\n",
      "  (output): Output(\n",
      "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Attention(\n",
      "  (selfAttention): SelfAttention(\n",
      "    (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): SelfOutput(\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "SelfAttention(\n",
      "  (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "SelfOutput(\n",
      "  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (LayerNorm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "LayerNorm()\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Intermediate(\n",
      "  (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      ")\n",
      "Linear(in_features=128, out_features=512, bias=True)\n",
      "Output(\n",
      "  (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (LayerNorm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=512, out_features=128, bias=True)\n",
      "LayerNorm()\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Encoder(\n",
      "  (attention): Attention(\n",
      "    (selfAttention): SelfAttention(\n",
      "      (query): Linear(in_features=136, out_features=136, bias=True)\n",
      "      (key): Linear(in_features=136, out_features=136, bias=True)\n",
      "      (value): Linear(in_features=136, out_features=136, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): SelfOutput(\n",
      "      (dense): Linear(in_features=136, out_features=136, bias=True)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): Intermediate(\n",
      "    (dense): Linear(in_features=136, out_features=512, bias=True)\n",
      "  )\n",
      "  (output): Output(\n",
      "    (dense): Linear(in_features=512, out_features=136, bias=True)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Attention(\n",
      "  (selfAttention): SelfAttention(\n",
      "    (query): Linear(in_features=136, out_features=136, bias=True)\n",
      "    (key): Linear(in_features=136, out_features=136, bias=True)\n",
      "    (value): Linear(in_features=136, out_features=136, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): SelfOutput(\n",
      "    (dense): Linear(in_features=136, out_features=136, bias=True)\n",
      "    (LayerNorm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "SelfAttention(\n",
      "  (query): Linear(in_features=136, out_features=136, bias=True)\n",
      "  (key): Linear(in_features=136, out_features=136, bias=True)\n",
      "  (value): Linear(in_features=136, out_features=136, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=136, out_features=136, bias=True)\n",
      "Linear(in_features=136, out_features=136, bias=True)\n",
      "Linear(in_features=136, out_features=136, bias=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "SelfOutput(\n",
      "  (dense): Linear(in_features=136, out_features=136, bias=True)\n",
      "  (LayerNorm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=136, out_features=136, bias=True)\n",
      "LayerNorm()\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Intermediate(\n",
      "  (dense): Linear(in_features=136, out_features=512, bias=True)\n",
      ")\n",
      "Linear(in_features=136, out_features=512, bias=True)\n",
      "Output(\n",
      "  (dense): Linear(in_features=512, out_features=136, bias=True)\n",
      "  (LayerNorm): LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=512, out_features=136, bias=True)\n",
      "LayerNorm()\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=150, out_features=150, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0, inplace=False)\n",
      "  (3): Linear(in_features=150, out_features=150, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0, inplace=False)\n",
      "  (6): Linear(in_features=150, out_features=1, bias=True)\n",
      "  (7): ScaledSigmoid(scale=8)\n",
      ")\n",
      "Linear(in_features=150, out_features=150, bias=True)\n",
      "Dropout(p=0, inplace=False)\n",
      "Linear(in_features=150, out_features=150, bias=True)\n",
      "Dropout(p=0, inplace=False)\n",
      "Linear(in_features=150, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Omics_DCSA_Model(omics_encode_dim_dict, drug_encode_dims, activation_func, activation_func_final, dense_layer_dim, device, ESPF, Drug_SelfAttention, pos_emb_type,\n",
    "                            drug_embedding_feature_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeatures_dict, max_drug_len,\n",
    "                            TCGA_pretrain_weight_path_dict= TCGA_pretrain_weight_path_dict)\n",
    "\n",
    "for module in model.modules():\n",
    "    print(module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
