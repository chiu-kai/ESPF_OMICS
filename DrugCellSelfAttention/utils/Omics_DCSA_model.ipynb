{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 128])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_len=50):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    pe = torch.zeros(max_len, d_model) # torch.Size([50, 128])\n",
    "    position = torch.arange(0, max_len).float().unsqueeze(1) # torch.Size([50, 1]) # 0~50\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                         (-torch.log(torch.Tensor([10000])) / d_model)) # [max_len / 2]\n",
    "                        #生成一個數位 [0， 2， 4， ...， d_model-2] 的張量（偶數索引）。\n",
    "    pe[:, 0::2] = torch.sin(position * div_term) #將 sine 函數應用於位置編碼張量 （pe） 的偶數維數。\n",
    "    pe[:, 1::2] = torch.cos(position * div_term) #將餘弦函數應用於位置編碼張量 （pe） 的奇數維數。\n",
    "    pe = pe.unsqueeze(0) # torch.Size([1, 50, 128])\n",
    "    # register pe to buffer and require no grads#緩衝區的參數在訓練期間不會更新\n",
    "    self.register_buffer('pe', pe)\n",
    "  def forward(self, x):\n",
    "    # x: [batch, seq_len, d_model]\n",
    "    # we can add positional encoding to x directly, and ignore other dimension\n",
    "    return x + self.pe[:,:x.size(1)]# x.size(1)= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, variance_epsilon=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = variance_epsilon\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta\n",
    "\n",
    "class Embeddings(nn.Module): # word embedding + positional encoding\n",
    "    \"\"\"Construct the embeddings from protein/target, position embeddings.\"\"\"\n",
    "    def __init__(self, hidden_size,max_drug_len,hidden_dropout_prob,substructure_size):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(substructure_size, hidden_size)#(2586,128)# 50個onehot categorical id(0~2585)用128維來表示類別資訊\n",
    "        #Learned Positional Embedding\n",
    "        #self.position_embeddings = nn.Embedding(max_drug_len, hidden_size)#(50, 128)# 50個pos id(0~50)用128維vector來表示位置資訊\n",
    "        #Sinusoidal Position Encoding\n",
    "        self.position_embeddings = PositionalEncoding(hidden_size, max_len=max_drug_len)#(128,50)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)#128\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)#0.1\n",
    "    def forward(self, input_ids):# torch.Size([bsz, 50]) # 50個子結構id \n",
    "        seq_length = input_ids.size(1) #input_ids:(batchsize=64,50)# seq_length:50 # 50個onehot categorical id(0~2585)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) #position_ids:torch.Size([50]) (0~50)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)#position_ids:torch.Size([64, 50])\n",
    "        words_embeddings = self.word_embeddings(input_ids) #input_ids:(batchsize=64,50)# generate(50,128)類別特徵\n",
    "        \n",
    "        #Learned Positional Embedding\n",
    "        # position_embeddings = self.position_embeddings(position_ids)# generate(50,128)位置特徵\n",
    "        # embeddings = words_embeddings + position_embeddings # embeddings:torch.Size([bsz, 50, 128])\n",
    "        \n",
    "        #Sinusoidal Position Encoding\n",
    "        pos_encoder = PositionalEncoding(d_model=128, max_len=50)\n",
    "        embeddings = pos_encoder(words_embeddings)  # Shape: [bsz, 50, 128]\n",
    "        # words_embeddings: torch.Size([bsz, 50, 128])50個sub,其對應的representation \n",
    "        # position_embeddings: torch.Size([bsz, 50, 128])\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)#LayerNorm embeddings torch.Size([bsz, 50, 128])\n",
    "        embeddings = self.dropout(embeddings)#dropout embeddings torch.Size([bsz, 50, 128])\n",
    "        return embeddings # emb.shape:torch.Size([bsz, 50, 128])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Type_Encoding(nn.Module):\n",
    "    def __init__(self, drug_cell_dim ,max_drug_len=50, num_types = 2, trans=False):\n",
    "        self.embedding_dim = drug_cell_dim\n",
    "        self.type_embedding = nn.Embedding(num_types, self.embedding_dim) #(2,128)\n",
    "        \n",
    "    def forward(self, omic_embeddings, drug_emb_masked):\n",
    "        type_ids = torch.arange(2, dtype=torch.long, device=drug_emb_masked.device)\n",
    "        type_ids = type_ids.unsqueeze(0).expand_as(omic_embeddings.size(0), omic_embeddings.size(1))\n",
    "        type_embeddings = self.type_embedding(type_ids)\n",
    "\n",
    "        \n",
    "        append_embeddings = torch.cat([drug_emb_masked, omic_embeddings], dim=1) #shape:[bsz,52,128] #Concatenate along the second dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob):\n",
    "        super(SelfAttention, self).__init__()# (128,8,0.1)\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads #8 頭數\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)#128/8=16 頭的維度\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size#8*16=128 頭的維度總和等於feature數\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "\n",
    "        self.dropout = nn.Dropout(attention_probs_dropout_prob)#0.1\n",
    "\n",
    "    def transpose_for_scores(self, x): # x: torch.Size([bsz, 50, 128]) # diveide the whole 128 features into 8 heads, result 16 features per head\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) # (8,16)\n",
    "        # x.size()[:-1] torch.Size([bsz, 50]) # new_x_shape: torch.Size([bsz, 50, 8, 16])\n",
    "        x = x.view(*new_x_shape) # changes the shape of x to the new_x_shape # x torch.Size([bsz, 50, 8, 16])\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask): \n",
    "        # hidden_states:emb.shape:torch.Size([bsz, 50, 128]); attention_mask: ex_e_mask:torch.Size([bsz, 1, 1, 50])\n",
    "        mixed_query_layer = self.query(hidden_states) #hidden_states: torch.Size([bsz, 50, 128])\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states) # mixed_value_layer: torch.Size([bsz, 50, 128])\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer) #value_layer:torch.Size([bsz, 8, 50, 16])\n",
    "        \n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))# key_layer.transpose(-1, -2):torch.Size([bsz, 8, 16, 50])\n",
    "        # attention_scores:torch.Size([bsz, 8, 50, 50])\n",
    "        # Scaled Dot-Product: Prevent the dot products from growing too large, causing gradient Vanishing.\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size) # /16\n",
    "        # attention_scores:torch.Size([bsz, 8, 50, 50])\n",
    "        attention_scores = attention_scores + attention_mask #torch.Size([bsz, 1, 1, 50])[-0,-0,-0,-0,....,-10000,-10000,....]\n",
    "        # attention_scores+ attention_mask:torch.Size([bsz, 8, 50, 50])\n",
    "        \n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs_0 = nn.Softmax(dim=-1)(attention_scores) # attSention_probs:torch.Size([bsz, 8, 50, 50])\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs_drop = self.dropout(attention_probs_0)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs_drop, value_layer) #context_layer:torch.Size([bsz, 8, 50, 16])\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous() #context_layer:torch.Size([bsz, 50, 8, 16])\n",
    "        # context_layer.size()[:-2] torch.Size([bsz, 50])\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) #new_context_layer_shape:torch.Size([bsz, 50, 128]) #(128,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape) #context_layer:torch.Size([bsz, 50, 128])\n",
    "        return context_layer, attention_probs_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfOutput(nn.Module): # apply linear and skip conneaction and LayerNorm and dropout after self-attention\n",
    "    def __init__(self, hidden_size, dropout_prob):\n",
    "        super(SelfOutput, self).__init__()# (128,0.1)\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n",
    "        super(Attention, self).__init__()\n",
    "        self.selfAttention = SelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        self.output = SelfOutput(hidden_size, hidden_dropout_prob)# apply linear and skip conneaction and LayerNorm and dropout after self-attention\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        # input_tensor:emb.shape:torch.Size([64, 50, 128]); attention_mask: ex_e_mask:torch.Size([64, 1, 1, 50])\n",
    "        self_output, attention_probs_0 = self.selfAttention(input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output, attention_probs_0     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intermediate(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super(Intermediate, self).__init__()# (128,512)\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.relu(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class Output(nn.Module):# do linear, skip connection, LayerNorm, dropout after intermediate(Feed Forward block)\n",
    "    def __init__(self, intermediate_size, hidden_size, hidden_dropout_prob):\n",
    "        super(Output, self).__init__()# (512,128,0.1)\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states # transformer 最後的輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  # Transformer Encoder for drug feature # Drug_SelfAttention\n",
    "    def __init__(self, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):#(128,512,8,0.1,0.1)\n",
    "        super(Encoder, self).__init__() # (128,512,8,0.1,0.1)\n",
    "        self.attention = Attention(hidden_size, num_attention_heads,\n",
    "                                   attention_probs_dropout_prob, hidden_dropout_prob)# (128,8,0.1,0.1)\n",
    "        self.intermediate = Intermediate(hidden_size, intermediate_size)# (128,512)\n",
    "        self.output = Output(intermediate_size, hidden_size, hidden_dropout_prob)# (512,128,0.1)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        # hidden_states:emb.shape:torch.Size([64, 50, 128]); attention_mask: ex_e_mask:torch.Size([64, 1, 1, 50])\n",
    "        attention_output,attention_probs_0 = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output , attention_probs_0    # transformer 最後的輸出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_types, num_heads, num_layers, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.type_encoding = Type_Encoding(input_dim, num_types)\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=num_heads, num_encoder_layers=num_layers, dropout=dropout)\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, substructure, omics):\n",
    "        # Concatenate substructure and omics sequences\n",
    "        concatenated_seq = torch.cat((substructure, omics), dim=1)  # Shape: (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        # Create type indices\n",
    "        substructure_type_indices = torch.zeros(substructure.size(0), substructure.size(1), dtype=torch.long, device=substructure.device)\n",
    "        omics_type_indices = torch.ones(omics.size(0), omics.size(1), dtype=torch.long, device=omics.device)\n",
    "        \n",
    "        type_indices = torch.cat((substructure_type_indices, omics_type_indices), dim=1)  # Shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Apply type encoding\n",
    "        type_encoded_seq = concatenated_seq + self.type_encoding.type_embedding(type_indices)\n",
    "        \n",
    "        # Transpose for transformer input\n",
    "        type_encoded_seq = type_encoded_seq.transpose(0, 1)  # Shape: (seq_len, batch_size, input_dim)\n",
    "        \n",
    "        # Apply transformer\n",
    "        transformer_output = self.transformer(type_encoded_seq)  # Shape: (seq_len, batch_size, input_dim)\n",
    "        \n",
    "        # Transpose back to original shape\n",
    "        transformer_output = transformer_output.transpose(0, 1)  # Shape: (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        return transformer_output\n",
    "\n",
    "# Example usage\n",
    "batch_size = 2\n",
    "substructure_len, omics_len = 5, 7\n",
    "input_dim = 128\n",
    "num_types = 2\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "substructure = torch.randn(batch_size, substructure_len, input_dim)\n",
    "omics = torch.randn(batch_size, omics_len, input_dim)\n",
    "\n",
    "transformer_model = TransformerModel(input_dim=input_dim, num_types=num_types, num_heads=num_heads, num_layers=num_layers, dropout=dropout)\n",
    "output = transformer_model(substructure, omics)\n",
    "\n",
    "print(\"Transformer Output Shape:\", output.shape)  # (batch_size, seq_len, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Omics_DCSA_model(nn.Module):\n",
    "     def __init__(self,omics_encode_dim_dict,drug_encode_dims, activation_func,activation_func_final,dense_layer_dim, device, ESPF, Drug_SelfAttention, pos_emb_type,\n",
    "                 hidden_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeaetures_dict, max_drug_len, TCGA_pretrain_weight_path_dict=None):\n",
    "        super(Omics_DCSA_model, self)._init_()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        def load_TCGA_pretrain_weight(model, pretrained_weights_path, device):\n",
    "            state_dict = torch.load(pretrained_weights_path, map_location=device)  # Load the state_dict\n",
    "            encoder_state_dict = {key[len(\"encoder.\"):]: value for key, value in state_dict.items() if key.startswith('encoder')}  # Extract encoder weights\n",
    "            model.load_state_dict(encoder_state_dict)  # Load only the encoder part\n",
    "            model_keys = set(model.state_dict().keys())  # Check if the keys match\n",
    "            loaded_keys = set(encoder_state_dict.keys())\n",
    "            if model_keys == loaded_keys:\n",
    "                print(f\"State_dict for {model} loaded successfully.\")\n",
    "            else:\n",
    "                print(f\"State_dict does not match the model's architecture for {model}.\")\n",
    "                print(\"Model keys: \", model_keys, \" Loaded keys: \", loaded_keys)\n",
    "        \n",
    "        self.MLP4omics_dict = nn.ModuleDict()\n",
    "        for omic_type in omics_numfeaetures_dict.keys():\n",
    "            self.MLP4omics_dict[omic_type] = nn.Sequential(\n",
    "                nn.Linear(omics_numfeaetures_dict[omic_type], omics_encode_dim_dict[omic_type][0]),\n",
    "                activation_func,\n",
    "                nn.Linear(omics_encode_dim_dict[omic_type][0], omics_encode_dim_dict[omic_type][1]),\n",
    "                activation_func_final,\n",
    "                nn.Linear(omics_encode_dim_dict[omic_type][1], omics_encode_dim_dict[omic_type][2])\n",
    "            )\n",
    "            # Initialize with TCGA pretrain weight\n",
    "            if TCGA_pretrain_weight_path_dict is not None:\n",
    "                load_TCGA_pretrain_weight(self.MLP4omics_dict[omic_type], TCGA_pretrain_weight_path_dict[omic_type], device)\n",
    "            else: # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "                self._init_weights(self.MLP4omics_dict[omic_type])\n",
    "\n",
    "            #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "            self.match_drug_dim = nn.Linear(omics_encode_dim_dict[omic_type][2], hidden_size)\n",
    "            self._init_weights(self.match_drug_dim)\n",
    "        \n",
    "#ESPF            \n",
    "        self.emb_f = Embeddings(hidden_size,max_drug_len,hidden_dropout_prob, pos_emb_type,substructure_size = 2586)#(128,50,0.1,2586)\n",
    "        if Drug_SelfAttention is False: \n",
    "            self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "        # self.output = SelfOutput(hidden_size, hidden_dropout_prob) # (128,0.1) # apply linear and skip conneaction and LayerNorm and dropout after attention\n",
    "        # if attention is True  \n",
    "        elif Drug_SelfAttention is True: \n",
    "            self.TransformerEncoder = Encoder(hidden_size, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob)#(128,512,8,0.1,0.1)\n",
    "\n",
    "# Drug_Cell_SelfAttention\n",
    "        self.Drug_Cell_SelfAttention = Encoder(hidden_size+num_attention_heads, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob)#(128+8,512,8,0.1,0.1)\n",
    "\n",
    "# Define the final prediction network \n",
    "        self.model_final_add = nn.Sequential(\n",
    "            nn.Linear(dense_layer_dim, dense_layer_dim),\n",
    "            activation_func,\n",
    "            nn.Dropout(p=0),\n",
    "            nn.Linear(dense_layer_dim, dense_layer_dim),\n",
    "            activation_func,\n",
    "            nn.Dropout(p=0),\n",
    "            nn.Linear(dense_layer_dim, 1),\n",
    "            activation_func_final)\n",
    "        # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "        self._init_weights(self.model_final_add)\n",
    "\n",
    "        self.print_flag = True\n",
    "        self.attention_probs = None # store Attention score matrix\n",
    "\n",
    "    def forward(self, omics_tensor_dict,drug, device, Drug_SelfAttention):\n",
    "\n",
    "        omic_embeddings_ls = []\n",
    "        # Loop through each omic type and pass through its respective model\n",
    "        for omic_type, omic_tensor in omics_tensor_dict.items():\n",
    "            omic_embed = self.MLP4omics_dict[omic_type](omic_tensor.to(device=device)) #(bsz, 50)\n",
    "            #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "            omic_embed = self.match_drug_dim(omic_embed) #(bsz, 128)\n",
    "            omic_embeddings_ls.append(omic_embed)\n",
    "        # omic_embeddings = torch.cat(omic_embeddings_ls, dim=1)  # change list to tensor, because omic_embeddings need to be tensor to torch.cat([omic_embeddings, drug_emb_masked], dim=1) \n",
    "\n",
    "    #ESPF encoding        \n",
    "        mask = drug[:, 1, :].to(device=device) # torch.Size([bsz, 50]),dytpe(long)\n",
    "        drug_embed = drug[:, 0, :].to(device=device) # drug_embed :torch.Size([bsz, 50]),dytpe(long)\n",
    "        drug_embed = self.emb_f(drug_embed) # (bsz, 50, 128) #word embedding、position encoding、LayerNorm、dropout\n",
    "        # Embeddings take int inputs, so no need to convert to float like nn.Linear layer\n",
    "        \n",
    "# mask for Drug Cell SelfAttention\n",
    "        omics_items = torch.ones(mask.size(0), len(omic_embeddings_ls), dtype=mask.dtype, device=mask.device)  # Shape: [bsz, len(omic_embeddings_ls)]\n",
    "        DrugCell_mask = torch.cat([mask, omics_items], dim=1)  # Shape: [bsz, 50 + len(omic_embeddings_ls)]\n",
    "\n",
    "        if Drug_SelfAttention is False:\n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n Drug_SelfAttention is not applied \\n\")\n",
    "                self.print_flag  = False\n",
    "            # to apply mask to emb, treat mask like attention score matrix (weight), then do softmax and dropout, then multiply with emb\n",
    "            mask_weight =mask.clone().float().unsqueeze(1).repeat(1, 50, 1)# (bsz, 50)->(bsz,50,50)\n",
    "            mask_weight = (1.0 - mask_weight) * -10000.0\n",
    "            mask_weight = nn.Softmax(dim=-1)(mask_weight)\n",
    "            mask_weight = self.dropout(mask_weight)\n",
    "            drug_emb_masked = torch.matmul(mask_weight, drug_embed) # emb_masked: torch.Size([bsz, 50, 128])\n",
    "            # 沒做: class SelfOutput(nn.Module): # apply linear and skip conneaction and LayerNorm and dropout after \n",
    "            # 沒做: positional encoding\n",
    "            \n",
    "        elif Drug_SelfAttention is True:\n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n Drug_SelfAttention is applied \\n\")\n",
    "                self.print_flag  = False\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2) # mask.shape: torch.Size([bsz, 1, 1, 50])\n",
    "            mask = (1.0 - mask) * -10000.0\n",
    "            drug_emb_masked, AttenScorMat_DrugSelf  = self.TransformerEncoder(drug_embed, mask)# hidden_states:drug_embed.shape:torch.Size([bsz, 50, 128]); mask: ex_e_mask:torch.Size([bsz, 1, 1, 50])\n",
    "            # drug_emb_masked: torch.Size([bsz, 50, 128]) \n",
    "            # attention_probs_0 = nn.Softmax(dim=-1)(attention_scores) # attention_probs_0:torch.Size([bsz, 8, 50, 50])(without dropout)\n",
    "        elif Drug_SelfAttention is None:\n",
    "                print(\"\\n Drug_SelfAttention is assign to None , please assign to False or True \\n\")\n",
    "\n",
    "# Drug_Cell_SelfAttention\n",
    "        # omic_embeddings_ls:[(bsz,128),(bsz,128)] \n",
    "        # # drug_emb_masked:[bsz,50,128] #已經做完word embedding和position encoding\n",
    "\n",
    "        omic_embeddings = torch.stack(omic_embeddings_ls, dim=1) #shape:[bsz,c,128] #Stack omic_embeddings_ls along the second dimension\n",
    "        # Type encoding (to distinguish between drug and omics)\n",
    "        drug_type_encoding = torch.ones_like(drug_emb_masked[..., :1])  # Shape: [bsz, 50, 1]\n",
    "        omics_type_encoding = torch.zeros_like(omic_embeddings[..., :1])  # Shape: [bsz, i, 1]\n",
    "        # Concatenate type encoding with the respective data\n",
    "        drug_emb_masked = torch.cat([drug_emb_masked, drug_type_encoding], dim=-1)  # Shape: [bsz, 50, 129]\n",
    "        omic_embeddings = torch.cat([omic_embeddings, omics_type_encoding], dim=-1)  # Shape: [bsz, c, 129]\n",
    "\n",
    "        # Final concatenated tensor (drug sequence and omics data with type encoding)\n",
    "        append_embeddings = torch.cat([drug_emb_masked, omic_embeddings], dim=1)  # Shape: [bsz, 50+c, 129]\n",
    "\n",
    "        padding_dim = self.num_attention_heads - 1  # Extra dimensions to add\n",
    "        print(\"padding_dim\", padding_dim)\n",
    "        pad = torch.zeros(append_embeddings.size(0), append_embeddings.size(1), padding_dim, device=append_embeddings.device)\n",
    "        append_embeddings = torch.cat([append_embeddings, pad], dim=-1)  # New shape: [bsz, 50+i, new_hidden_size]\n",
    "\n",
    "        append_embeddings, AttenScorMat_DrugCellSelf  = self.Drug_Cell_SelfAttention(append_embeddings, DrugCell_mask)\n",
    "        # append_embeddings: torch.Size([bsz, 50+c, 136]) # AttenScorMat_DrugCellSelf:torch.Size([bsz, 8, 50+c, 50+c])(without dropout)\n",
    "\n",
    "        #skip connect the omics embeddings # not as necessary as skip connect the drug embeddings \n",
    "        append_embeddings = torch.cat([ torch.cat(omic_embeddings_ls, dim=1), append_embeddings.reshape(append_embeddings.size(0), -1)], dim=1) # dim=1: turn into 1D \n",
    "        #omic_embeddings_ls(bsz, 128) , append_embeddings(bsz, 50+c, 136)\n",
    "        # drug 有50*128，omices有i*128，可能會差太多，看drug要不要先降維根omics一樣i*128 # 先不要\n",
    "    \n",
    "    # Final MLP\n",
    "        output = self.model_final_add(append_embeddings)\n",
    "        return output, AttenScorMat_DrugSelf, AttenScorMat_DrugCellSelf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "train_max =2\n",
    "train_min = 1.9\n",
    "val_max = 1.2\n",
    "val_min = 1\n",
    "if (train_min <= 1.5 < train_max) and (val_min <= 1.5 < val_max):\n",
    "    print(1.5, 0)\n",
    "else:\n",
    "    print(None, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
