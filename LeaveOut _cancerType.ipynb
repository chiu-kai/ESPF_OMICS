{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_encode_dims None\n",
      "dense_layer_dim [7064, 700, 70, 1]\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# pip install subword-nmt seaborn lifelines openpyxl matplotlib scikit-learn openTSNE\n",
    "# pip install torchmetrics==1.2.0 pandas==2.1.4 numpy==1.26.4\n",
    "# python3 ./main_kfold.py --config utils/config.py\n",
    "import torch.nn as nn\n",
    "from utils.Loss import Custom_LossFunction,Custom_Weighted_LossFunction,FocalLoss\n",
    "from utils.Custom_Activation_Function import ScaledSigmoid, ReLU_clamp\n",
    "from utils.Metrics import MetricsCalculator_nntorch\n",
    "\n",
    "model_inference = True # False\n",
    "if model_inference is True:\n",
    "    cohort = \"TCGA\"\n",
    "    geneNUM = \"\" # _4692genes tcgadata tcgalabel tcgadata_4692genes tcgalabel_4692genes\n",
    "\n",
    "test = False #False, True: batch_size = 3, num_epoch = 2, full dataset\n",
    "drug_df_path= \"../data/GDSC/TransCDR/214drug SMILES MACCS.csv\"\n",
    "AUC_df_path_numerical = \"../data/GDSC/TransCDR/CDR_bi_n154603_848ccl_214drug(ModelID)SMILES.csv\" # gdsc1+2_ccle_z-score　gdsc1+2_ccle_AUC\n",
    "AUC_df_path = \"../data/GDSC/TransCDRCDR_bi_n154603_848ccl_214drug(ModelID)SMILES.csv\"\n",
    "omics_files = {\n",
    "    'Mut': \"\",\n",
    "    'Exp': \"../data/DAPL/share/ccle_uq1000_feature_sorted.csv\" , # \"../data/CCLE/CCLE_exp_476samples_4692genes.txt\",\n",
    "    # Add more omics types and paths as needed\n",
    "    }\n",
    "omics_dict = {'Mut':0,'Exp':1,'CN':2, 'Eff':3, 'Dep':4, 'Met':5}\n",
    "omics_data_dict = {}\n",
    "omics_data_tensor_dict = {}\n",
    "omics_numfeatures_dict = {}\n",
    "omics_encode_dim_dict ={'Mut':[128,32],'Exp':[128,32],  # Dr.Chiu:exp[500,200,50]  [1000,100,50] 'Mut':[128,32],'Exp':[128,32], 'Mut':[1000,100,50],'Exp':[1000,100,50],\n",
    "                        'CN':[100,50,30], 'Eff':[100,50,30], 'Dep':[100,50,30], 'Met':[100,50,30]}\n",
    "\n",
    "TCGA_pretrain_weight_path_dict = {'Mut': \"./results/Encoder_tcga_mut_1000_100_50_best_loss_0.0066.pt\",\n",
    "                                  'Exp': \"./results/Encoder_tcga_exp_128_32_best_loss_0.2182988.pt\", # \"./results/Encoder_tcga_exp_128_32_best_loss_0.2182988.pt\", \"./results/Encoder_tcga_exp_1000_100_50_best_loss_0.7.pt\"\n",
    "                                  # Add more omics types and paths as needed\n",
    "                                }\n",
    "seed = 42\n",
    "#hyperparameter\n",
    "model_name = \"Omics_DCSA_Model\" # Omics_DrugESPF_Model  Omics_DCSA_Model\n",
    "AUCtransform = None #\"-log2\"\n",
    "# splitType= 'byCCL' # byCCL byDrug \n",
    "splitType= 'CancerType' # ModelID or drug_name or CancerType\n",
    "response = \"lnIC50\"\n",
    "#------------------graph-------------\n",
    "drug_graph = False # False True\n",
    "drug_graph_pool = \"add\"\n",
    "DCSA = False # False True # Drug_Cell_SelfAttention\n",
    "drug_pretrain_weight_path = '../data/DAPL/share/pretrain/drug_encoder.pth' \n",
    "#------------------------------------\n",
    "kfoldCV = 5\n",
    "include_omics = ['Exp']\n",
    "DA_Folder = \"None\" # None VAE_w10SC VAEwC_1\n",
    "if DA_Folder != 'None':\n",
    "    omics_files['Exp'] = f\"../data/DAPL/share/pretrain/{DA_Folder}/ccle_latent_results.pkl\" #\n",
    "max_drug_len=50 # 不夠補零補到50 / 超過取前50個subwords(index) !!!!須改方法!!!! \n",
    "drug_embedding_feature_size = 128\n",
    "ESPF = True # False True\n",
    "Drug_SelfAttention = True\n",
    "n_layer = 1 # transformer layer number\n",
    "pos_emb_type = 'sinusoidal' # 'learned' 'sinusoidal'\n",
    "#需再修改-----------\n",
    "\n",
    "intermediate_size =256 # graph:64; ESPF: 256 \n",
    "num_attention_heads = 8        \n",
    "attention_probs_dropout_prob = 0.1\n",
    "hidden_dropout_prob = 0.1\n",
    "classifier_drop = 0\n",
    "\n",
    "if ESPF is True:\n",
    "    drug_dim = max_drug_len* drug_embedding_feature_size #50*128\n",
    "    drug_encode_dims =[drug_dim//4,drug_dim//16,drug_dim//64] #  \n",
    "    dense_layer_dim = sum(omics_encode_dim_dict[omic_type][len(omics_encode_dim_dict[omic_type])-1] for omic_type in include_omics) + drug_encode_dims[-1] # MLPDim\n",
    "elif ESPF is False:\n",
    "    drug_encode_dims =[110,55,22] #MACCS166\n",
    "    dense_layer_dim = sum(omics_encode_dim_dict[omic_type][len(omics_encode_dim_dict[omic_type])-1] for omic_type in include_omics) + drug_encode_dims[-1] # MLPDim\n",
    "if model_name == \"Omics_DCSA_Model\":\n",
    "    drug_encode_dims = None\n",
    "    conc_dim = (max_drug_len+len(include_omics))*(drug_embedding_feature_size+num_attention_heads)+ (len(include_omics)*drug_embedding_feature_size)\n",
    "    dense_layer_dim =[7064, 700, 70, 1]#[conc_dim, conc_dim//10, conc_dim//100, 1] #7064or3736\n",
    "if model_name == \"GIN_DCSA_model\":\n",
    "    if DCSA is True:\n",
    "        drug_encode_dims = None\n",
    "        conc_dim = (1+len(include_omics))*(drug_embedding_feature_size+num_attention_heads)+ (len(include_omics)*drug_embedding_feature_size)\n",
    "        dense_layer_dim = [conc_dim, conc_dim//2, 1] # [conc_dim, conc_dim//2, conc_dim//2 , 1] 40*2+32=112,56,28,1\n",
    "    elif DCSA is False:\n",
    "        drug_encode_dims = None\n",
    "        conc_dim = (1+len(include_omics))*drug_embedding_feature_size # 32*2=64\n",
    "        dense_layer_dim = [conc_dim, conc_dim//2, 1] #[conc_dim, conc_dim, conc_dim, 1] 64\n",
    "print(\"drug_encode_dims\",drug_encode_dims)\n",
    "print(\"dense_layer_dim\",dense_layer_dim)\n",
    "#需再修改-------------\n",
    "TrackGradient = False # False True\n",
    "\n",
    "activation_func = nn.ReLU()  # ReLU activation function # Leaky ReLu\n",
    "activation_func_final = nn.Sigmoid() # ScaledSigmoid(scale=8) GroundT range ( 0 ~ scale ) # ReLU_clamp(max=8)\n",
    "#nn.Sigmoid()or ReLU() or Linear/identity(when -log2AUC)\n",
    "batch_size = 200\n",
    "num_epoch = 200 # for k fold CV \n",
    "patience = 20\n",
    "learning_rate=1e-05\n",
    "\n",
    "warmup_lr = True # False True\n",
    "decrese_epoch = 60\n",
    "Decrease_percent = 1\n",
    "continuous = True\n",
    "\n",
    "CosineAnnealing_LR = False # False True\n",
    "T_max = 3 # CosinesAnnealingLR step size\n",
    "eta_min = 1e-06 # CosinesAnnealingLR minimum learning rate\n",
    "\n",
    "criterion = Custom_Weighted_LossFunction(loss_type=\"weighted_BCE\", loss_lambda=1.0, regular_type=None, regular_lambda=1e-06) #nn.MSELoss()#\n",
    "# criterion =  FocalLoss(loss_type=\"MSE\", alpha=8.0, gamma=1.0, regular_type=None, regular_lambda=1e-05) # loss_type=\"MSE\"/\"MAE\"\n",
    "# criterion = FocalHuberLoss(loss_type=\"FocalHuberLoss\",delta=0.2, alpha=0.3, gamma=2.0, regular_type=None, regular_lambda=1e-05)\n",
    "if 'BCE' in criterion.loss_type : \n",
    "    metrics_type_set = [\"Accuracy\",\"AUROC\", \"AUPRC\", \"Sensitivity\",\"Specificity\", \"Precision\", \"F1\", \"F1_RecSpe\", \"F1_RecSpePre\" ] \n",
    "    metric=\"F1_RecSpe\" # best_prob_threshold_metric\n",
    "    best_prob_threshold=0.5\n",
    "else:\n",
    "    metrics_type_set = [\"MSE\", \"R^2\"] #\"MSE\",\"MAE\"  None\n",
    "    metric=None # best_prob_threshold_metric\n",
    "    best_prob_threshold=None\n",
    "metrics_calculator = MetricsCalculator_nntorch(types = metrics_type_set)\n",
    "\"\"\" A customizable loss function class.\n",
    "    Args:\n",
    "        loss_type (str): The type of loss to use (\"RMSE\", \"MSE\", \"MAE\",\"BCE\",\"MAE+BCE\", \"MAE+MSE\", \"MAE+RMSE\")/(\"weighted_RMSE\", \"weighted_MSE\", \"weighted_MAE\", \"weighted_MAE+MSE\", \"weighted_MAE+RMSE\").\n",
    "        loss_lambda (float): The lambda weight for the additional loss (MSE or RMSE) if applicable. Default is MAE+ 1.0*(MSE or RMSE).\n",
    "        regular_type (str): The type of regularization to use (\"L1\", \"L2\", \"L1+L2\"), or None for no regularization.\n",
    "        regular_lambda (float): The lambda weight for regularization. Default is 1e-05.\n",
    "        \n",
    "        # Binary Cross Entropy Loss # already done sigmoid\"\"\"\n",
    "hyperparameter_print = f'  metric ={metric}\\n best_prob_threshold ={best_prob_threshold}\\n cohort ={cohort}\\n geneNUM={geneNUM}\\n drug_df_path ={drug_df_path}\\n AUC_df_path_numerical ={AUC_df_path_numerical}\\n AUC_df_path ={AUC_df_path}\\n omics_dict ={omics_dict}\\n omics_files ={omics_files}\\n TCGA_pretrain_weight_path_dict ={TCGA_pretrain_weight_path_dict}\\n drug_pretrain_weight_path ={drug_pretrain_weight_path}\\n seed ={seed}\\n  model_name ={model_name}\\n AUCtransform ={AUCtransform}\\n splitType ={splitType}\\n response ={response}\\n drug_graph ={drug_graph}\\n drug_graph_pool ={drug_graph_pool}\\n DCSA ={DCSA}\\n kfoldCV ={kfoldCV}\\n omics_encode_dim ={[(omic_type,omics_encode_dim_dict[omic_type]) for omic_type in include_omics]}\\n DA_Folder ={DA_Folder}\\n max_drug_len ={max_drug_len}\\n drug_embedding_feature_size ={drug_embedding_feature_size}\\n ESPF ={ESPF}\\n Drug_SelfAttention ={Drug_SelfAttention}\\n n_layer ={n_layer}\\n pos_emb_type ={pos_emb_type}\\n intermediate_size ={intermediate_size}\\n num_attention_heads ={num_attention_heads}\\n attention_probs_dropout_prob ={attention_probs_dropout_prob}\\n hidden_dropout_prob ={hidden_dropout_prob}\\n drug_encode_dims ={drug_encode_dims}\\n dense_layer_dim = {dense_layer_dim}\\n activation_func = {activation_func}\\n activation_func_final = {activation_func_final}\\n batch_size = {batch_size}\\n num_epoch = {num_epoch}\\n patience = {patience}\\n decrese_epoch = {decrese_epoch}\\n Decrease_percent = {Decrease_percent}\\n continuous ={continuous}\\n learning_rate = {learning_rate}\\n criterion ={criterion}\\n'\n",
    "\n",
    "\n",
    "\n",
    "__translation_table__ = str.maketrans({\n",
    "    \"*\": \"\",    \"/\": \"\",    \":\": \"-\",    \"%\": \"\",\n",
    "    \"'\": \"\",    \"\\\"\": \"\",    \"[\": \"\",    \"]\": \"\",\n",
    "    \",\": \"\" })\n",
    "\n",
    "if drug_graph is True:\n",
    "    hyperparameter_folder_part = (f\"{model_name}_{splitType}_DCSA{DCSA}\").translate(__translation_table__)\n",
    "else:\n",
    "    hyperparameter_folder_part = (f\"{model_name}_{splitType}_ESPF{ESPF}_DrugSelfAtten{Drug_SelfAttention}\").translate(__translation_table__)\n",
    "\n",
    "if test is True:\n",
    "    print(\"Running in test mode, using small batch size and few epochs for quick testing.\")\n",
    "    batch_size = 5\n",
    "    num_epoch = 2\n",
    "    kfoldCV = 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader, Subset\n",
    "import torch.nn.init as init\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import importlib.util\n",
    "import pickle\n",
    "import torchmetrics\n",
    "from scipy.stats import ttest_ind\n",
    "import time\n",
    "\n",
    "from utils.ESPF_drug2emb import drug2emb_encoder\n",
    "# from utils.Model import Omics_DrugESPF_Model, Omics_DCSA_Model, GIN_DCSA_model\n",
    "from utils.split_data_id import split_id,repeat_func\n",
    "from utils.create_dataloader import OmicsDrugDataset,InstanceResponseDataset\n",
    "# from utils.train import train, evaluation\n",
    "from utils.correlation import correlation_func\n",
    "from utils.plot import loss_curve, correlation_density, Density_Plot_of_AUC_Values, Confusion_Matrix_plot, TCGA_predAUDRC_box_plot_twoClass\n",
    "from utils.tools import get_data_value_range,set_seed,get_vram_usage\n",
    "print(\"*\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115bffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_weight_path = './inference/BF3_test_loss0.0565073_BestValEpo15_ModelOmics_DCSA_Model_byCCL_OmicsMut_Exp_ESPFTrue_DrugSelfAttentionTrue/'\n",
    "best_weight_path = './results/2025-0708-0400_BF0_weighted_BCE_test_loss0.6038948_BestValEpo3_Omics_DCSA_Model_ModelID_ESPFTrue_DrugSelfAttenTrue_Exp1426_nlayer1_DA-None/'\n",
    "best_prob_threshold = 0.4682268500328064\n",
    "BF=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016168f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n",
      "exp_df samples: 593 , AUC_df_numerical samples: 31664\n",
      "len(matched_samples) 588\n",
      "Exp num_features 1426\n",
      "drug_df (214, 2)\n",
      "drug_df (214, 2)\n",
      "AUC_df_numerical (31664, 13)\n",
      "AUC_df_numerical match samples (31664, 13)\n",
      "AUC_df (31664, 13)\n",
      "drug_df (214, 2)\n",
      "drug_encode <class 'pandas.core.series.Series'>\n",
      "num_ccl,num_drug:  588 214\n"
     ]
    }
   ],
   "source": [
    "# information\n",
    "struct_time   = time.localtime()\n",
    "timestamp    = time.strftime(\"%Y-%m%d-%H%M\", struct_time)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "# 檢查exp和AUC的samples是否一致\n",
    "if DA_Folder != 'None':\n",
    "    with open(omics_files['Exp'], 'rb') as f:\n",
    "        latent_dict = pickle.load(f)\n",
    "        exp_df = pd.DataFrame(latent_dict).T\n",
    "else:\n",
    "    exp_df = pd.read_csv(omics_files[\"Exp\"], sep=',', index_col=0)\n",
    "exp_df = exp_df.sort_index(axis=0).sort_index(axis=1)\n",
    "AUC_df_numerical = pd.read_csv(AUC_df_path_numerical, sep=',', index_col=0)\n",
    "AUC_df_numerical = AUC_df_numerical.sort_values(by='drug_name').sort_values(by='ModelID')\n",
    "print(f\"exp_df samples: {len(exp_df.index)} , AUC_df_numerical samples: {len(AUC_df_numerical.index)}\")\n",
    "matched_samples = sorted(set(AUC_df_numerical['ModelID']) & set(exp_df.index))\n",
    "print(\"len(matched_samples)\",len(matched_samples))\n",
    "# 讀取omics資料\n",
    "set_seed(seed)\n",
    "scaler_dict = {}  # To store scalers for each omic_type\n",
    "for omic_type in include_omics:\n",
    "    if DA_Folder != 'None':\n",
    "        omics_data_dict[omic_type] = exp_df.loc[matched_samples]\n",
    "    else:\n",
    "        omics_data_dict[omic_type] = pd.read_csv(omics_files[omic_type], sep=',', index_col=0).loc[matched_samples]\n",
    "        omics_data_dict[omic_type] = omics_data_dict[omic_type].sort_index(axis=0).sort_index(axis=1)\n",
    "        if omic_type == \"Exp\":# apply Column-wise Standardization \n",
    "            scaler = StandardScaler() \n",
    "            omics_data_dict[omic_type] = pd.DataFrame(scaler.fit_transform(omics_data_dict[omic_type]),index=omics_data_dict[omic_type].index,columns=omics_data_dict[omic_type].columns)\n",
    "            scaler_dict[omic_type] = scaler  # save the fitted scaler for latter inference\n",
    "        \n",
    "    # omics_data_tensor_dict[omic_type]  = torch.tensor(omics_data_dict[omic_type].values, dtype=torch.float32).to(device)\n",
    "    omics_numfeatures_dict[omic_type] = omics_data_dict[omic_type].shape[1]\n",
    "    # print(f\"{omic_type} tensor shape:\", omics_data_tensor_dict[omic_type].shape)\n",
    "    print(f\"{omic_type} num_features\",omics_numfeatures_dict[omic_type])\n",
    "drug_df = pd.read_csv( drug_df_path, sep=',', index_col=0)\n",
    "print(\"drug_df\",drug_df.shape)\n",
    "drug_df = drug_df.sort_index(axis=0).sort_index(axis=1)\n",
    "if \"BRD_ID\" in drug_df.columns:\n",
    "    drug_df[\"BRD_ID\"] = drug_df[\"BRD_ID\"].replace({\"BRD-K61250484-001-02-3\": \"BRD-6125\",\n",
    "                                                    \"BRD-K91701654-001-03-1 (CID5354033)\": \"BRD-K91701654-001-03-1\",\n",
    "                                                    \"BRD-K18787491-001-08-6 (CID3006531)\": \"BRD-K18787491-001-08-6\"})\n",
    "print(\"drug_df\",drug_df.shape)\n",
    "print(\"AUC_df_numerical\",AUC_df_numerical.shape)\n",
    "# matched AUCfile and omics_data samples\n",
    "AUC_df_numerical = AUC_df_numerical[AUC_df_numerical['ModelID'].isin(matched_samples)]\n",
    "print(\"AUC_df_numerical match samples\",AUC_df_numerical.shape)\n",
    "# median_value = np.nanmedian(AUC_df_numerical.values)  # Directly calculate median, ignoring NaNs\n",
    "# print(\"median_value\",median_value)    \n",
    "if 'BCE' in criterion.loss_type :\n",
    "    AUC_df = AUC_df_numerical.copy()\n",
    "    print(\"AUC_df\",AUC_df.shape)\n",
    "    if \"BRD_ID\" in drug_df.columns:\n",
    "        drug_df = drug_df[drug_df[\"BRD_ID\"].isin(AUC_df.columns.str.extract(r\"(BRD-[^\\)]+)\", expand=False))]\n",
    "    print(\"drug_df\",drug_df.shape)\n",
    "else:\n",
    "    AUC_df = AUC_df_numerical.copy()\n",
    "del AUC_df_numerical\n",
    "if 'weighted' in criterion.loss_type :    \n",
    "    if 'BCE' in criterion.loss_type :\n",
    "        weighted_threshold = None\n",
    "        total_samples = (~np.isnan(AUC_df[\"Label\"])).sum().item()\n",
    "        fewWt_samples = (AUC_df[\"Label\"] == 0).sum().item()\n",
    "        moreWt_samples = (AUC_df[\"Label\"] == 1).sum().item()\n",
    "        few_weight = total_samples / (2 * fewWt_samples)  \n",
    "        more_weight = total_samples / (2 * moreWt_samples)\n",
    "    else:\n",
    "        # Set threshold based on the 90th percentile # 將高於threshold的AUC權重增加\n",
    "        weighted_threshold = np.nanpercentile(AUC_df[response], 90)    \n",
    "        total_samples = (~np.isnan(AUC_df[response])).sum().item()\n",
    "        fewWt_samples = (AUC_df[response] > weighted_threshold).sum().item()\n",
    "        moreWt_samples = total_samples - fewWt_samples\n",
    "        few_weight = total_samples / (2 * fewWt_samples)  \n",
    "        more_weight = total_samples / (2 * moreWt_samples)  \n",
    "else:\n",
    "    weighted_threshold = None\n",
    "    few_weight = None\n",
    "    more_weight = None\n",
    "# convert SMILES to subword token by ESPF\n",
    "if ESPF is True:\n",
    "    # 挑出重複的SMILES\n",
    "    duplicate =  drug_df[\"SMILES\"][drug_df[\"SMILES\"].duplicated(keep=False)]\n",
    "    vocab_path = \"./ESPF/drug_codes_chembl_freq_1500.txt\" # token\n",
    "    sub_csv = pd.read_csv(\"./ESPF/subword_units_map_chembl_freq_1500.csv\")# token with frequency\n",
    "    drug_df[\"drug_encode\"] = pd.Series(drug_df[\"SMILES\"]).apply(drug2emb_encoder, args=(vocab_path, sub_csv, max_drug_len))\n",
    "    print(\"drug_encode\",type(drug_df[\"drug_encode\"]))\n",
    "    drug_df[\"drug_encode\"] = [i[:2] for i in drug_df[\"drug_encode\"].values]\n",
    "    # drug_features_tensor = torch.tensor(np.array([i[:2] for i in drug_encode.values]), dtype=torch.long).to(device)#drug_features_tensor = torch.tensor(np.array(drug_encode.values.tolist()), dtype=torch.long).to(device)\n",
    "else:\n",
    "    drug_df[\"drug_encode\"]=[list(map(int, item.split(','))) for item in drug_df[\"MACCS166bits\"].values]\n",
    "    # drug_features_tensor = torch.tensor(np.array(drug_encode_list), dtype=torch.long).to(device)\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "num_ccl = list(omics_data_dict.values())[0].shape[0]\n",
    "num_drug = drug_df[\"drug_encode\"].shape[0]\n",
    "print(\"num_ccl,num_drug: \",num_ccl,num_drug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcd995d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cell_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "drug_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label_bi",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Unnamed: 0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "COSMIC_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "drug_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lnIC50",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "smiles",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "assay_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "GDSC_tissue",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cancer_type",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ModelID",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f6de366e-4986-4ce7-86f1-be9cb71eb6c6",
       "rows": [
        [
         "15777",
         "OVCAR-3",
         "AKT inhibitor VIII",
         "S",
         "127790",
         "905933",
         "171",
         "0.28",
         "O=c1[nH]c2ccccc2n1C1CCN(Cc2ccc(-c3[nH]c4cc5ncnc5cc4nc3-c3ccccc3)cc2)CC1",
         "5500994172383112813930_E09",
         "ovary",
         "OV",
         "1",
         "ACH-000001"
        ],
        [
         "23546",
         "OVCAR-3",
         "Ruxolitinib",
         "R",
         "127696",
         "905933",
         "206",
         "4.37",
         "N#CC[C@H](C1CCCC1)n1cc(-c2ncnc3[nH]ccc23)cn1",
         "5500994172383112813930_E09",
         "ovary",
         "OV",
         "0",
         "ACH-000001"
        ],
        [
         "20686",
         "OVCAR-3",
         "UNC0638",
         "R",
         "127738",
         "905933",
         "245",
         "6.67",
         "COc1cc2c(NC3CCN(C(C)C)CC3)nc(C3CCCCC3)nc2cc1OCCCN1CCCC1",
         "5500994172383112813930_E09",
         "ovary",
         "OV",
         "0",
         "ACH-000001"
        ],
        [
         "16765",
         "OVCAR-3",
         "AZD6482",
         "S",
         "127778",
         "905933",
         "1066",
         "0.8",
         "Cc1cc([C@@H](C)Nc2ccccc2C(=O)O)c2nc(N3CCOCC3)cc(=O)n2c1",
         "5500994172383112813930_E09",
         "ovary",
         "OV",
         "1",
         "ACH-000001"
        ],
        [
         "7819",
         "OVCAR-3",
         "AZD6482",
         "S",
         "127792",
         "905933",
         "156",
         "-0.68",
         "Cc1cc([C@@H](C)Nc2ccccc2C(=O)O)c2nc(N3CCOCC3)cc(=O)n2c1",
         "5500994172383112813930_E09",
         "ovary",
         "OV",
         "1",
         "ACH-000001"
        ],
        [
         "13513",
         "OVCAR-3",
         "17-AAG",
         "R",
         "127825",
         "905933",
         "1026",
         "-0.52",
         "C=CCNC1=C2C[C@@H](C)C[C@H](OC)[C@H](O)[C@@H](C)/C=C(\\C)[C@H](OC(N)=O)[C@@H](OC)/C=C\\C=C(/C)C(=O)NC(=CC1=O)C2=O",
         "5500994172383112813930_E09",
         "ovary",
         "OV",
         "0",
         "ACH-000001"
        ],
        [
         "15874",
         "HL-60",
         "ATRA",
         "S",
         "34762",
         "905938",
         "1009",
         "0.38",
         "CC1=C(/C=C/C(C)=C/C=C/C(C)=C/C(=O)O)C(C)(C)CCC1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "18789",
         "HL-60",
         "T0901317",
         "S",
         "34643",
         "905938",
         "333",
         "2.97",
         "O=S(=O)(c1ccccc1)N(CC(F)(F)F)c1ccc(C(O)(C(F)(F)F)C(F)(F)F)cc1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "10951",
         "HL-60",
         "CX-5461",
         "S",
         "34633",
         "905938",
         "300",
         "2.39",
         "Cc1cnc(CNC(=O)c2c(=O)c3ccc(N4CCCN(C)CC4)nc3n3c2sc2ccccc23)cn1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "8434",
         "HL-60",
         "VX-702",
         "S",
         "34752",
         "905938",
         "1028",
         "1.62",
         "NC(=O)c1ccc(N(C(N)=O)c2c(F)cccc2F)nc1-c1ccc(F)cc1F",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "18187",
         "HL-60",
         "VX-680",
         "S",
         "34813",
         "905938",
         "32",
         "-1.34",
         "Cc1cc(Nc2cc(N3CCN(C)CC3)nc(Sc3ccc(NC(=O)C4CC4)cc3)n2)n[nH]1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "13316",
         "HL-60",
         "PF-4708671",
         "R",
         "34675",
         "905938",
         "1129",
         "4.54",
         "CCc1cncnc1N1CCN(Cc2nc3ccc(C(F)(F)F)cc3[nH]2)CC1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "18830",
         "HL-60",
         "BMS-708163",
         "S",
         "34655",
         "905938",
         "1072",
         "3.69",
         "NC(=O)[C@@H](CCC(F)(F)F)N(Cc1ccc(-c2ncon2)cc1F)S(=O)(=O)c1ccc(Cl)cc1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "4314",
         "HL-60",
         "Gemcitabine",
         "R",
         "34714",
         "905938",
         "135",
         "-4.36",
         "Nc1ccn([C@@H]2O[C@H](CO)[C@@H](O)C2(F)F)c(=O)n1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "19216",
         "HL-60",
         "SB590885",
         "S",
         "34768",
         "905938",
         "1061",
         "2.53",
         "CN(C)CCOc1ccc(-c2nc(-c3ccc4c(c3)CC/C4=N\\O)c(-c3ccncc3)[nH]2)cc1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "2932",
         "HL-60",
         "Doxorubicin",
         "R",
         "34699",
         "905938",
         "133",
         "-2.48",
         "COc1cccc2c1C(=O)c1c(O)c3c(c(O)c1C2=O)C[C@@](O)(C(=O)CO)C[C@@H]3O[C@H]1C[C@H](N)[C@H](O)[C@H](C)O1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "20584",
         "HL-60",
         "Shikonin",
         "R",
         "34690",
         "905938",
         "170",
         "0.61",
         "CC(C)=CC[C@@H](O)C1=CC(=O)c2c(O)ccc(O)c2C1=O",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "29088",
         "HL-60",
         "Gefitinib",
         "R",
         "34755",
         "905938",
         "1010",
         "0.94",
         "COc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1OCCCN1CCOCC1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "19193",
         "HL-60",
         "NU-7441",
         "S",
         "34756",
         "905938",
         "1038",
         "0.71",
         "O=c1cc(N2CCOCC2)oc2c(-c3cccc4c3sc3ccccc34)cccc12",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "8044",
         "HL-60",
         "JQ1",
         "S",
         "34668",
         "905938",
         "1218",
         "1.84",
         "Cc1sc2c(c1C)C(c1ccc(Cl)cc1)=N[C@@H](CC(=O)OC(C)(C)C)c1nnc(C)n1-2",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "13512",
         "HL-60",
         "TG101348",
         "S",
         "34609",
         "905938",
         "306",
         "0.42",
         "Cc1cnc(Nc2ccc(OCCN3CCCC3)cc2)nc1Nc1cccc(S(=O)(=O)NC(C)(C)C)c1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "3208",
         "HL-60",
         "JQ1",
         "S",
         "34686",
         "905938",
         "163",
         "-2.19",
         "Cc1sc2c(c1C)C(c1ccc(Cl)cc1)=N[C@@H](CC(=O)OC(C)(C)C)c1nnc(C)n1-2",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "6618",
         "HL-60",
         "BMS-754807",
         "R",
         "34712",
         "905938",
         "184",
         "-0.47",
         "C[C@@]1(C(=O)Nc2ccc(F)nc2)CCCN1c1nc(Nc2cc(C3CC3)[nH]n2)c2cccn2n1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "18697",
         "HL-60",
         "GDC0941",
         "S",
         "34758",
         "905938",
         "1058",
         "-2.35",
         "CS(=O)(=O)N1CCN(Cc2cc3nc(-c4cccc5[nH]ncc45)nc(N4CCOCC4)c3s2)CC1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "2268",
         "HL-60",
         "NVP-BEZ235",
         "R",
         "34757",
         "905938",
         "1057",
         "-2.36",
         "Cn1c(=O)n(-c2ccc(C(C)(C)C#N)cc2)c2c3cc(-c4cnc5ccccc5c4)ccc3ncc21",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "8187",
         "HL-60",
         "CI-1040",
         "S",
         "34764",
         "905938",
         "1015",
         "-1.79",
         "O=C(NOCC1CC1)c1ccc(F)c(F)c1Nc1ccc(I)cc1Cl",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "2306",
         "HL-60",
         "PD-0325901",
         "S",
         "34760",
         "905938",
         "1060",
         "-5.11",
         "O=C(NOC[C@H](O)CO)c1ccc(F)c(F)c1Nc1ccc(I)cc1F",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "1931",
         "HL-60",
         "AICAR",
         "S",
         "34761",
         "905938",
         "1001",
         "5.61",
         "NC(=O)c1ncn([C@@H]2O[C@H](CO)[C@@H](O)[C@H]2O)c1N",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "27097",
         "HL-60",
         "TPCA-1",
         "R",
         "34623",
         "905938",
         "305",
         "1.63",
         "NC(=O)Nc1sc(-c2ccc(F)cc2)cc1C(N)=O",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "17757",
         "HL-60",
         "YM201636",
         "R",
         "34625",
         "905938",
         "310",
         "2.0",
         "Nc1ccc(C(=O)Nc2cccc(-c3nc(N4CCOCC4)c4oc5ncccc5c4n3)c2)cn1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "28398",
         "HL-60",
         "BAY 61-3606",
         "R",
         "34709",
         "905938",
         "178",
         "2.0",
         "COc1ccc(-c2cc3nccn3c(Nc3ncccc3C(N)=O)n2)cc1OC",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "12785",
         "HL-60",
         "RDEA119",
         "S",
         "34763",
         "905938",
         "1014",
         "-2.38",
         "COc1cc(F)c(F)c(Nc2ccc(I)cc2F)c1NS(=O)(=O)C1(C[C@H](O)CO)CC1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "29796",
         "HL-60",
         "NVP-BHG712",
         "S",
         "34621",
         "905938",
         "295",
         "0.3",
         "Cc1ccc(C(=O)Nc2cccc(C(F)(F)F)c2)cc1Nc1nc(-c2cccnc2)nc2c1cnn2C",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "22200",
         "HL-60",
         "Lenalidomide",
         "S",
         "34728",
         "905938",
         "1020",
         "2.67",
         "Nc1cccc2c1CN(C1CCC(=O)NC1=O)C2=O",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "28855",
         "HL-60",
         "SNX-2112",
         "R",
         "34654",
         "905938",
         "328",
         "-2.96",
         "CC1(C)CC(=O)c2c(C(F)(F)F)nn(-c3ccc(C(N)=O)c(NC4CCC(O)CC4)c3)c2C1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "20452",
         "HL-60",
         "LY317615",
         "R",
         "34604",
         "905938",
         "229",
         "3.5",
         "Cn1cc(C2=C(c3cn(C4CCN(Cc5ccccn5)CC4)c4ccccc34)C(=O)NC2=O)c2ccccc21",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "1558",
         "HL-60",
         "GSK690693",
         "R",
         "34627",
         "905938",
         "326",
         "3.55",
         "CCn1c(-c2nonc2N)nc2c(C#CC(C)(C)O)ncc(OC[C@H]3CCCNC3)c21",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "26200",
         "HL-60",
         "Phenformin",
         "S",
         "34653",
         "905938",
         "196",
         "5.21",
         "NC(N)=NC(N)=NCCc1ccccc1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "30259",
         "HL-60",
         "GSK429286A",
         "S",
         "34596",
         "905938",
         "230",
         "3.0",
         "CC1=C(C(=O)Nc2cc3cn[nH]c3cc2F)C(c2ccc(C(F)(F)F)cc2)CC(=O)N1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "18957",
         "HL-60",
         "CGP-082996",
         "R",
         "34799",
         "905938",
         "54",
         "3.47",
         "CCNc1cc(NC2CCC(O)CC2)nc(Nc2ccc3c(ccn3Cc3ccccc3)c2)n1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "20536",
         "HL-60",
         "CEP-701",
         "R",
         "34740",
         "905938",
         "1024",
         "-1.89",
         "C[C@]12O[C@H](C[C@]1(O)CO)n1c3ccccc3c3c4c(c5c6ccccc6n2c5c31)CNC4=O",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "16749",
         "HL-60",
         "BMS-345541",
         "R",
         "34595",
         "905938",
         "203",
         "2.65",
         "Cc1ccc2nc(NCCN)c3ncc(C)n3c2c1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "25796",
         "HL-60",
         "AZ628",
         "S",
         "34783",
         "905938",
         "29",
         "-1.8",
         "Cc1ccc(NC(=O)c2cccc(C(C)(C)C#N)c2)cc1Nc1ccc2ncn(C)c(=O)c2c1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "9345",
         "HL-60",
         "AZD7762",
         "R",
         "34739",
         "905938",
         "1022",
         "-1.23",
         "NC(=O)Nc1cc(-c2cccc(F)c2)sc1C(=O)N[C@H]1CCCNC1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "16606",
         "HL-60",
         "SL 0101-1",
         "R",
         "34767",
         "905938",
         "1039",
         "4.05",
         "CC(=O)O[C@H]1[C@H](C)O[C@@H](Oc2c(-c3ccc(O)cc3)oc3cc(O)cc(O)c3c2=O)[C@H](O)[C@@H]1OC(C)=O",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "16937",
         "HL-60",
         "PIK-93",
         "S",
         "34622",
         "905938",
         "303",
         "0.69",
         "CC(=O)Nc1nc(C)c(-c2ccc(Cl)c(S(=O)(=O)NCCO)c2)s1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "30678",
         "HL-60",
         "BMS-509744",
         "R",
         "34791",
         "905938",
         "63",
         "2.52",
         "COc1cc(C)c(Sc2cnc(NC(=O)c3ccc(CNC(C)C(C)(C)C)cc3)s2)cc1C(=O)N1CCN(C(C)=O)CC1",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "16980",
         "HL-60",
         "CCT018159",
         "R",
         "34683",
         "905938",
         "1170",
         "2.89",
         "CCc1cc(-c2n[nH]c(C)c2-c2ccc3c(c2)OCCO3)c(O)cc1O",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "0",
         "ACH-000002"
        ],
        [
         "20172",
         "HL-60",
         "GSK2126458",
         "S",
         "34607",
         "905938",
         "283",
         "-5.46",
         "COc1ncc(-c2ccc3nccc(-c4ccnnc4)c3c2)cc1NS(=O)(=O)c1ccc(F)cc1F",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ],
        [
         "8604",
         "HL-60",
         "Vorinostat",
         "S",
         "34726",
         "905938",
         "1012",
         "-1.08",
         "O=C(CCCCCCC(=O)Nc1ccccc1)NO",
         "5500994172383112813929_A08",
         "acute_myeloid_leukaemia",
         "LAML",
         "1",
         "ACH-000002"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 31664
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_type</th>\n",
       "      <th>drug_name</th>\n",
       "      <th>Label_bi</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>COSMIC_ID</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>lnIC50</th>\n",
       "      <th>smiles</th>\n",
       "      <th>assay_name</th>\n",
       "      <th>GDSC_tissue</th>\n",
       "      <th>cancer_type</th>\n",
       "      <th>Label</th>\n",
       "      <th>ModelID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15777</th>\n",
       "      <td>OVCAR-3</td>\n",
       "      <td>AKT inhibitor VIII</td>\n",
       "      <td>S</td>\n",
       "      <td>127790</td>\n",
       "      <td>905933</td>\n",
       "      <td>171</td>\n",
       "      <td>0.28</td>\n",
       "      <td>O=c1[nH]c2ccccc2n1C1CCN(Cc2ccc(-c3[nH]c4cc5ncn...</td>\n",
       "      <td>5500994172383112813930_E09</td>\n",
       "      <td>ovary</td>\n",
       "      <td>OV</td>\n",
       "      <td>1</td>\n",
       "      <td>ACH-000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23546</th>\n",
       "      <td>OVCAR-3</td>\n",
       "      <td>Ruxolitinib</td>\n",
       "      <td>R</td>\n",
       "      <td>127696</td>\n",
       "      <td>905933</td>\n",
       "      <td>206</td>\n",
       "      <td>4.37</td>\n",
       "      <td>N#CC[C@H](C1CCCC1)n1cc(-c2ncnc3[nH]ccc23)cn1</td>\n",
       "      <td>5500994172383112813930_E09</td>\n",
       "      <td>ovary</td>\n",
       "      <td>OV</td>\n",
       "      <td>0</td>\n",
       "      <td>ACH-000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20686</th>\n",
       "      <td>OVCAR-3</td>\n",
       "      <td>UNC0638</td>\n",
       "      <td>R</td>\n",
       "      <td>127738</td>\n",
       "      <td>905933</td>\n",
       "      <td>245</td>\n",
       "      <td>6.67</td>\n",
       "      <td>COc1cc2c(NC3CCN(C(C)C)CC3)nc(C3CCCCC3)nc2cc1OC...</td>\n",
       "      <td>5500994172383112813930_E09</td>\n",
       "      <td>ovary</td>\n",
       "      <td>OV</td>\n",
       "      <td>0</td>\n",
       "      <td>ACH-000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16765</th>\n",
       "      <td>OVCAR-3</td>\n",
       "      <td>AZD6482</td>\n",
       "      <td>S</td>\n",
       "      <td>127778</td>\n",
       "      <td>905933</td>\n",
       "      <td>1066</td>\n",
       "      <td>0.80</td>\n",
       "      <td>Cc1cc([C@@H](C)Nc2ccccc2C(=O)O)c2nc(N3CCOCC3)c...</td>\n",
       "      <td>5500994172383112813930_E09</td>\n",
       "      <td>ovary</td>\n",
       "      <td>OV</td>\n",
       "      <td>1</td>\n",
       "      <td>ACH-000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7819</th>\n",
       "      <td>OVCAR-3</td>\n",
       "      <td>AZD6482</td>\n",
       "      <td>S</td>\n",
       "      <td>127792</td>\n",
       "      <td>905933</td>\n",
       "      <td>156</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>Cc1cc([C@@H](C)Nc2ccccc2C(=O)O)c2nc(N3CCOCC3)c...</td>\n",
       "      <td>5500994172383112813930_E09</td>\n",
       "      <td>ovary</td>\n",
       "      <td>OV</td>\n",
       "      <td>1</td>\n",
       "      <td>ACH-000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13278</th>\n",
       "      <td>CAL-72</td>\n",
       "      <td>CHIR-99021</td>\n",
       "      <td>S</td>\n",
       "      <td>98294</td>\n",
       "      <td>906827</td>\n",
       "      <td>1241</td>\n",
       "      <td>2.85</td>\n",
       "      <td>Cc1cnc(-c2cnc(NCCNc3ccc(C#N)cn3)nc2-c2ccc(Cl)c...</td>\n",
       "      <td>5500994158987071513209_H07</td>\n",
       "      <td>osteosarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ACH-001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19308</th>\n",
       "      <td>CAL-72</td>\n",
       "      <td>Docetaxel</td>\n",
       "      <td>S</td>\n",
       "      <td>98356</td>\n",
       "      <td>906827</td>\n",
       "      <td>1007</td>\n",
       "      <td>-8.92</td>\n",
       "      <td>CC(=O)O[C@@]12CO[C@@H]1C[C@H](O)[C@@]1(C)C(=O)...</td>\n",
       "      <td>5500994158987071513209_H07</td>\n",
       "      <td>osteosarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ACH-001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22074</th>\n",
       "      <td>CAL-72</td>\n",
       "      <td>Temsirolimus</td>\n",
       "      <td>S</td>\n",
       "      <td>98391</td>\n",
       "      <td>906827</td>\n",
       "      <td>1016</td>\n",
       "      <td>-4.19</td>\n",
       "      <td>CO[C@H]1C[C@@H]2CC[C@@H](C)[C@@](O)(O2)C(=O)C(...</td>\n",
       "      <td>5500994158987071513209_H07</td>\n",
       "      <td>osteosarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ACH-001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29339</th>\n",
       "      <td>CAL-72</td>\n",
       "      <td>MP470</td>\n",
       "      <td>S</td>\n",
       "      <td>98267</td>\n",
       "      <td>906827</td>\n",
       "      <td>293</td>\n",
       "      <td>0.91</td>\n",
       "      <td>S=C(NCc1ccc2c(c1)OCO2)N1CCN(c2ncnc3c2oc2ccccc2...</td>\n",
       "      <td>5500994158987071513209_H07</td>\n",
       "      <td>osteosarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ACH-001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18266</th>\n",
       "      <td>CAL-72</td>\n",
       "      <td>Camptothecin</td>\n",
       "      <td>S</td>\n",
       "      <td>98361</td>\n",
       "      <td>906827</td>\n",
       "      <td>1003</td>\n",
       "      <td>-6.80</td>\n",
       "      <td>CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2cc3ccccc3nc2-1</td>\n",
       "      <td>5500994158987071513209_H07</td>\n",
       "      <td>osteosarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ACH-001715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31664 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cell_type           drug_name Label_bi  Unnamed: 0  COSMIC_ID  drug_id  \\\n",
       "15777   OVCAR-3  AKT inhibitor VIII        S      127790     905933      171   \n",
       "23546   OVCAR-3         Ruxolitinib        R      127696     905933      206   \n",
       "20686   OVCAR-3             UNC0638        R      127738     905933      245   \n",
       "16765   OVCAR-3             AZD6482        S      127778     905933     1066   \n",
       "7819    OVCAR-3             AZD6482        S      127792     905933      156   \n",
       "...         ...                 ...      ...         ...        ...      ...   \n",
       "13278    CAL-72          CHIR-99021        S       98294     906827     1241   \n",
       "19308    CAL-72           Docetaxel        S       98356     906827     1007   \n",
       "22074    CAL-72        Temsirolimus        S       98391     906827     1016   \n",
       "29339    CAL-72               MP470        S       98267     906827      293   \n",
       "18266    CAL-72        Camptothecin        S       98361     906827     1003   \n",
       "\n",
       "       lnIC50                                             smiles  \\\n",
       "15777    0.28  O=c1[nH]c2ccccc2n1C1CCN(Cc2ccc(-c3[nH]c4cc5ncn...   \n",
       "23546    4.37       N#CC[C@H](C1CCCC1)n1cc(-c2ncnc3[nH]ccc23)cn1   \n",
       "20686    6.67  COc1cc2c(NC3CCN(C(C)C)CC3)nc(C3CCCCC3)nc2cc1OC...   \n",
       "16765    0.80  Cc1cc([C@@H](C)Nc2ccccc2C(=O)O)c2nc(N3CCOCC3)c...   \n",
       "7819    -0.68  Cc1cc([C@@H](C)Nc2ccccc2C(=O)O)c2nc(N3CCOCC3)c...   \n",
       "...       ...                                                ...   \n",
       "13278    2.85  Cc1cnc(-c2cnc(NCCNc3ccc(C#N)cn3)nc2-c2ccc(Cl)c...   \n",
       "19308   -8.92  CC(=O)O[C@@]12CO[C@@H]1C[C@H](O)[C@@]1(C)C(=O)...   \n",
       "22074   -4.19  CO[C@H]1C[C@@H]2CC[C@@H](C)[C@@](O)(O2)C(=O)C(...   \n",
       "29339    0.91  S=C(NCc1ccc2c(c1)OCO2)N1CCN(c2ncnc3c2oc2ccccc2...   \n",
       "18266   -6.80  CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2cc3ccccc3nc2-1   \n",
       "\n",
       "                       assay_name   GDSC_tissue cancer_type  Label     ModelID  \n",
       "15777  5500994172383112813930_E09         ovary          OV      1  ACH-000001  \n",
       "23546  5500994172383112813930_E09         ovary          OV      0  ACH-000001  \n",
       "20686  5500994172383112813930_E09         ovary          OV      0  ACH-000001  \n",
       "16765  5500994172383112813930_E09         ovary          OV      1  ACH-000001  \n",
       "7819   5500994172383112813930_E09         ovary          OV      1  ACH-000001  \n",
       "...                           ...           ...         ...    ...         ...  \n",
       "13278  5500994158987071513209_H07  osteosarcoma         NaN      1  ACH-001715  \n",
       "19308  5500994158987071513209_H07  osteosarcoma         NaN      1  ACH-001715  \n",
       "22074  5500994158987071513209_H07  osteosarcoma         NaN      1  ACH-001715  \n",
       "29339  5500994158987071513209_H07  osteosarcoma         NaN      1  ACH-001715  \n",
       "18266  5500994158987071513209_H07  osteosarcoma         NaN      1  ACH-001715  \n",
       "\n",
       "[31664 rows x 13 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancerType_mapping = {'ovary': 'Ovarian Cancer',\n",
    "                        'acute_myeloid_leukaemia': 'na',\n",
    "                        'large_intestine': 'Colon/Colorectal Cancer',\n",
    "                        'melanoma': 'Skin Cancer',\n",
    "                        'lung_NSCLC_adenocarcinoma': 'Lung Cancer',\n",
    "                        'lung_NSCLC_large cell': 'Lung Cancer',\n",
    "                        'Bladder': 'Bladder Cancer',\n",
    "                        'breast': 'Breast Cancer',\n",
    "                        'B_cell_leukemia': 'na',\n",
    "                        'pancreas': 'Pancreatic Cancer',\n",
    "                        'myeloma': 'na',\n",
    "                        'glioma': 'Brain Cancer',\n",
    "                        'leukemia': 'na',\n",
    "                        'kidney': 'Kidney Cancer',\n",
    "                        'rhabdomyosarcoma': 'Sarcoma',\n",
    "                        'anaplastic_large_cell_lymphoma': 'na',\n",
    "                        'fibrosarcoma': 'Sarcoma',\n",
    "                        'medulloblastoma': 'Brain Cancer',\n",
    "                        'B_cell_lymphoma': 'na',\n",
    "                        'thyroid': 'Thyroid Cancer',\n",
    "                        'lymphoblastic_leukemia': 'na',\n",
    "                        'T_cell_leukemia': 'na',\n",
    "                        'chronic_myeloid_leukaemia': 'na',\n",
    "                        'neuroblastoma': 'na',\n",
    "                        'ewings_sarcoma': 'Sarcoma',\n",
    "                        'prostate': 'Prostate Cancer',\n",
    "                        'mesothelioma': 'na',\n",
    "                        'lymphoblastic_T_cell_leukaemia': 'na',\n",
    "                        'lymphoid_neoplasm other': 'na',\n",
    "                        'lung_small_cell_carcinoma': 'Lung Cancer',\n",
    "                        'stomach': 'Gastric Cancer',\n",
    "                        'Burkitt_lymphoma': 'na',\n",
    "                        'lung_NSCLC_squamous_cell_carcinoma': 'Lung Cancer',\n",
    "                        'head and neck': 'Head and Neck Cancer',\n",
    "                        'endometrium': 'Endometrial/Uterine Cancer',\n",
    "                        'haematopoietic_neoplasm other': 'na',\n",
    "                        'liver': 'Liver Cancer',\n",
    "                        'Hodgkin_lymphoma': 'na',\n",
    "                        'oesophagus': 'Esophageal Cancer',\n",
    "                        'osteosarcoma': 'Sarcoma',\n",
    "                        'soft_tissue_other': 'Sarcoma',\n",
    "                        'lung_NSCLC_not specified': 'Lung Cancer',\n",
    "                        'uterus': 'Endometrial/Uterine Cancer',\n",
    "                        'chondrosarcoma': 'Sarcoma',\n",
    "                        'digestive_system_other': 'Colon/Colorectal Cancer',\n",
    "                        'bone_other': 'Sarcoma',\n",
    "                        'lung_NSCLC_carcinoid': 'Lung Cancer',\n",
    "                        'skin_other': 'Skin Cancer',\n",
    "                        'cervix': 'Cervical Cancer',\n",
    "                        'urogenital_system_other': 'Prostate Cancer',\n",
    "                        'adrenal_gland': 'na'}\n",
    "AUC_df['CancerType'] = AUC_df['GDSC_tissue'].map(cancerType_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1fe405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitType = ModelID or drug_name\n",
    "all_samples = AUC_df['CancerType'].unique()\n",
    "\n",
    "# train_val_samples, test_samples = train_test_split(all_samples, test_size=0.1, random_state=42)\n",
    "# train_val_df = AUC_df[AUC_df[splitType].isin(train_val_samples)]\n",
    "# test_df = AUC_df[AUC_df[splitType].isin(test_samples)]  \n",
    "# #create dataset\n",
    "# set_seed(seed)\n",
    "# def collate_fn(batch):\n",
    "#         gene_feature, drug_list, target = zip(*batch)\n",
    "#         return list(gene_feature), list(drug_list), list(target)\n",
    "# test_dataset = InstanceResponseDataset(test_df, omics_data_dict, drug_df, drug_graph, include_omics, device)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn) #, num_workers=4, pin_memory=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171b41d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import GINConv, global_add_pool, global_mean_pool,global_max_pool\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "# Modules------------------------------------------------------------------------------------------------------------------------------------------------------        \n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "  def __init__(self, hidden_size, max_len=50):\n",
    "    super(SinusoidalPositionalEncoding, self).__init__()\n",
    "    pe = torch.zeros(max_len, hidden_size) # torch.Size([50, 128])\n",
    "    position = torch.arange(0, max_len).float().unsqueeze(1) # torch.Size([50, 1]) # 0~50\n",
    "    div_term = torch.exp(torch.arange(0, hidden_size, 2).float() *\n",
    "                         (-torch.log(torch.Tensor([10000])) / hidden_size)) # [max_len / 2]\n",
    "                        #生成一個數位 [0， 2， 4， ...， hidden_size-2] 的張量（偶數索引）。\n",
    "    pe[:, 0::2] = torch.sin(position * div_term) #將 sine 函數應用於位置編碼張量 （pe） 的偶數維數。\n",
    "    pe[:, 1::2] = torch.cos(position * div_term) #將餘弦函數應用於位置編碼張量 （pe） 的奇數維數。\n",
    "    pe = pe.unsqueeze(0) # torch.Size([1, 50, 128])\n",
    "    # register pe to buffer and require no grads#緩衝區的參數在訓練期間不會更新\n",
    "    self.register_buffer('pe', pe)\n",
    "  def forward(self, x):\n",
    "    # x: [batch, seq_len, hidden_size]\n",
    "    # we can add positional encoding to x directly, and ignore other dimension\n",
    "    return x + self.pe[:,:x.size(1)].to(x.device)# x.size(1)= 50\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size, max_len):#(128, 50)\n",
    "        super().__init__()\n",
    "        self.position_embeddings = nn.Embedding(max_len, hidden_size)#(128, 50) # 50個pos id(0~50)用128維vector來表示位置資訊\n",
    "    def forward(self, x):# x: torch.Size([bsz, 50]) # 50個子結構id\n",
    "            # seq_length = seq.size(1) #seq:(batchsize=64,50)# seq_length:50 # 50個onehot categorical id 𝜖(0~2585)\n",
    "            # position_ids = torch.arange(seq_length, dtype=torch.long, device=seq.device) #position_ids:torch.Size([50]) (0~50)\n",
    "            # position_ids = position_ids.unsqueeze(0).expand_as(seq)#position_ids:torch.Size([bsz, 50])\n",
    "        seq_length = x.size(1) #x:(batchsize=64,50)# 50個onehot categorical id 𝜖(0~2585)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=x.device) #position_ids:torch.Size([50]) (0~50)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(x) #position_ids =>torch.Size([bsz, 50])\n",
    "        return self.position_embeddings(position_ids)# generate torch.Size([bsz, 50, 128])位置特徵，每一個位置都用128為來描述\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, variance_epsilon=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = variance_epsilon\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta\n",
    "class Embeddings(nn.Module): # word embedding + positional encoding\n",
    "    \"\"\"Construct the embeddings from protein/target, position embeddings.\"\"\"\n",
    "    def __init__(self, hidden_size,max_drug_len,hidden_dropout_prob, pos_emb_type ,substructure_size):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(substructure_size, hidden_size)#(2586,128)# 50個onehot categorical id(0~2585)用128維來表示類別資訊\n",
    "        self.pos_emb_type = pos_emb_type\n",
    "        if pos_emb_type == \"learned\":#Learned Positional Embedding\n",
    "            self.position_embeddings = LearnedPositionalEncoding(hidden_size, max_len=max_drug_len)#(128,50)\n",
    "                #self.position_embeddings = nn.Embedding(max_drug_len, hidden_size)#(50, 128)# 50個pos id(0~50)用128維vector來表示位置資訊\n",
    "        elif pos_emb_type == \"sinusoidal\":#Sinusoidal Position Encoding\n",
    "            self.position_embeddings = SinusoidalPositionalEncoding(hidden_size, max_len=max_drug_len)#(128,50)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)#128\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)#0.1\n",
    "    def forward(self, seq):# torch.Size([bsz, 50]) # 50個子結構id \n",
    "        words_embeddings = self.word_embeddings(seq) #seq:(batchsize=64,50)# generate(bze,50,128)類別特徵\n",
    "        # words_embeddings: torch.Size([bsz, 50, 128])50個sub,其對應的representation\n",
    "    #Learned Positional Embedding\n",
    "        if self.pos_emb_type == \"learned\":\n",
    "            position_embeddings = self.position_embeddings(seq)# generate torch.Size([bsz, 50, 128])位置特徵\n",
    "            #position_embeddings: torch.Size([bsz, 50, 128])\n",
    "            embeddings = words_embeddings + position_embeddings # embeddings:torch.Size([bsz, 50, 128])\n",
    "    #Sinusoidal Position Encoding\n",
    "        elif self.pos_emb_type == \"sinusoidal\":\n",
    "            embeddings = self.position_embeddings(words_embeddings)  # Shape: [bsz, 50, 128] \n",
    "        embeddings = self.LayerNorm(embeddings)#LayerNorm embeddings torch.Size([bsz, 50, 128])\n",
    "        embeddings = self.dropout(embeddings)#dropout embeddings torch.Size([bsz, 50, 128])\n",
    "        return embeddings # emb.shape:torch.Size([bsz, 50, 128])\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob):\n",
    "        super(SelfAttention, self).__init__()# (128,8,0.1)\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads #8 頭數\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)#128/8=16 頭的維度\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size#8*16=128 頭的維度總和等於feature數\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)#(128,128)\n",
    "        self.dropout = nn.Dropout(attention_probs_dropout_prob)#0.1\n",
    "    def transpose_for_scores(self, x): # x: torch.Size([bsz, 50, 128]) # diveide the whole 128 features into 8 heads, result 16 features per head\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) # (8,16)\n",
    "        # x.size()[:-1] torch.Size([bsz, 50]) # new_x_shape: torch.Size([bsz, 50, 8, 16])\n",
    "        x = x.view(*new_x_shape) # changes the shape of x to the new_x_shape # x torch.Size([bsz, 50, 8, 16])\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    def forward(self, hidden_states, mask): \n",
    "        # hidden_states:emb.shape:torch.Size([bsz, 50, 128]); attention_mask: ex_e_mask:torch.Size([bsz, 1, 1, 50])\n",
    "        mixed_query_layer = self.query(hidden_states) #hidden_states: torch.Size([bsz, 50, 128])\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states) # mixed_value_layer: torch.Size([bsz, 50, 128])\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer) #value_layer:torch.Size([bsz, 8, 50, 16])\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))# key_layer.transpose(-1, -2):torch.Size([bsz, 8, 16, 50])\n",
    "        # attention_scores:torch.Size([bsz, 8, 50, 50])\n",
    "        # Scaled Dot-Product: Prevent the dot products from growing too large, causing gradient Vanishing.\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size) # /16\n",
    "        attention_mask = torch.bmm(mask.float().unsqueeze(2), mask.float().unsqueeze(1))#(bsz,len)->(bsz,len,1),(bsz,1,len)#after batch-wise outer product(bsz,len,len) #bmm only take float\n",
    "        attention_mask = attention_mask.unsqueeze(1).expand_as(attention_scores) # not neccessory because broadcasting\n",
    "        attention_scores = attention_scores.masked_fill(attention_mask == 0, -1e8) #torch.Size([bsz, head, 50, 50])[i,i,i,i,....,-1e8,-1e8,-1e8,....] # Replaces values where mask == 0 with -1e8 # -1e8 softmax need very small number so it will output 0\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs_0 = F.softmax(attention_scores, dim=-1) # attention_probs_0:torch.Size([bsz, 8, 50, 50])\n",
    "        attention_probs_0 = attention_probs_0.masked_fill(attention_mask == 0, 0)#Replaces values where mask == 0 with 0.0: to prevent the empty element to have value due to softmax \n",
    "        # This is actually dropping out entire tokens to attend to, which might seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs_drop = self.dropout(attention_probs_0)\n",
    "        context_layer = torch.matmul(attention_probs_drop, value_layer) #context_layer:torch.Size([bsz, 8, 50, 16])\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous() #context_layer:torch.Size([bsz, 50, 8, 16])\n",
    "        # context_layer.size()[:-2] torch.Size([bsz, 50])\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) #new_context_layer_shape:torch.Size([bsz, 50, 128]) #(128,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape) #context_layer:torch.Size([bsz, 50, 128])\n",
    "        return context_layer, attention_probs_0\n",
    "class SelfOutput(nn.Module): # apply linear and skip conneaction and LayerNorm and dropout after self-attention\n",
    "    def __init__(self, hidden_size, dropout_prob):\n",
    "        super(SelfOutput, self).__init__()# (128,0.1)\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n",
    "        super(Attention, self).__init__()\n",
    "        self.selfAttention = SelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        self.output = SelfOutput(hidden_size, hidden_dropout_prob)# apply linear and skip conneaction and LayerNorm and dropout after self-attention\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        # input_tensor:emb.shape:torch.Size([64, 50, 128]); attention_mask: ex_e_mask:torch.Size([64, 1, 1, 50])\n",
    "        self_output, attention_probs_0 = self.selfAttention(input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output, attention_probs_0    \n",
    "class Intermediate(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super(Intermediate, self).__init__()# (128,512)\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.relu(hidden_states)\n",
    "        return hidden_states\n",
    "class Output(nn.Module):# do linear, skip connection, LayerNorm, dropout after intermediate(Feed Forward block)\n",
    "    def __init__(self, intermediate_size, hidden_size, hidden_dropout_prob):\n",
    "        super(Output, self).__init__()# (512,128,0.1)\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states # transformer 最後的輸出\n",
    "class Encoder(nn.Module):  # Transformer Encoder for drug feature # Drug_SelfAttention\n",
    "    def __init__(self, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):#(128/136,512,8,0.1,0.1)\n",
    "        super(Encoder, self).__init__() # (128,512,8,0.1,0.1)\n",
    "        self.attention = Attention(hidden_size, num_attention_heads,\n",
    "                                   attention_probs_dropout_prob, hidden_dropout_prob)# (128,8,0.1,0.1)\n",
    "        self.intermediate = Intermediate(hidden_size, intermediate_size)# (128,512)\n",
    "        self.output = Output(intermediate_size, hidden_size, hidden_dropout_prob)# (512,128,0.1)\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        # hidden_states:emb.shape:torch.Size([64, 50, 128or136]); attention_mask: ex_e_mask:torch.Size([64, 1, 1, 50or50+c])\n",
    "        attention_output,attention_probs_0 = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output , attention_probs_0    # transformer 最後的輸出\n",
    "class Encoder_MultipleLayers(nn.Module): # 用Encoder更新representation n_layer次 # DeepTTA paper寫6次\n",
    "    def __init__(self, hidden_size, intermediate_size,\n",
    "                 num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob, n_layer): #(128, 512, 8, 0.1, 0.1, 6)\n",
    "        super(Encoder_MultipleLayers, self).__init__()\n",
    "        layer = Encoder(hidden_size, intermediate_size, num_attention_heads,\n",
    "                        attention_probs_dropout_prob, hidden_dropout_prob) # (128,512,8,0.1,0.1)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layer)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        for layer_module in self.layers:\n",
    "            hidden_states, attention_probs_0 = layer_module(hidden_states, attention_mask) \n",
    "        return hidden_states, attention_probs_0  # transformer 最後的輸出\n",
    "\n",
    "    def output_all_layers(self, hidden_states, attention_mask):\n",
    "        \"\"\" 需要時才計算所有層的輸出，不佔用額外記憶體 \"\"\"\n",
    "        embeddings_all_layers = []\n",
    "        attention_probs_all_layers = []\n",
    "        for layer_module in self.layers:\n",
    "            hidden_states, attention_probs = layer_module(hidden_states, attention_mask)\n",
    "            embeddings_all_layers.append(hidden_states)\n",
    "            attention_probs_all_layers.append(attention_probs)\n",
    "        return embeddings_all_layers, attention_probs_all_layers    \n",
    "# End of Modules------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def create_mlpEncoder(dimList, activation_func,drop=0.1):\n",
    "    layers = []\n",
    "    for i in range(len(dimList) - 1):  \n",
    "        layers.append(nn.Linear(dimList[i], dimList[i + 1]))\n",
    "        if i < len(dimList) - 2:  \n",
    "            layers.append(activation_func) \n",
    "#             layers.append(nn.Dropout(drop))    \n",
    "    return nn.Sequential(*layers)\n",
    "# Models------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "class Omics_DrugESPF_Model(nn.Module):\n",
    "    def __init__(self,omics_encode_dim_dict,drug_encode_dims, activation_func,activation_func_final,dense_layer_dim, device, ESPF, Drug_SelfAttention,\n",
    "                 pos_emb_type, hidden_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeatures_dict, max_drug_len,n_layer,DA_Folder, TCGA_pretrain_weight_path_dict=None):\n",
    "        super(Omics_DrugESPF_Model, self).__init__()\n",
    "        self.n_layer = n_layer\n",
    "\n",
    "        if DA_Folder != 'None':\n",
    "            self.MLP4omics_dict = nn.ModuleDict()\n",
    "            for omic_type in omics_numfeatures_dict.keys():\n",
    "                self.MLP4omics_dict[omic_type] = nn.Sequential(\n",
    "                    nn.Identity()  # just pass through the input, no linear combination no transformation\n",
    "                )\n",
    "        else:    \n",
    "            def load_TCGA_pretrain_weight(model, pretrained_weights_path, device):\n",
    "                state_dict = torch.load(pretrained_weights_path, map_location=device)  # Load the state_dict\n",
    "                encoder_state_dict = {key[len(\"encoder.\"):]: value for key, value in state_dict.items() if key.startswith('encoder')}  # Extract encoder weights\n",
    "                model.load_state_dict(encoder_state_dict)  # Load only the encoder part\n",
    "                model_keys = set(model.state_dict().keys())  # Check if the keys match\n",
    "                loaded_keys = set(encoder_state_dict.keys())\n",
    "                if model_keys == loaded_keys:\n",
    "                    print(f\"State_dict for {model} loaded successfully.\")\n",
    "                else:\n",
    "                    print(f\"State_dict does not match the model's architecture for {model}.\")\n",
    "                    print(\"Model keys: \", model_keys, \" Loaded keys: \", loaded_keys)\n",
    "    # Create subnetworks for each omic type dynamically\n",
    "            self.MLP4omics_dict = nn.ModuleDict()\n",
    "            for omic_type in omics_numfeatures_dict.keys():\n",
    "                self.MLP4omics_dict[omic_type] = create_mlpEncoder([omics_numfeatures_dict[omic_type]] + omics_encode_dim_dict[omic_type], activation_func,drop=0.1\n",
    "                )\n",
    "                # Initialize with TCGA pretrain weight\n",
    "                if TCGA_pretrain_weight_path_dict is not None:\n",
    "                    load_TCGA_pretrain_weight(self.MLP4omics_dict[omic_type], TCGA_pretrain_weight_path_dict[omic_type], device)\n",
    "                else: # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "                    self._init_weights(self.MLP4omics_dict[omic_type])\n",
    "# Define subnetwork for drug ESPF features\n",
    "        if ESPF is True:\n",
    "            self.emb_f = Embeddings(hidden_size,max_drug_len,hidden_dropout_prob, pos_emb_type, substructure_size = 2586)#(128,50,0.1,2586)\n",
    "            self._init_weights(self.emb_f)\n",
    "            # if attention is not True \n",
    "            if Drug_SelfAttention is False: \n",
    "                self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "            # self.output = SelfOutput(hidden_size, hidden_dropout_prob) # (128,0.1) # apply linear and skip conneaction and LayerNorm and dropout after attention\n",
    "            # if attention is True  \n",
    "            elif Drug_SelfAttention is True: \n",
    "                #self.TransformerEncoder = Encoder(hidden_size, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob)#(128,512,8,0.1,0.1)\n",
    "                self.TransformerEncoder = Encoder_MultipleLayers(hidden_size, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob, n_layer=n_layer)#(128,512,8,0.1,0.1,3)\n",
    "                self._init_weights(self.TransformerEncoder)\n",
    "            self.MLP4ESPF = nn.Sequential(\n",
    "                nn.Linear(max_drug_len * hidden_size, drug_encode_dims[0]),\n",
    "                activation_func,\n",
    "                nn.Dropout(hidden_dropout_prob),\n",
    "                nn.BatchNorm1d(drug_encode_dims[0]),\n",
    "                nn.Linear(drug_encode_dims[0], drug_encode_dims[1]),\n",
    "                activation_func,\n",
    "                nn.Dropout(hidden_dropout_prob),\n",
    "                nn.BatchNorm1d(drug_encode_dims[1]),\n",
    "                nn.Linear(drug_encode_dims[1], drug_encode_dims[2]),\n",
    "                activation_func,\n",
    "                nn.BatchNorm1d(drug_encode_dims[2]))\n",
    "            # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "            self._init_weights(self.MLP4ESPF)\n",
    "        else: # MACCS166\n",
    "            self.MLP4MACCS = nn.Sequential( # 166->[110,55,22]\n",
    "                nn.Linear(166, drug_encode_dims[0]),\n",
    "                activation_func,\n",
    "                nn.BatchNorm1d(drug_encode_dims[0]),\n",
    "                # nn.Dropout(hidden_dropout_prob),\n",
    "                nn.Linear(drug_encode_dims[0], drug_encode_dims[1]),\n",
    "                activation_func,\n",
    "                nn.BatchNorm1d(drug_encode_dims[1]),\n",
    "                # nn.Dropout(hidden_dropout_prob),\n",
    "                nn.Linear(drug_encode_dims[1], drug_encode_dims[2]),\n",
    "                activation_func,\n",
    "                nn.BatchNorm1d(drug_encode_dims[2]))\n",
    "            # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "            self._init_weights(self.MLP4MACCS)\n",
    "# Define the final prediction network \n",
    "        self.model_final_add = nn.Sequential(\n",
    "            nn.Linear(dense_layer_dim, dense_layer_dim),\n",
    "            activation_func,\n",
    "            nn.BatchNorm1d(dense_layer_dim),\n",
    "            nn.Dropout(p=0),\n",
    "            nn.Linear(dense_layer_dim, dense_layer_dim), \n",
    "            activation_func,\n",
    "            nn.BatchNorm1d(dense_layer_dim),\n",
    "            nn.Dropout(p=0),\n",
    "            nn.Linear(dense_layer_dim, 1),\n",
    "            activation_func_final)\n",
    "        # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "        self._init_weights(self.model_final_add)\n",
    "        self.print_flag = True\n",
    "        self.attention_probs = None # store Attention score matrix\n",
    "    def _init_weights(self, model):\n",
    "        if isinstance(model, nn.Linear):  # 直接初始化 nn.Linear 層\n",
    "            init.kaiming_uniform_(model.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "            if model.bias is not None:\n",
    "                init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.LayerNorm):\n",
    "            init.ones_(model.weight)\n",
    "            init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.ModuleList) or isinstance(model, nn.Sequential):  # 遍歷子層\n",
    "            for layer in model:\n",
    "                self._init_weights(layer)\n",
    "    def forward(self, omics_tensor_dict,drug, device,**kwargs):\n",
    "        ESPF,Drug_SelfAttention = kwargs['ESPF'],kwargs['Drug_SelfAttention']\n",
    "        omic_embeddings = []\n",
    "        # Loop through each omic type and pass through its respective model\n",
    "        for omic_type, omic_tensor in omics_tensor_dict.items():\n",
    "            omic_embed = self.MLP4omics_dict[omic_type](omic_tensor)#.to(device=device)\n",
    "            omic_embeddings.append(omic_embed)\n",
    "        omic_embeddings = torch.cat(omic_embeddings, dim=1)  # change list to tensor, because omic_embeddings need to be tensor to torch.cat([omic_embeddings, drug_emb_masked], dim=1) \n",
    "        if ESPF is True:\n",
    "            mask = drug[:, 1, :] # torch.Size([bsz, 50]),dytpe(long) .to(device=device)\n",
    "            drug_embed = drug[:, 0, :] # drug_embed :torch.Size([bsz, 50]),dytpe(long) .to(device=device)\n",
    "            drug_embed = self.emb_f(drug_embed) # (bsz, 50, 128) # Embeddings take int inputs, so no need to convert to float like nn.Linear layer\n",
    "            if Drug_SelfAttention is False:\n",
    "                if self.print_flag is True:\n",
    "                    print(\"\\n Drug_SelfAttention is not applied \\n\")\n",
    "                    self.print_flag  = False\n",
    "                # to apply mask to emb, treat mask like attention score matrix (weight), then do softmax and dropout, then multiply with emb\n",
    "                mask_weightBMM = torch.bmm(mask.float().unsqueeze(2), mask.float().unsqueeze(1))# (bsz, 50)->(bsz,50,50)(empty index of seq should not have value) \n",
    "                mask_weight = (1.0 - mask_weightBMM) * -1e8\n",
    "                mask_weight = F.softmax(mask_weightBMM, dim=-1)\n",
    "                mask_weight = mask_weight.masked_fill(mask_weightBMM == 0, 0.0)\n",
    "                mask_weight = self.dropout(mask_weight)\n",
    "                drug_emb_masked = torch.bmm(mask_weight, drug_embed) # emb_masked: torch.Size([bsz, 50, 128]) # matmul矩陣相乘\n",
    "                # 沒做: class SelfOutput(nn.Module): # apply linear and skip conneaction and LayerNorm and dropout after attention\n",
    "            elif Drug_SelfAttention is True:\n",
    "                if self.print_flag is True:\n",
    "                    print(\"\\n Drug_SelfAttention is applied \\n\")\n",
    "                    self.print_flag  = False\n",
    "                drug_emb_masked, attention_probs_0  = self.TransformerEncoder(drug_embed, mask)# hidden_states:drug_embed.shape:torch.Size([bsz, 50, 128]); mask: ex_e_mask:torch.Size([bsz, 1, 1, 50])\n",
    "                self.attention_probs = attention_probs_0\n",
    "            elif Drug_SelfAttention is None:\n",
    "                    print(\"\\n Drug_SelfAttention is assign to None , please assign to False or True \\n\")\n",
    "            drug_emb_masked = drug_emb_masked.reshape(-1,drug_emb_masked.shape[1]*drug_emb_masked.shape[2]) # flatten to (bsz, 50*128)\n",
    "            drug_final_emb = self.MLP4ESPF(drug_emb_masked) # 6400->1600->400->100\n",
    "        else: # MACCS166 \n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n MACCS166 is applied \\n\")\n",
    "                self.print_flag  = False\n",
    "            drug = drug.to(torch.float32) # long -> float, because the input of linear layer should be float,才能和float的weight相乘\n",
    "            drug_final_emb = self.MLP4MACCS(drug)# 166->[110,55,22] # to device, because the weight is on device .to(device=device)\n",
    "        # Concatenate embeddings from all subnetworks\n",
    "        combined_mut_drug_embed = torch.cat([omic_embeddings, drug_final_emb], dim=1)#dim=1: turn into 1D\n",
    "        output_before_final_activation = self.model_final_add[:-1](combined_mut_drug_embed)\n",
    "        output = self.model_final_add[-1](output_before_final_activation)\n",
    "        return output, self.attention_probs,'',output_before_final_activation\n",
    "# Omics_DCSA_Model\n",
    "class Omics_DCSA_Model(nn.Module):\n",
    "    def __init__(self,omics_encode_dim_dict,drug_encode_dims, activation_func,activation_func_final,dense_layer_dim, device, ESPF, Drug_SelfAttention, pos_emb_type,\n",
    "                 hidden_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeatures_dict, max_drug_len,n_layer,DA_Folder, TCGA_pretrain_weight_path_dict=None):\n",
    "        super(Omics_DCSA_Model, self).__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.n_layer = n_layer\n",
    "        if DA_Folder != 'None':\n",
    "            self.MLP4omics_dict = nn.ModuleDict()\n",
    "            for omic_type in omics_numfeatures_dict.keys():\n",
    "                self.MLP4omics_dict[omic_type] = nn.Sequential(\n",
    "                    nn.Identity()  # just pass through the input, no linear combination no transformation\n",
    "                )\n",
    "                #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "                self.match_drug_dim = nn.Linear(omics_encode_dim_dict[omic_type][-1], hidden_size)\n",
    "                self._init_weights(self.match_drug_dim)\n",
    "        else:\n",
    "            def load_TCGA_pretrain_weight(model, pretrained_weights_path, device):\n",
    "                state_dict = torch.load(pretrained_weights_path, map_location=device)  # Load the state_dict\n",
    "                encoder_state_dict = {key[len(\"encoder.\"):]: value for key, value in state_dict.items() if key.startswith('encoder')}  # Extract encoder weights\n",
    "                model.load_state_dict(encoder_state_dict)  # Load only the encoder part\n",
    "                model_keys = set(model.state_dict().keys())  # Check if the keys match\n",
    "                loaded_keys = set(encoder_state_dict.keys())\n",
    "                if model_keys == loaded_keys:\n",
    "                    print(f\"State_dict for {model} loaded successfully.\")\n",
    "                else:\n",
    "                    print(f\"State_dict does not match the model's architecture for {model}.\")\n",
    "                    print(\"Model keys: \", model_keys, \" Loaded keys: \", loaded_keys)\n",
    "            self.MLP4omics_dict = nn.ModuleDict()\n",
    "            # for omic_type in omics_numfeatures_dict.keys():\n",
    "            #     self.MLP4omics_dict[omic_type] = nn.Sequential(\n",
    "            #         nn.Linear(omics_numfeatures_dict[omic_type], omics_encode_dim_dict[omic_type][0]),\n",
    "            #         activation_func,\n",
    "            #         nn.Linear(omics_encode_dim_dict[omic_type][0], omics_encode_dim_dict[omic_type][1]),\n",
    "            #         activation_func,\n",
    "            #         nn.Linear(omics_encode_dim_dict[omic_type][1], omics_encode_dim_dict[omic_type][2]),\n",
    "            #         activation_func\n",
    "            #     )\n",
    "            for omic_type in omics_numfeatures_dict.keys():\n",
    "                self.MLP4omics_dict[omic_type] = create_mlpEncoder([omics_numfeatures_dict[omic_type]] + omics_encode_dim_dict[omic_type], activation_func,drop=0.1\n",
    "                )\n",
    "                # Initialize with TCGA pretrain weight\n",
    "                if TCGA_pretrain_weight_path_dict is not None:\n",
    "                    load_TCGA_pretrain_weight(self.MLP4omics_dict[omic_type], TCGA_pretrain_weight_path_dict[omic_type], device)\n",
    "                else: # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "                    self._init_weights(self.MLP4omics_dict[omic_type])\n",
    "\n",
    "                #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "                self.match_drug_dim = nn.Linear(omics_encode_dim_dict[omic_type][-1], hidden_size)\n",
    "                self._init_weights(self.match_drug_dim)\n",
    "#ESPF            \n",
    "        self.emb_f = Embeddings(hidden_size,max_drug_len,hidden_dropout_prob, pos_emb_type,substructure_size = 2586)#(128,50,0.1,2586)\n",
    "        self._init_weights(self.emb_f)\n",
    "        if Drug_SelfAttention is False: \n",
    "            self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "# if attention is True  \n",
    "        elif Drug_SelfAttention is True: \n",
    "            # self.TransformerEncoder = Encoder(hidden_size, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob)#(128,512,8,0.1,0.1)\n",
    "            self.TransformerEncoder = Encoder_MultipleLayers(hidden_size, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob, n_layer=n_layer)#(128,512,8,0.1,0.1,3)\n",
    "            self._init_weights(self.TransformerEncoder)\n",
    "# Drug_Cell_SelfAttention\n",
    "        self.Drug_Cell_SelfAttention = Encoder_MultipleLayers(hidden_size+num_attention_heads, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob, n_layer=n_layer)\n",
    "        self._init_weights(self.Drug_Cell_SelfAttention)\n",
    "# Define the final prediction network \n",
    "        self.model_final_add = nn.Sequential(\n",
    "            nn.Linear(dense_layer_dim[0], dense_layer_dim[1]),\n",
    "            activation_func,\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.BatchNorm1d(dense_layer_dim[1]), \n",
    "            nn.Linear(dense_layer_dim[1], dense_layer_dim[2]),\n",
    "            activation_func,\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.BatchNorm1d(dense_layer_dim[2]), \n",
    "            nn.Linear(dense_layer_dim[2], dense_layer_dim[3]),\n",
    "            activation_func_final)\n",
    "        # Initialize weights with Kaiming uniform initialization, bias with zero\n",
    "        self._init_weights(self.model_final_add)\n",
    "        self.print_flag = True\n",
    "    def _init_weights(self, model):\n",
    "        if isinstance(model, nn.Linear):  # 直接初始化 nn.Linear 層\n",
    "            init.kaiming_uniform_(model.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "            if model.bias is not None:\n",
    "                init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.LayerNorm):\n",
    "            init.ones_(model.weight)\n",
    "            init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.ModuleList) or isinstance(model, nn.Sequential):  # 遍歷子層\n",
    "            for layer in model:\n",
    "                self._init_weights(layer)\n",
    "    def forward(self, omics_tensor_dict,drug, device, **kwargs):\n",
    "        Drug_SelfAttention = kwargs['Drug_SelfAttention']\n",
    "        omic_embeddings_ls = []\n",
    "        # Loop through each omic type and pass through its respective model\n",
    "        for omic_type, omic_tensor in omics_tensor_dict.items():\n",
    "            omic_embed = self.MLP4omics_dict[omic_type](omic_tensor)# .to(device=device)\n",
    "            #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "            omic_embed = self.match_drug_dim(omic_embed) #(bsz, 128)\n",
    "            omic_embeddings_ls.append(omic_embed)\n",
    "    #ESPF encoding        \n",
    "        mask = drug[:, 1, :]#.to(device=device) # torch.Size([bsz, 50]),dytpe(long)\n",
    "        drug_embed = drug[:, 0, :]#.to(device=device) # drug_embed :torch.Size([bsz, 50]),dytpe(long)\n",
    "        drug_embed = self.emb_f(drug_embed) # (bsz, 50, 128) #word embedding、position encoding、LayerNorm、dropout\n",
    "# mask for Drug Cell SelfAttention\n",
    "        omics_items = torch.ones(mask.size(0), len(omic_embeddings_ls), dtype=mask.dtype, device=mask.device)  # Shape: [bsz, len(omic_embeddings_ls)]\n",
    "        DrugCell_mask = torch.cat([mask, omics_items], dim=1)  # Shape: [bsz, 50 + len(omic_embeddings_ls)]\n",
    "        if Drug_SelfAttention is False:\n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n Drug_SelfAttention is not applied \\n\")\n",
    "                self.print_flag  = False\n",
    "            # to apply mask to emb, treat mask like attention score matrix (weight), then do softmax and dropout, then multiply with emb\n",
    "            mask_weightBMM = torch.bmm(mask.float().unsqueeze(2), mask.float().unsqueeze(1))# (bsz, 50)->(bsz,50,50)(empty index of seq should not have value) \n",
    "            mask_weight = (1.0 - mask_weightBMM) * -1e8 # 1,0經過softmax 0的機率不會是0，所以要把0變成-1e8經過softmax之後才會是0\n",
    "            mask_weight = F.softmax(mask_weightBMM, dim=-1)\n",
    "            mask_weight = mask_weight.masked_fill(mask_weightBMM == 0, 0.0) # 把全部都是0的vector都變成是0，因為softmax會把機率均分 \n",
    "            mask_weight = self.dropout(mask_weight)\n",
    "            drug_emb_masked = torch.bmm(mask_weight, drug_embed) # emb_masked: torch.Size([bsz, 50, 128])# bmm only take 3D tensor # matmul can broadcast\n",
    "            # 沒做: class SelfOutput(nn.Module): # apply linear and skip conneaction and LayerNorm and dropout after \n",
    "            # 沒做: positional encoding\n",
    "            AttenScorMat_DrugSelf = None\n",
    "        elif Drug_SelfAttention is True:\n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n Drug_SelfAttention is applied \\n\")\n",
    "                self.print_flag  = False       \n",
    "            drug_emb_masked, AttenScorMat_DrugSelf  = self.TransformerEncoder(drug_embed, mask)# hidden_states:drug_embed.shape:torch.Size([bsz, 50, 128]); mask: ex_e_mask:torch.Size([bsz, 1, 1, 50])\n",
    "        elif Drug_SelfAttention is None:\n",
    "                print(\"\\n Drug_SelfAttention is assign to None , please assign to False or True \\n\")\n",
    "\n",
    "# Drug_Cell_SelfAttention\n",
    "        omic_embeddings = torch.stack(omic_embeddings_ls, dim=1) #shape:[bsz,c,128] #Stack omic_embeddings_ls along the second dimension, c: number of omic types\n",
    "        append_embeddings = torch.cat([drug_emb_masked, omic_embeddings], dim=1) #shape:[bsz,50+c,128] #Concatenate along the second dimension\n",
    "# Type encoding (to distinguish between drug and omics)\n",
    "        drug_type_encoding = torch.ones_like(drug_emb_masked[..., :1])  # Shape: [bsz, 50, 1]\n",
    "        omics_type_encoding = torch.zeros_like(omic_embeddings[..., :1])  # Shape: [bsz, i, 1]\n",
    "        # Concatenate type encoding with the respective data\n",
    "        drug_emb_masked = torch.cat([drug_emb_masked, drug_type_encoding], dim=-1)  # Shape: [bsz, 50, 129]\n",
    "        omic_embeddings = torch.cat([omic_embeddings, omics_type_encoding], dim=-1)  # Shape: [bsz, c, 129]\n",
    "        # Final concatenated tensor (drug sequence and omics data with type encoding)\n",
    "        append_embeddings = torch.cat([drug_emb_masked, omic_embeddings], dim=1)  # Shape: [bsz, 50+c, 129]\n",
    "        padding_dim = self.num_attention_heads - 1  # Extra dimensions to add # padding_dim=7\n",
    "        pad = torch.zeros(append_embeddings.size(0), append_embeddings.size(1), padding_dim, device=append_embeddings.device)\n",
    "        append_embeddings = torch.cat([append_embeddings, pad], dim=-1)  # New shape: [bsz, 50+c, new_hidden_size]\n",
    "        append_embeddings, AttenScorMat_DrugCellSelf  = self.Drug_Cell_SelfAttention(append_embeddings, DrugCell_mask)# DrugCell_mask.shape: torch.Size([bsz, 50+c])\n",
    "        append_embeddings = torch.cat([ torch.cat(omic_embeddings_ls, dim=1), append_embeddings.reshape(append_embeddings.size(0), -1)], dim=1) # dim=1: turn into 1D \n",
    "    # Final MLP\n",
    "        output_before_final_activation = self.model_final_add[:-1](append_embeddings)\n",
    "        output = self.model_final_add[-1](output_before_final_activation)\n",
    "        return output, AttenScorMat_DrugSelf, AttenScorMat_DrugCellSelf,output_before_final_activation\n",
    "# GINConv model\n",
    "class GINConvNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim=78, output_dim=32, GINconv_drop=0.2, pretrain_flag=False):\n",
    "        super(GINConvNet, self).__init__()\n",
    "        self.flag = pretrain_flag\n",
    "        dim = 32\n",
    "        self.dropout = nn.Dropout(GINconv_drop)\n",
    "        self.relu = nn.ReLU()\n",
    "        # convolution layers\n",
    "        nn1 = Sequential(Linear(input_dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "        self.fc1_xd = Linear(dim, 10)\n",
    "        # combined layers\n",
    "        self.out = nn.Linear(10, 10)\n",
    "    def forward(self, data, pretrain_flag=False):\n",
    "        x, edge_index = data.x, data.edge_index \n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        # x = self.bn1(x)\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        # x = self.bn2(x)\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        # x = self.bn3(x)SS\n",
    "        x = self.relu(self.conv4(x, edge_index))\n",
    "        # x = self.bn4(x)\n",
    "        x = self.relu(self.conv5(x, edge_index)) # x shape after conv: torch.Size([62, 32])\n",
    "        # x = self.bn5(x)\n",
    "        if self.flag == False:\n",
    "            if drug_graph_pool == \"add\":\n",
    "                x = global_add_pool(x, batch=data.batch) # after global_add_pool batch=None torch.Size([bze, 32])\n",
    "            elif drug_graph_pool == \"mean\":\n",
    "                x = global_mean_pool(x, batch=data.batch) # after global_add_pool batch=None torch.Size([bze, 32])\n",
    "            elif drug_graph_pool == \"max\":\n",
    "                x = global_max_pool(x, batch=data.batch) # after global_add_pool batch=None torch.Size([bze, 32])\n",
    "        x = self.relu(self.fc1_xd(x)) # output: torch.Size([1, 10])\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x # output: torch.Size([1, 10])\n",
    "# GIN_DCSA_model\n",
    "class GIN_DCSA_model(nn.Module):\n",
    "    def __init__(self,omics_encode_dim_dict, activation_func,activation_func_final,dense_layer_dim, device,\n",
    "                 hidden_size, intermediate_size, num_attention_heads , attention_probs_dropout_prob, hidden_dropout_prob, omics_numfeatures_dict, n_layer, DA_Folder, TCGA_pretrain_weight_path_dict=None):\n",
    "        super(GIN_DCSA_model, self).__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.n_layer = n_layer\n",
    "        if DA_Folder != 'None':\n",
    "            self.MLP4omics_dict = nn.ModuleDict()\n",
    "            for omic_type in omics_numfeatures_dict.keys():\n",
    "                self.MLP4omics_dict[omic_type] = nn.Sequential(\n",
    "                    nn.Identity()  # just pass through the input, no linear combination no transformation\n",
    "                )\n",
    "                #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "                self.match_drug_dim = nn.Linear(omics_encode_dim_dict[omic_type][-1], hidden_size)\n",
    "                self._init_weights(self.match_drug_dim)\n",
    "        else:\n",
    "            def load_TCGA_pretrain_weight(model, pretrained_weights_path, device):\n",
    "                state_dict = torch.load(pretrained_weights_path, map_location=device)  # Load the state_dict\n",
    "                encoder_state_dict = {key[len(\"encoder.\"):]: value for key, value in state_dict.items() if key.startswith('encoder')}  # Extract encoder weights\n",
    "                model.load_state_dict(encoder_state_dict)  # Load only the encoder part\n",
    "                model_keys = set(model.state_dict().keys())  # Check if the keys match\n",
    "                loaded_keys = set(encoder_state_dict.keys())\n",
    "                if model_keys == loaded_keys:\n",
    "                    print(f\"State_dict for {model} loaded successfully.\")\n",
    "                else:\n",
    "                    print(f\"State_dict does not match the model's architecture for {model}.\")\n",
    "                    print(\"Model keys: \", model_keys, \" Loaded keys: \", loaded_keys)\n",
    "            self.MLP4omics_dict = nn.ModuleDict()\n",
    "            for omic_type in omics_numfeatures_dict.keys():\n",
    "                self.MLP4omics_dict[omic_type] = create_mlpEncoder([omics_numfeatures_dict[omic_type]] + omics_encode_dim_dict[omic_type], activation_func,drop=classifier_drop\n",
    "                )\n",
    "                # Initialize with TCGA pretrain weight\n",
    "                if TCGA_pretrain_weight_path_dict is not None:\n",
    "                    load_TCGA_pretrain_weight(self.MLP4omics_dict[omic_type], TCGA_pretrain_weight_path_dict[omic_type], device)\n",
    "                else: # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "                    self._init_weights(self.MLP4omics_dict[omic_type])\n",
    "                #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "                self.match_drug_dim = nn.Linear(omics_encode_dim_dict[omic_type][-1], hidden_size)\n",
    "                self._init_weights(self.match_drug_dim)\n",
    "# GINConvNet\n",
    "        self.GINConv = GINConvNet(input_dim=78, output_dim=10, pretrain_flag=False)\n",
    "        if drug_pretrain_weight_path is not None:\n",
    "            self.GINConv.load_state_dict(torch.load(drug_pretrain_weight_path))  # 藥物模型預訓練權重\n",
    "        self.match_cell_dim = nn.Linear(10, hidden_size)\n",
    "        self._init_weights(self.match_cell_dim)\n",
    "# Drug_Cell_SelfAttention\n",
    "        self.Drug_Cell_SelfAttention = Encoder(hidden_size+num_attention_heads, intermediate_size, num_attention_heads,attention_probs_dropout_prob, hidden_dropout_prob)#(128+8,512,8,0.1,0.1)\n",
    "# Define the final prediction network \n",
    "        self.model_final_add = nn.Sequential(\n",
    "            nn.Linear(dense_layer_dim[0], dense_layer_dim[1]),\n",
    "            activation_func,\n",
    "            nn.Dropout(p=0),\n",
    "            nn.Linear(dense_layer_dim[1], dense_layer_dim[2]),\n",
    "            activation_func,\n",
    "            nn.Dropout(p=0),\n",
    "            nn.Linear(dense_layer_dim[2], 1),\n",
    "            activation_func_final)\n",
    "        # Initialize weights with Kaiming uniform initialization, bias with aero\n",
    "        self._init_weights(self.model_final_add)\n",
    "        self.print_flag = True\n",
    "        self.attention_probs = None # store Attention score matrix\n",
    "    def _init_weights(self, model):\n",
    "        if isinstance(model, nn.Linear):  # 直接初始化 nn.Linear 層\n",
    "            init.kaiming_uniform_(model.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "            if model.bias is not None:\n",
    "                init.zeros_(model.bias)\n",
    "        elif isinstance(model, nn.ModuleList) or isinstance(model, nn.Sequential):  # 遍歷子層\n",
    "            for layer in model:\n",
    "                self._init_weights(layer)\n",
    "    def forward(self, omics_tensor_dict,drug, device, **kwargs):\n",
    "        DCSA = kwargs['DCSA']\n",
    "        omic_embeddings_ls = []\n",
    "# Loop through each omic type and pass through its respective model\n",
    "        for omic_type, omic_tensor in omics_tensor_dict.items():\n",
    "            omic_embed = self.MLP4omics_dict[omic_type](omic_tensor)# .to(device=device)\n",
    "            #apply a linear tranformation to omics embedding to match the hidden size of the drug\n",
    "            omic_embed = self.match_drug_dim(omic_embed) #(bsz, hidden_size)\n",
    "            omic_embeddings_ls.append(omic_embed)\n",
    "        Drug_graph_feat = self.GINConv(drug,pretrain_flag=False) # Drug_graph_feat(bsz, hidden_size)\n",
    "        Drug_graph_feat = self.match_cell_dim(Drug_graph_feat) # match Drug_graph_feat to hidden_size\n",
    "        AttenScorMat_DrugCellSelf= None\n",
    "        if DCSA is True:\n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n DCSA is True \\n\")\n",
    "                self.print_flag  = False\n",
    "    # mask for Drug Cell SelfAttention\n",
    "            DrugCell_mask = torch.ones(Drug_graph_feat.size(0), 1+len(omic_embeddings_ls), dtype=Drug_graph_feat.dtype, device=Drug_graph_feat.device)  # Shape: [bsz, 1+len(omic_embeddings_ls)]\n",
    "            omic_embeddings = torch.stack(omic_embeddings_ls, dim=1) #shape:[bsz,c,hidden_size] #Stack omic_embeddings_ls along the second dimension, c: number of omic types\n",
    "            Drug_graph_feat = Drug_graph_feat.unsqueeze(1)  # Add a dimension to match the shape of omic_embeddings\n",
    "            append_embeddings = torch.cat([Drug_graph_feat, omic_embeddings], dim=1) #shape:[bsz,1+c,hidden_size] #Concatenate along the second dimension\n",
    "    # Type encoding (to distinguish between drug and omics)\n",
    "            drug_type_encoding = torch.ones_like(Drug_graph_feat[..., :1])  # Shape: [bsz, 1, 1]\n",
    "            omics_type_encoding = torch.zeros_like(omic_embeddings[..., :1])  # Shape: [bsz, c, 1]\n",
    "    # Concatenate type encoding with the respective data\n",
    "            Drug_graph_feat = torch.cat([Drug_graph_feat, drug_type_encoding], dim=-1)  # Shape: [bsz, 1, hidden_size+1]\n",
    "            omic_embeddings = torch.cat([omic_embeddings, omics_type_encoding], dim=-1)  # Shape: [bsz, c, hidden_size+1]\n",
    "    # Final concatenated tensor (drug sequence and omics data with type encoding)\n",
    "            append_embeddings = torch.cat([Drug_graph_feat, omic_embeddings], dim=1)  # Shape: [bsz, 1+c, hidden_size+1]\n",
    "            padding_dim = self.num_attention_heads - 1  # Extra dimensions to add # padding_dim=7\n",
    "            pad = torch.zeros(append_embeddings.size(0), append_embeddings.size(1), padding_dim, device=append_embeddings.device)\n",
    "            append_embeddings = torch.cat([append_embeddings, pad], dim=-1)  # New shape: [bsz, 1+c, hidden_size+8]\n",
    "    # self.Drug_Cell_SelfAttention\n",
    "            append_embeddings, AttenScorMat_DrugCellSelf  = self.Drug_Cell_SelfAttention(append_embeddings, DrugCell_mask)# DrugCell_mask.shape: torch.Size([bsz, 1+c])\n",
    "            append_embeddings = torch.cat([ torch.cat(omic_embeddings_ls, dim=1), append_embeddings.reshape(append_embeddings.size(0), -1)], dim=1) # dim=1: turn into 1D \n",
    "            #omic_embeddings_ls(bsz, c, hidden_size) + append_embeddings(bsz, 1+c, hidden_size+8) => (bsz, (1+c)*(hidden_size+8) + c*hidden_size )\n",
    "        elif DCSA is False:\n",
    "            if self.print_flag is True:\n",
    "                print(\"\\n DCSA is False \\n\")\n",
    "                self.print_flag  = False\n",
    "            append_embeddings = torch.cat([ torch.cat(omic_embeddings_ls, dim=1), Drug_graph_feat], dim=1)\n",
    "# Final MLP\n",
    "        output_before_final_activation = self.model_final_add[:-1](append_embeddings)\n",
    "        output = self.model_final_add[-1](output_before_final_activation)\n",
    "        AttenScorMat_DrugSelf= None\n",
    "        return output, AttenScorMat_DrugSelf, AttenScorMat_DrugCellSelf,output_before_final_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b43715",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracemalloc import start\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Batch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "def warmup_lr_scheduler(optimizer, decrese_epoch, Decrease_percent,continuous=True):\n",
    "    def f(epoch):\n",
    "        if epoch >= decrese_epoch:\n",
    "            if continuous is True:\n",
    "                return Decrease_percent ** (epoch-decrese_epoch+1)\n",
    "            elif continuous is not True:\n",
    "                return Decrease_percent\n",
    "        return 1\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "def log_gradient_norms(model):\n",
    "    total_norm = 0.0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:  # Skip parameters without gradients\n",
    "            param_norm = param.grad.data.norm(2)  # L2 norm (Euclidean distance)# sum(ei^2)^0.5\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    print(f\"Total Gradient Norm: {total_norm:.4f}\")\n",
    "    return total_norm\n",
    "kwargs = {\"ESPF\":ESPF,\"Drug_SelfAttention\":Drug_SelfAttention,\"DCSA\":DCSA}\n",
    "def evaluation(model, eval_epoch_loss_W_penalty_ls, eval_epoch_loss_WO_penalty_ls, \n",
    "               criterion, eval_loader, device,ESPF,Drug_SelfAttention, \n",
    "               weighted_threshold, few_weight, more_weight, \n",
    "               outputcontrol='' ):\n",
    "    torch.manual_seed(42)\n",
    "    eval_outputs = [] # for correlation\n",
    "    eval_targets = [] # for correlation\n",
    "    eval_outputs_before_final_activation_list = []\n",
    "    predAUCwithUnknownGT = []\n",
    "    mean_batch_eval_loss_W_penalty = None #np.float32(0.0)\n",
    "    mean_batch_eval_loss_WO_penalty= None #np.float32(0.0)\n",
    "    model.eval()\n",
    "    model.requires_grad = False\n",
    "    weight_loss_mask = None\n",
    "    with torch.no_grad():\n",
    "        for inputs in eval_loader:\n",
    "            # omics_tensor_dict,drug, target = inputs[0],inputs[1], inputs[-1]#.to(device=device)\n",
    "            gene_list, drug_data_list, target_list = inputs # batch\n",
    "            omics_tensor_dict = {omic: torch.stack([d[omic] for d in gene_list], dim=0) for omic in include_omics}\n",
    "            target = torch.stack(target_list) # torch.Size([bsz, 1]) # bsz: batch size\n",
    "            if drug_graph is True:\n",
    "                drug = Batch.from_data_list(drug_data_list)\n",
    "            else:\n",
    "                drug = torch.stack(drug_data_list)\n",
    "            model_output = model(omics_tensor_dict, drug, device, **kwargs) #drug.to(torch.float32)\n",
    "            outputs = model_output[0]  # model_output[1] # model_output[2] # output.shape(n_sample, 1)\n",
    "            mask = ~torch.isnan(target)# Create a mask for non-NaN values in tensor # 去除nan的項 # mask.shape(n_sample, 1)\n",
    "            target = target[mask]# Apply the mask to filter out NaN values from both tensors # target.shape(n_sample, 1)->(n_sample-nan, 1)\n",
    "            predAUCwithUnknownGT.append(outputs.detach().cpu().numpy().reshape(-1))# for unknown GroundTruth\n",
    "            outputs = outputs[mask] #dtype = 'float32'\n",
    "            eval_outputs.append(outputs.detach().reshape(-1)) #dtype = 'float32' # [tensor]\n",
    "            eval_targets.append(target.detach().reshape(-1)) # [tensor]\n",
    "            if outputcontrol != 'plotLossCurve':\n",
    "                eval_outputs_before_final_activation_list.append((model_output[3])[mask].detach().cpu().numpy().reshape(-1))\n",
    "            if 'weighted' in criterion.loss_type :    \n",
    "                if 'BCE' in criterion.loss_type :\n",
    "                    weight_loss_mask = torch.where(torch.cat(eval_targets) == 1, few_weight, more_weight) # 手動對正樣本給 few_weight 倍權重，負樣本給 more_weight 倍                        \n",
    "                else:\n",
    "                    weight_loss_mask = torch.where(torch.cat(eval_targets) > weighted_threshold, few_weight, more_weight)\n",
    "        mean_batch_eval_loss_W_penalty = criterion(torch.cat(eval_outputs),torch.cat(eval_targets), model, weight_loss_mask)# with weighted loss # without batch effect the loss\n",
    "        mean_batch_eval_loss_WO_penalty = criterion.loss_WO_penalty.cpu().detach().numpy()\n",
    "        # just for evaluation in train epoch loop , and plot the epochs loss, not for correlation\n",
    "        if outputcontrol =='plotLossCurve': \n",
    "            # print(f'Epoch [{epoch + 1}/{num_epoch}] - mean_batch Validation Loss: {mean_batch_eval_loss:.8f}')\n",
    "            eval_epoch_loss_W_penalty_ls.append(mean_batch_eval_loss_W_penalty.cpu().detach().numpy() )# \n",
    "            eval_epoch_loss_WO_penalty_ls.append(mean_batch_eval_loss_WO_penalty )\n",
    "            return (eval_targets, eval_outputs,\n",
    "                    eval_epoch_loss_W_penalty_ls,  eval_epoch_loss_WO_penalty_ls,  \n",
    "                    mean_batch_eval_loss_WO_penalty)\n",
    "        # for inference after train epoch loop, and store output for correlation\n",
    "        elif outputcontrol == 'correlation':\n",
    "            # print(f'Evaluation {outputcontrol} Loss: {mean_batch_eval_loss:.8f}')\n",
    "            return (eval_targets, eval_outputs,mean_batch_eval_loss_WO_penalty,\n",
    "                    eval_outputs_before_final_activation_list)\n",
    "        elif outputcontrol =='inference':\n",
    "            AttenScorMat_DrugSelf = model_output[1]\n",
    "            AttenScorMat_DrugCellSelf = model_output[2]\n",
    "            return (eval_targets, eval_outputs,predAUCwithUnknownGT, \n",
    "                    AttenScorMat_DrugSelf,AttenScorMat_DrugCellSelf,\n",
    "                    eval_outputs_before_final_activation_list, mean_batch_eval_loss_WO_penalty)\n",
    "        else:\n",
    "            print('error occur when outputcontrol argument is not correct')\n",
    "            return 'error occur when outputcontrol argument is not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c05154a",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
